{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files used: 5000\n",
      "Approach-- #epoch100-#iter20-lambda0.001\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 13:54:43,281 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 13:54:39.837392-#epoch100-#iter20-lambda0.001'\n",
      "2018-05-26 13:54:43,283 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 13:54:39.837392-#epoch100-#iter20-lambda0.001'\n",
      "2018-05-26 13:54:46,008 Start optimization\n",
      "2018-05-26 13:54:56,397 Epoch 0, Average loss: 36.5355, learning rate: 0.0003\n",
      "2018-05-26 13:55:08,313 Epoch 1, Average loss: 31.2280, learning rate: 0.0003\n",
      "2018-05-26 13:55:20,199 Epoch 2, Average loss: 31.9969, learning rate: 0.0003\n",
      "2018-05-26 13:55:32,276 Epoch 3, Average loss: 33.7306, learning rate: 0.0003\n",
      "2018-05-26 13:55:44,685 Epoch 4, Average loss: 31.7567, learning rate: 0.0003\n",
      "2018-05-26 13:55:57,244 Epoch 5, Average loss: 36.2812, learning rate: 0.0003\n",
      "2018-05-26 13:56:09,979 Epoch 6, Average loss: 22.6769, learning rate: 0.0003\n",
      "2018-05-26 13:56:22,963 Epoch 7, Average loss: 26.4038, learning rate: 0.0003\n",
      "2018-05-26 13:56:36,363 Epoch 8, Average loss: 26.3189, learning rate: 0.0003\n",
      "2018-05-26 13:56:49,911 Epoch 9, Average loss: 18.8806, learning rate: 0.0003\n",
      "2018-05-26 13:57:03,728 Epoch 10, Average loss: 26.1910, learning rate: 0.0003\n",
      "2018-05-26 13:57:17,836 Epoch 11, Average loss: 26.1251, learning rate: 0.0003\n",
      "2018-05-26 13:57:32,337 Epoch 12, Average loss: 25.3881, learning rate: 0.0003\n",
      "2018-05-26 13:57:46,973 Epoch 13, Average loss: 20.4716, learning rate: 0.0003\n",
      "2018-05-26 13:58:01,843 Epoch 14, Average loss: 35.2111, learning rate: 0.0003\n",
      "2018-05-26 13:58:17,123 Epoch 15, Average loss: 21.4038, learning rate: 0.0003\n",
      "2018-05-26 13:58:32,562 Epoch 16, Average loss: 20.5624, learning rate: 0.0003\n",
      "2018-05-26 13:58:48,380 Epoch 17, Average loss: 22.0587, learning rate: 0.0003\n",
      "2018-05-26 13:59:04,289 Epoch 18, Average loss: 19.2304, learning rate: 0.0003\n",
      "2018-05-26 13:59:20,929 Epoch 19, Average loss: 21.8114, learning rate: 0.0003\n",
      "2018-05-26 13:59:36,120 Epoch 20, Average loss: 18.5076, learning rate: 0.0003\n",
      "2018-05-26 13:59:51,670 Epoch 21, Average loss: 13.9565, learning rate: 0.0003\n",
      "2018-05-26 14:00:07,552 Epoch 22, Average loss: 24.7670, learning rate: 0.0003\n",
      "2018-05-26 14:00:23,705 Epoch 23, Average loss: 15.5888, learning rate: 0.0003\n",
      "2018-05-26 14:00:40,507 Epoch 24, Average loss: 22.9791, learning rate: 0.0003\n",
      "2018-05-26 14:00:57,115 Epoch 25, Average loss: 18.6895, learning rate: 0.0003\n",
      "2018-05-26 14:01:13,916 Epoch 26, Average loss: 21.1952, learning rate: 0.0003\n",
      "2018-05-26 14:01:30,940 Epoch 27, Average loss: 16.8138, learning rate: 0.0003\n",
      "2018-05-26 14:01:48,526 Epoch 28, Average loss: 16.7844, learning rate: 0.0003\n",
      "2018-05-26 14:02:06,256 Epoch 29, Average loss: 13.0952, learning rate: 0.0003\n",
      "2018-05-26 14:02:24,234 Epoch 30, Average loss: 19.5848, learning rate: 0.0003\n",
      "2018-05-26 14:02:42,460 Epoch 31, Average loss: 20.0715, learning rate: 0.0003\n",
      "2018-05-26 14:03:01,052 Epoch 32, Average loss: 17.2980, learning rate: 0.0003\n",
      "2018-05-26 14:03:20,078 Epoch 33, Average loss: 16.7221, learning rate: 0.0003\n",
      "2018-05-26 14:03:39,456 Epoch 34, Average loss: 19.1560, learning rate: 0.0003\n",
      "2018-05-26 14:03:59,093 Epoch 35, Average loss: 13.4039, learning rate: 0.0003\n",
      "2018-05-26 14:04:19,065 Epoch 36, Average loss: 19.5307, learning rate: 0.0003\n",
      "2018-05-26 14:04:39,348 Epoch 37, Average loss: 13.6207, learning rate: 0.0003\n",
      "2018-05-26 14:05:00,090 Epoch 38, Average loss: 13.8035, learning rate: 0.0003\n",
      "2018-05-26 14:05:21,147 Epoch 39, Average loss: 19.0568, learning rate: 0.0003\n",
      "2018-05-26 14:05:42,331 Epoch 40, Average loss: 15.3270, learning rate: 0.0003\n",
      "2018-05-26 14:06:04,063 Epoch 41, Average loss: 16.4509, learning rate: 0.0003\n",
      "2018-05-26 14:06:26,037 Epoch 42, Average loss: 15.6320, learning rate: 0.0003\n",
      "2018-05-26 14:06:48,196 Epoch 43, Average loss: 15.9800, learning rate: 0.0003\n",
      "2018-05-26 14:07:10,695 Epoch 44, Average loss: 13.5771, learning rate: 0.0003\n",
      "2018-05-26 14:07:33,585 Epoch 45, Average loss: 24.4650, learning rate: 0.0003\n",
      "2018-05-26 14:07:56,762 Epoch 46, Average loss: 13.0548, learning rate: 0.0003\n",
      "2018-05-26 14:08:20,285 Epoch 47, Average loss: 11.4569, learning rate: 0.0003\n",
      "2018-05-26 14:08:44,120 Epoch 48, Average loss: 12.9808, learning rate: 0.0003\n",
      "2018-05-26 14:09:08,205 Epoch 49, Average loss: 11.5873, learning rate: 0.0003\n",
      "2018-05-26 14:09:32,821 Epoch 50, Average loss: 21.8624, learning rate: 0.0003\n",
      "2018-05-26 14:09:57,588 Epoch 51, Average loss: 13.3004, learning rate: 0.0003\n",
      "2018-05-26 14:10:22,758 Epoch 52, Average loss: 11.7851, learning rate: 0.0003\n",
      "2018-05-26 14:10:48,140 Epoch 53, Average loss: 9.6996, learning rate: 0.0003\n",
      "2018-05-26 14:11:13,905 Epoch 54, Average loss: 17.6705, learning rate: 0.0003\n",
      "2018-05-26 14:11:40,282 Epoch 55, Average loss: 13.4236, learning rate: 0.0003\n",
      "2018-05-26 14:12:06,784 Epoch 56, Average loss: 10.5337, learning rate: 0.0003\n",
      "2018-05-26 14:12:33,599 Epoch 57, Average loss: 15.4747, learning rate: 0.0003\n",
      "2018-05-26 14:13:00,761 Epoch 58, Average loss: 10.2579, learning rate: 0.0003\n",
      "2018-05-26 14:13:28,124 Epoch 59, Average loss: 14.8740, learning rate: 0.0003\n",
      "2018-05-26 14:13:56,125 Epoch 60, Average loss: 21.6946, learning rate: 0.0003\n",
      "2018-05-26 14:14:24,408 Epoch 61, Average loss: 11.2471, learning rate: 0.0003\n",
      "2018-05-26 14:14:52,918 Epoch 62, Average loss: 11.9673, learning rate: 0.0003\n",
      "2018-05-26 14:15:22,011 Epoch 63, Average loss: 12.5142, learning rate: 0.0003\n",
      "2018-05-26 14:15:51,091 Epoch 64, Average loss: 10.8828, learning rate: 0.0003\n",
      "2018-05-26 14:16:20,448 Epoch 65, Average loss: 8.9415, learning rate: 0.0003\n",
      "2018-05-26 14:16:50,077 Epoch 66, Average loss: 14.0429, learning rate: 0.0003\n",
      "2018-05-26 14:17:20,106 Epoch 67, Average loss: 10.6708, learning rate: 0.0003\n",
      "2018-05-26 14:17:50,439 Epoch 68, Average loss: 9.1784, learning rate: 0.0003\n",
      "2018-05-26 14:18:21,241 Epoch 69, Average loss: 12.8949, learning rate: 0.0003\n",
      "2018-05-26 14:18:52,137 Epoch 70, Average loss: 13.4347, learning rate: 0.0003\n",
      "2018-05-26 14:19:23,671 Epoch 71, Average loss: 8.1045, learning rate: 0.0003\n",
      "2018-05-26 14:19:55,077 Epoch 72, Average loss: 17.0606, learning rate: 0.0003\n",
      "2018-05-26 14:20:26,836 Epoch 73, Average loss: 13.1647, learning rate: 0.0003\n",
      "2018-05-26 14:20:58,938 Epoch 74, Average loss: 11.7623, learning rate: 0.0003\n",
      "2018-05-26 14:21:31,245 Epoch 75, Average loss: 10.5396, learning rate: 0.0003\n",
      "2018-05-26 14:22:04,140 Epoch 76, Average loss: 15.2332, learning rate: 0.0003\n",
      "2018-05-26 14:22:37,853 Epoch 77, Average loss: 18.2682, learning rate: 0.0003\n",
      "2018-05-26 14:23:11,724 Epoch 78, Average loss: 15.0315, learning rate: 0.0003\n",
      "2018-05-26 14:23:45,893 Epoch 79, Average loss: 11.7350, learning rate: 0.0003\n",
      "2018-05-26 14:24:20,229 Epoch 80, Average loss: 7.1359, learning rate: 0.0003\n",
      "2018-05-26 14:24:54,989 Epoch 81, Average loss: 7.4286, learning rate: 0.0003\n",
      "2018-05-26 14:25:30,141 Epoch 82, Average loss: 8.1525, learning rate: 0.0003\n",
      "2018-05-26 14:26:05,490 Epoch 83, Average loss: 12.0112, learning rate: 0.0003\n",
      "2018-05-26 14:26:41,152 Epoch 84, Average loss: 13.3968, learning rate: 0.0003\n",
      "2018-05-26 14:27:17,218 Epoch 85, Average loss: 8.3682, learning rate: 0.0003\n",
      "2018-05-26 14:27:53,659 Epoch 86, Average loss: 11.3051, learning rate: 0.0003\n",
      "2018-05-26 14:28:30,348 Epoch 87, Average loss: 14.5384, learning rate: 0.0003\n",
      "2018-05-26 14:29:07,768 Epoch 88, Average loss: 9.6802, learning rate: 0.0003\n",
      "2018-05-26 14:29:45,070 Epoch 89, Average loss: 12.2227, learning rate: 0.0003\n",
      "2018-05-26 14:30:22,839 Epoch 90, Average loss: 13.2716, learning rate: 0.0003\n",
      "2018-05-26 14:31:00,969 Epoch 91, Average loss: 16.3334, learning rate: 0.0003\n",
      "2018-05-26 14:31:39,146 Epoch 92, Average loss: 11.7968, learning rate: 0.0003\n",
      "2018-05-26 14:32:17,976 Epoch 93, Average loss: 13.2474, learning rate: 0.0003\n",
      "2018-05-26 14:32:56,940 Epoch 94, Average loss: 10.5793, learning rate: 0.0003\n",
      "2018-05-26 14:33:36,181 Epoch 95, Average loss: 12.7228, learning rate: 0.0003\n",
      "2018-05-26 14:34:15,755 Epoch 96, Average loss: 8.0257, learning rate: 0.0003\n",
      "2018-05-26 14:34:56,125 Epoch 97, Average loss: 11.8652, learning rate: 0.0003\n",
      "2018-05-26 14:35:36,361 Epoch 98, Average loss: 15.4115, learning rate: 0.0003\n",
      "2018-05-26 14:36:16,711 Epoch 99, Average loss: 16.9880, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 14:36:22,160 Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda0.0001\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 14:36:26,123 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 14:36:22.630679-#epoch100-#iter20-lambda0.0001'\n",
      "2018-05-26 14:36:26,124 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 14:36:22.630679-#epoch100-#iter20-lambda0.0001'\n",
      "2018-05-26 14:36:29,063 Start optimization\n",
      "2018-05-26 14:36:39,927 Epoch 0, Average loss: 32.9037, learning rate: 0.0003\n",
      "2018-05-26 14:36:52,431 Epoch 1, Average loss: 34.9466, learning rate: 0.0003\n",
      "2018-05-26 14:37:04,923 Epoch 2, Average loss: 32.7300, learning rate: 0.0003\n",
      "2018-05-26 14:37:17,673 Epoch 3, Average loss: 38.0474, learning rate: 0.0003\n",
      "2018-05-26 14:37:30,627 Epoch 4, Average loss: 24.6214, learning rate: 0.0003\n",
      "2018-05-26 14:37:43,800 Epoch 5, Average loss: 28.4354, learning rate: 0.0003\n",
      "2018-05-26 14:37:59,676 Epoch 6, Average loss: 19.6131, learning rate: 0.0003\n",
      "2018-05-26 14:38:14,530 Epoch 7, Average loss: 18.8453, learning rate: 0.0003\n",
      "2018-05-26 14:38:29,568 Epoch 8, Average loss: 24.8447, learning rate: 0.0003\n",
      "2018-05-26 14:38:44,664 Epoch 9, Average loss: 18.2329, learning rate: 0.0003\n",
      "2018-05-26 14:39:00,094 Epoch 10, Average loss: 16.1362, learning rate: 0.0003\n",
      "2018-05-26 14:39:15,676 Epoch 11, Average loss: 18.7101, learning rate: 0.0003\n",
      "2018-05-26 14:39:31,512 Epoch 12, Average loss: 17.3396, learning rate: 0.0003\n",
      "2018-05-26 14:39:47,421 Epoch 13, Average loss: 17.5051, learning rate: 0.0003\n",
      "2018-05-26 14:40:03,552 Epoch 14, Average loss: 22.2181, learning rate: 0.0003\n",
      "2018-05-26 14:40:20,290 Epoch 15, Average loss: 21.1690, learning rate: 0.0003\n",
      "2018-05-26 14:40:37,074 Epoch 16, Average loss: 11.1446, learning rate: 0.0003\n",
      "2018-05-26 14:40:54,098 Epoch 17, Average loss: 17.1313, learning rate: 0.0003\n",
      "2018-05-26 14:41:11,327 Epoch 18, Average loss: 20.3068, learning rate: 0.0003\n",
      "2018-05-26 14:41:28,084 Epoch 19, Average loss: 21.8270, learning rate: 0.0003\n",
      "2018-05-26 14:41:45,409 Epoch 20, Average loss: 29.1861, learning rate: 0.0003\n",
      "2018-05-26 14:42:02,717 Epoch 21, Average loss: 115.4090, learning rate: 0.0003\n",
      "2018-05-26 14:42:20,721 Epoch 22, Average loss: 115.4724, learning rate: 0.0003\n",
      "2018-05-26 14:42:38,961 Epoch 23, Average loss: 115.4642, learning rate: 0.0003\n",
      "2018-05-26 14:42:57,409 Epoch 24, Average loss: 115.4175, learning rate: 0.0003\n",
      "2018-05-26 14:43:16,565 Epoch 25, Average loss: 115.4943, learning rate: 0.0003\n",
      "2018-05-26 14:43:35,946 Epoch 26, Average loss: 115.4714, learning rate: 0.0003\n",
      "2018-05-26 14:43:55,114 Epoch 27, Average loss: 115.4650, learning rate: 0.0003\n",
      "2018-05-26 14:44:14,236 Epoch 28, Average loss: 115.4793, learning rate: 0.0003\n",
      "2018-05-26 14:44:33,696 Epoch 29, Average loss: 115.4743, learning rate: 0.0003\n",
      "2018-05-26 14:44:54,014 Epoch 30, Average loss: 115.4374, learning rate: 0.0003\n",
      "2018-05-26 14:45:14,225 Epoch 31, Average loss: 115.3924, learning rate: 0.0003\n",
      "2018-05-26 14:45:34,821 Epoch 32, Average loss: 115.5988, learning rate: 0.0003\n",
      "2018-05-26 14:45:55,699 Epoch 33, Average loss: 115.4441, learning rate: 0.0003\n",
      "2018-05-26 14:46:17,143 Epoch 34, Average loss: 115.5037, learning rate: 0.0003\n",
      "2018-05-26 14:46:38,218 Epoch 35, Average loss: 115.4953, learning rate: 0.0003\n",
      "2018-05-26 14:46:59,898 Epoch 36, Average loss: 115.4437, learning rate: 0.0003\n",
      "2018-05-26 14:47:21,744 Epoch 37, Average loss: 115.5320, learning rate: 0.0003\n",
      "2018-05-26 14:47:44,092 Epoch 38, Average loss: 115.4691, learning rate: 0.0003\n",
      "2018-05-26 14:48:07,146 Epoch 39, Average loss: 115.4820, learning rate: 0.0003\n",
      "2018-05-26 14:48:30,332 Epoch 40, Average loss: 115.4584, learning rate: 0.0003\n",
      "2018-05-26 14:48:54,692 Epoch 41, Average loss: 115.4109, learning rate: 0.0003\n",
      "2018-05-26 14:49:19,282 Epoch 42, Average loss: 115.5334, learning rate: 0.0003\n",
      "2018-05-26 14:49:43,871 Epoch 43, Average loss: 115.4287, learning rate: 0.0003\n",
      "2018-05-26 14:50:08,690 Epoch 44, Average loss: 115.5307, learning rate: 0.0003\n",
      "2018-05-26 14:50:33,949 Epoch 45, Average loss: 115.4940, learning rate: 0.0003\n",
      "2018-05-26 14:50:58,629 Epoch 46, Average loss: 115.4543, learning rate: 0.0003\n",
      "2018-05-26 14:51:24,326 Epoch 47, Average loss: 115.4792, learning rate: 0.0003\n",
      "2018-05-26 14:51:49,985 Epoch 48, Average loss: 115.4341, learning rate: 0.0003\n",
      "2018-05-26 14:52:16,566 Epoch 49, Average loss: 115.4301, learning rate: 0.0003\n",
      "2018-05-26 14:52:43,606 Epoch 50, Average loss: 115.4665, learning rate: 0.0003\n",
      "2018-05-26 14:53:11,561 Epoch 51, Average loss: 115.4664, learning rate: 0.0003\n",
      "2018-05-26 14:53:38,917 Epoch 52, Average loss: 115.3705, learning rate: 0.0003\n",
      "2018-05-26 14:54:06,192 Epoch 53, Average loss: 115.4306, learning rate: 0.0003\n",
      "2018-05-26 14:54:33,453 Epoch 54, Average loss: 115.4269, learning rate: 0.0003\n",
      "2018-05-26 14:55:01,121 Epoch 55, Average loss: 115.4176, learning rate: 0.0003\n",
      "2018-05-26 14:55:29,219 Epoch 56, Average loss: 115.4948, learning rate: 0.0003\n",
      "2018-05-26 14:55:57,702 Epoch 57, Average loss: 115.5134, learning rate: 0.0003\n",
      "2018-05-26 14:56:26,895 Epoch 58, Average loss: 115.4885, learning rate: 0.0003\n",
      "2018-05-26 14:56:55,801 Epoch 59, Average loss: 115.4362, learning rate: 0.0003\n",
      "2018-05-26 14:57:25,868 Epoch 60, Average loss: 115.3885, learning rate: 0.0003\n",
      "2018-05-26 14:57:55,244 Epoch 61, Average loss: 115.4423, learning rate: 0.0003\n",
      "2018-05-26 14:58:25,036 Epoch 62, Average loss: 115.5209, learning rate: 0.0003\n",
      "2018-05-26 14:58:55,381 Epoch 63, Average loss: 115.4619, learning rate: 0.0003\n",
      "2018-05-26 14:59:26,474 Epoch 64, Average loss: 115.5071, learning rate: 0.0003\n",
      "2018-05-26 14:59:57,333 Epoch 65, Average loss: 115.5172, learning rate: 0.0003\n",
      "2018-05-26 15:00:28,777 Epoch 66, Average loss: 115.4705, learning rate: 0.0003\n",
      "2018-05-26 15:01:00,168 Epoch 67, Average loss: 115.4645, learning rate: 0.0003\n",
      "2018-05-26 15:01:32,532 Epoch 68, Average loss: 115.5168, learning rate: 0.0003\n",
      "2018-05-26 15:02:05,017 Epoch 69, Average loss: 115.4598, learning rate: 0.0003\n",
      "2018-05-26 15:02:37,633 Epoch 70, Average loss: 115.4890, learning rate: 0.0003\n",
      "2018-05-26 15:03:11,456 Epoch 71, Average loss: 115.4700, learning rate: 0.0003\n",
      "2018-05-26 15:03:45,060 Epoch 72, Average loss: 115.4799, learning rate: 0.0003\n",
      "2018-05-26 15:04:19,076 Epoch 73, Average loss: 115.4276, learning rate: 0.0003\n",
      "2018-05-26 15:04:52,488 Epoch 74, Average loss: 115.4572, learning rate: 0.0003\n",
      "2018-05-26 15:05:26,849 Epoch 75, Average loss: 115.4592, learning rate: 0.0003\n",
      "2018-05-26 15:06:01,234 Epoch 76, Average loss: 115.4421, learning rate: 0.0003\n",
      "2018-05-26 15:06:36,165 Epoch 77, Average loss: 115.5290, learning rate: 0.0003\n",
      "2018-05-26 15:07:12,883 Epoch 78, Average loss: 115.4521, learning rate: 0.0003\n",
      "2018-05-26 15:07:49,373 Epoch 79, Average loss: 115.4757, learning rate: 0.0003\n",
      "2018-05-26 15:08:25,829 Epoch 80, Average loss: 115.5292, learning rate: 0.0003\n",
      "2018-05-26 15:09:02,292 Epoch 81, Average loss: 115.4928, learning rate: 0.0003\n",
      "2018-05-26 15:09:38,778 Epoch 82, Average loss: 115.5059, learning rate: 0.0003\n",
      "2018-05-26 15:10:15,333 Epoch 83, Average loss: 115.4776, learning rate: 0.0003\n",
      "2018-05-26 15:10:52,064 Epoch 84, Average loss: 115.4191, learning rate: 0.0003\n",
      "2018-05-26 15:11:29,991 Epoch 85, Average loss: 115.4230, learning rate: 0.0003\n",
      "2018-05-26 15:12:07,653 Epoch 86, Average loss: 115.5135, learning rate: 0.0003\n",
      "2018-05-26 15:12:45,712 Epoch 87, Average loss: 115.4977, learning rate: 0.0003\n",
      "2018-05-26 15:13:24,203 Epoch 88, Average loss: 115.3875, learning rate: 0.0003\n",
      "2018-05-26 15:14:04,085 Epoch 89, Average loss: 115.4974, learning rate: 0.0003\n",
      "2018-05-26 15:14:42,826 Epoch 90, Average loss: 115.4837, learning rate: 0.0003\n",
      "2018-05-26 15:15:21,940 Epoch 91, Average loss: 115.4404, learning rate: 0.0003\n",
      "2018-05-26 15:16:01,315 Epoch 92, Average loss: 115.4206, learning rate: 0.0003\n",
      "2018-05-26 15:16:41,188 Epoch 93, Average loss: 115.4866, learning rate: 0.0003\n",
      "2018-05-26 15:17:22,010 Epoch 94, Average loss: 115.4450, learning rate: 0.0003\n",
      "2018-05-26 15:18:02,721 Epoch 95, Average loss: 115.5123, learning rate: 0.0003\n",
      "2018-05-26 15:18:43,719 Epoch 96, Average loss: 115.4683, learning rate: 0.0003\n",
      "2018-05-26 15:19:24,956 Epoch 97, Average loss: 115.4684, learning rate: 0.0003\n",
      "2018-05-26 15:20:06,490 Epoch 98, Average loss: 115.4855, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 15:20:48,921 Epoch 99, Average loss: 115.4380, learning rate: 0.0003\n",
      "2018-05-26 15:20:54,610 Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda1e-05\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 15:21:00,009 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 15:20:55.142408-#epoch100-#iter20-lambda1e-05'\n",
      "2018-05-26 15:21:00,010 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 15:20:55.142408-#epoch100-#iter20-lambda1e-05'\n",
      "2018-05-26 15:21:02,972 Start optimization\n",
      "2018-05-26 15:21:14,141 Epoch 0, Average loss: 46.2725, learning rate: 0.0003\n",
      "2018-05-26 15:21:26,670 Epoch 1, Average loss: 25.9532, learning rate: 0.0003\n",
      "2018-05-26 15:21:39,063 Epoch 2, Average loss: 22.9542, learning rate: 0.0003\n",
      "2018-05-26 15:21:51,724 Epoch 3, Average loss: 18.6229, learning rate: 0.0003\n",
      "2018-05-26 15:22:04,629 Epoch 4, Average loss: 26.1724, learning rate: 0.0003\n",
      "2018-05-26 15:22:17,913 Epoch 5, Average loss: 21.8533, learning rate: 0.0003\n",
      "2018-05-26 15:22:31,328 Epoch 6, Average loss: 19.0042, learning rate: 0.0003\n",
      "2018-05-26 15:22:44,887 Epoch 7, Average loss: 24.3033, learning rate: 0.0003\n",
      "2018-05-26 15:22:58,863 Epoch 8, Average loss: 20.2009, learning rate: 0.0003\n",
      "2018-05-26 15:23:13,188 Epoch 9, Average loss: 22.4745, learning rate: 0.0003\n",
      "2018-05-26 15:23:27,696 Epoch 10, Average loss: 28.1428, learning rate: 0.0003\n",
      "2018-05-26 15:23:42,506 Epoch 11, Average loss: 20.1669, learning rate: 0.0003\n",
      "2018-05-26 15:23:57,606 Epoch 12, Average loss: 19.3104, learning rate: 0.0003\n",
      "2018-05-26 15:24:13,115 Epoch 13, Average loss: 20.6896, learning rate: 0.0003\n",
      "2018-05-26 15:24:28,726 Epoch 14, Average loss: 16.2021, learning rate: 0.0003\n",
      "2018-05-26 15:24:44,551 Epoch 15, Average loss: 16.4417, learning rate: 0.0003\n",
      "2018-05-26 15:25:00,646 Epoch 16, Average loss: 18.9010, learning rate: 0.0003\n",
      "2018-05-26 15:25:16,923 Epoch 17, Average loss: 19.3681, learning rate: 0.0003\n",
      "2018-05-26 15:25:33,594 Epoch 18, Average loss: 17.6714, learning rate: 0.0003\n",
      "2018-05-26 15:25:50,475 Epoch 19, Average loss: 19.8385, learning rate: 0.0003\n",
      "2018-05-26 15:26:07,451 Epoch 20, Average loss: 15.9427, learning rate: 0.0003\n",
      "2018-05-26 15:26:23,885 Epoch 21, Average loss: 15.6742, learning rate: 0.0003\n",
      "2018-05-26 15:26:40,353 Epoch 22, Average loss: 21.4155, learning rate: 0.0003\n",
      "2018-05-26 15:26:57,763 Epoch 23, Average loss: 17.9018, learning rate: 0.0003\n",
      "2018-05-26 15:27:14,856 Epoch 24, Average loss: 16.5162, learning rate: 0.0003\n",
      "2018-05-26 15:27:32,717 Epoch 25, Average loss: 16.9687, learning rate: 0.0003\n",
      "2018-05-26 15:27:50,785 Epoch 26, Average loss: 20.1033, learning rate: 0.0003\n",
      "2018-05-26 15:28:08,949 Epoch 27, Average loss: 22.1870, learning rate: 0.0003\n",
      "2018-05-26 15:28:27,412 Epoch 28, Average loss: 17.9913, learning rate: 0.0003\n",
      "2018-05-26 15:28:46,845 Epoch 29, Average loss: 17.9230, learning rate: 0.0003\n",
      "2018-05-26 15:29:06,095 Epoch 30, Average loss: 11.9045, learning rate: 0.0003\n",
      "2018-05-26 15:29:25,958 Epoch 31, Average loss: 13.3671, learning rate: 0.0003\n",
      "2018-05-26 15:29:45,784 Epoch 32, Average loss: 12.5766, learning rate: 0.0003\n",
      "2018-05-26 15:30:06,233 Epoch 33, Average loss: 15.2883, learning rate: 0.0003\n",
      "2018-05-26 15:30:27,122 Epoch 34, Average loss: 17.1209, learning rate: 0.0003\n",
      "2018-05-26 15:30:48,310 Epoch 35, Average loss: 13.6483, learning rate: 0.0003\n",
      "2018-05-26 15:31:09,521 Epoch 36, Average loss: 18.6751, learning rate: 0.0003\n",
      "2018-05-26 15:31:31,069 Epoch 37, Average loss: 17.8155, learning rate: 0.0003\n",
      "2018-05-26 15:31:53,165 Epoch 38, Average loss: 12.3019, learning rate: 0.0003\n",
      "2018-05-26 15:32:15,799 Epoch 39, Average loss: 15.8034, learning rate: 0.0003\n",
      "2018-05-26 15:32:38,208 Epoch 40, Average loss: 14.8618, learning rate: 0.0003\n",
      "2018-05-26 15:33:00,837 Epoch 41, Average loss: 21.2349, learning rate: 0.0003\n",
      "2018-05-26 15:33:24,207 Epoch 42, Average loss: 17.1174, learning rate: 0.0003\n",
      "2018-05-26 15:33:47,846 Epoch 43, Average loss: 17.3165, learning rate: 0.0003\n",
      "2018-05-26 15:34:11,981 Epoch 44, Average loss: 14.1196, learning rate: 0.0003\n",
      "2018-05-26 15:34:36,167 Epoch 45, Average loss: 14.8970, learning rate: 0.0003\n",
      "2018-05-26 15:35:00,627 Epoch 46, Average loss: 10.1246, learning rate: 0.0003\n",
      "2018-05-26 15:35:25,467 Epoch 47, Average loss: 14.9002, learning rate: 0.0003\n",
      "2018-05-26 15:35:50,647 Epoch 48, Average loss: 19.3325, learning rate: 0.0003\n",
      "2018-05-26 15:36:16,157 Epoch 49, Average loss: 19.4001, learning rate: 0.0003\n",
      "2018-05-26 15:36:41,807 Epoch 50, Average loss: 10.5887, learning rate: 0.0003\n",
      "2018-05-26 15:37:07,971 Epoch 51, Average loss: 13.3692, learning rate: 0.0003\n",
      "2018-05-26 15:37:34,254 Epoch 52, Average loss: 9.5879, learning rate: 0.0003\n",
      "2018-05-26 15:38:01,602 Epoch 53, Average loss: 9.7765, learning rate: 0.0003\n",
      "2018-05-26 15:38:28,853 Epoch 54, Average loss: 9.9745, learning rate: 0.0003\n",
      "2018-05-26 15:38:56,547 Epoch 55, Average loss: 14.7679, learning rate: 0.0003\n",
      "2018-05-26 15:39:25,258 Epoch 56, Average loss: 17.1050, learning rate: 0.0003\n",
      "2018-05-26 15:39:54,375 Epoch 57, Average loss: 16.2129, learning rate: 0.0003\n",
      "2018-05-26 15:40:22,520 Epoch 58, Average loss: 13.0945, learning rate: 0.0003\n",
      "2018-05-26 15:40:50,958 Epoch 59, Average loss: 11.6947, learning rate: 0.0003\n",
      "2018-05-26 15:41:20,422 Epoch 60, Average loss: 9.3376, learning rate: 0.0003\n",
      "2018-05-26 15:41:50,654 Epoch 61, Average loss: 13.6586, learning rate: 0.0003\n",
      "2018-05-26 15:42:21,229 Epoch 62, Average loss: 16.7202, learning rate: 0.0003\n",
      "2018-05-26 15:42:51,717 Epoch 63, Average loss: 9.9021, learning rate: 0.0003\n",
      "2018-05-26 15:43:22,536 Epoch 64, Average loss: 10.1981, learning rate: 0.0003\n",
      "2018-05-26 15:43:53,330 Epoch 65, Average loss: 8.3531, learning rate: 0.0003\n",
      "2018-05-26 15:44:24,225 Epoch 66, Average loss: 9.2214, learning rate: 0.0003\n",
      "2018-05-26 15:44:55,417 Epoch 67, Average loss: 11.5686, learning rate: 0.0003\n",
      "2018-05-26 15:45:26,777 Epoch 68, Average loss: 13.4168, learning rate: 0.0003\n",
      "2018-05-26 15:45:58,685 Epoch 69, Average loss: 15.1625, learning rate: 0.0003\n",
      "2018-05-26 15:46:30,861 Epoch 70, Average loss: 12.8045, learning rate: 0.0003\n",
      "2018-05-26 15:47:03,206 Epoch 71, Average loss: 8.1436, learning rate: 0.0003\n",
      "2018-05-26 15:47:36,106 Epoch 72, Average loss: 10.0415, learning rate: 0.0003\n",
      "2018-05-26 15:48:09,820 Epoch 73, Average loss: 21.3862, learning rate: 0.0003\n",
      "2018-05-26 15:48:43,591 Epoch 74, Average loss: 16.4423, learning rate: 0.0003\n",
      "2018-05-26 15:49:17,497 Epoch 75, Average loss: 12.3677, learning rate: 0.0003\n",
      "2018-05-26 15:49:52,036 Epoch 76, Average loss: 10.3001, learning rate: 0.0003\n",
      "2018-05-26 15:50:28,072 Epoch 77, Average loss: 14.5796, learning rate: 0.0003\n",
      "2018-05-26 15:51:03,809 Epoch 78, Average loss: 13.5889, learning rate: 0.0003\n",
      "2018-05-26 15:51:39,034 Epoch 79, Average loss: 11.4353, learning rate: 0.0003\n",
      "2018-05-26 15:52:14,633 Epoch 80, Average loss: 8.8355, learning rate: 0.0003\n",
      "2018-05-26 15:52:50,933 Epoch 81, Average loss: 15.1200, learning rate: 0.0003\n",
      "2018-05-26 15:53:27,924 Epoch 82, Average loss: 13.5070, learning rate: 0.0003\n",
      "2018-05-26 15:54:04,942 Epoch 83, Average loss: 8.0688, learning rate: 0.0003\n",
      "2018-05-26 15:54:43,769 Epoch 84, Average loss: 23.5256, learning rate: 0.0003\n",
      "2018-05-26 15:55:23,461 Epoch 85, Average loss: 8.9999, learning rate: 0.0003\n",
      "2018-05-26 15:56:01,523 Epoch 86, Average loss: 17.3800, learning rate: 0.0003\n",
      "2018-05-26 15:56:39,969 Epoch 87, Average loss: 19.1368, learning rate: 0.0003\n",
      "2018-05-26 15:57:18,867 Epoch 88, Average loss: 11.1171, learning rate: 0.0003\n",
      "2018-05-26 15:57:57,612 Epoch 89, Average loss: 18.0934, learning rate: 0.0003\n",
      "2018-05-26 15:58:37,090 Epoch 90, Average loss: 12.6947, learning rate: 0.0003\n",
      "2018-05-26 15:59:16,929 Epoch 91, Average loss: 16.1429, learning rate: 0.0003\n",
      "2018-05-26 15:59:56,502 Epoch 92, Average loss: 8.4380, learning rate: 0.0003\n",
      "2018-05-26 16:00:36,428 Epoch 93, Average loss: 9.7833, learning rate: 0.0003\n",
      "2018-05-26 16:01:17,700 Epoch 94, Average loss: 11.7405, learning rate: 0.0003\n",
      "2018-05-26 16:01:58,241 Epoch 95, Average loss: 15.7752, learning rate: 0.0003\n",
      "2018-05-26 16:02:39,560 Epoch 96, Average loss: 18.4318, learning rate: 0.0003\n",
      "2018-05-26 16:03:21,418 Epoch 97, Average loss: 10.5331, learning rate: 0.0003\n",
      "2018-05-26 16:04:03,402 Epoch 98, Average loss: 10.1847, learning rate: 0.0003\n",
      "2018-05-26 16:04:45,842 Epoch 99, Average loss: 13.1364, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 16:04:51,590 Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda1e-06\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 16:04:55,715 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 16:04:52.123913-#epoch100-#iter20-lambda1e-06'\n",
      "2018-05-26 16:04:55,717 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-26 16:04:52.123913-#epoch100-#iter20-lambda1e-06'\n",
      "2018-05-26 16:04:58,740 Start optimization\n",
      "2018-05-26 16:05:10,056 Epoch 0, Average loss: 36.7944, learning rate: 0.0003\n",
      "2018-05-26 16:05:22,687 Epoch 1, Average loss: 39.7351, learning rate: 0.0003\n",
      "2018-05-26 16:05:35,337 Epoch 2, Average loss: 33.0005, learning rate: 0.0003\n",
      "2018-05-26 16:05:48,251 Epoch 3, Average loss: 27.2898, learning rate: 0.0003\n",
      "2018-05-26 16:06:01,445 Epoch 4, Average loss: 26.5139, learning rate: 0.0003\n",
      "2018-05-26 16:06:14,818 Epoch 5, Average loss: 30.6003, learning rate: 0.0003\n",
      "2018-05-26 16:06:28,612 Epoch 6, Average loss: 18.7591, learning rate: 0.0003\n",
      "2018-05-26 16:06:42,533 Epoch 7, Average loss: 20.9967, learning rate: 0.0003\n",
      "2018-05-26 16:06:56,809 Epoch 8, Average loss: 19.4122, learning rate: 0.0003\n",
      "2018-05-26 16:07:11,298 Epoch 9, Average loss: 25.2785, learning rate: 0.0003\n",
      "2018-05-26 16:07:25,887 Epoch 10, Average loss: 26.1597, learning rate: 0.0003\n",
      "2018-05-26 16:07:43,592 Epoch 11, Average loss: 25.9046, learning rate: 0.0003\n",
      "2018-05-26 16:08:00,312 Epoch 12, Average loss: 19.8976, learning rate: 0.0003\n",
      "2018-05-26 16:08:17,189 Epoch 13, Average loss: 19.8197, learning rate: 0.0003\n",
      "2018-05-26 16:08:34,042 Epoch 14, Average loss: 17.2561, learning rate: 0.0003\n",
      "2018-05-26 16:08:51,374 Epoch 15, Average loss: 15.5065, learning rate: 0.0003\n",
      "2018-05-26 16:09:08,797 Epoch 16, Average loss: 14.0616, learning rate: 0.0003\n",
      "2018-05-26 16:09:26,448 Epoch 17, Average loss: 16.1499, learning rate: 0.0003\n",
      "2018-05-26 16:09:44,465 Epoch 18, Average loss: 20.0198, learning rate: 0.0003\n",
      "2018-05-26 16:10:01,277 Epoch 19, Average loss: 21.8311, learning rate: 0.0003\n",
      "2018-05-26 16:10:18,428 Epoch 20, Average loss: 18.9307, learning rate: 0.0003\n",
      "2018-05-26 16:10:35,894 Epoch 21, Average loss: 15.7648, learning rate: 0.0003\n",
      "2018-05-26 16:10:53,953 Epoch 22, Average loss: 18.1843, learning rate: 0.0003\n",
      "2018-05-26 16:11:12,499 Epoch 23, Average loss: 25.9829, learning rate: 0.0003\n",
      "2018-05-26 16:11:31,677 Epoch 24, Average loss: 21.1122, learning rate: 0.0003\n",
      "2018-05-26 16:11:51,049 Epoch 25, Average loss: 24.1276, learning rate: 0.0003\n",
      "2018-05-26 16:12:10,623 Epoch 26, Average loss: 16.4182, learning rate: 0.0003\n",
      "2018-05-26 16:12:30,188 Epoch 27, Average loss: 18.1457, learning rate: 0.0003\n",
      "2018-05-26 16:12:49,789 Epoch 28, Average loss: 22.8083, learning rate: 0.0003\n",
      "2018-05-26 16:13:10,092 Epoch 29, Average loss: 19.3632, learning rate: 0.0003\n",
      "2018-05-26 16:13:30,379 Epoch 30, Average loss: 9.0106, learning rate: 0.0003\n",
      "2018-05-26 16:13:51,202 Epoch 31, Average loss: 17.5919, learning rate: 0.0003\n",
      "2018-05-26 16:14:12,357 Epoch 32, Average loss: 18.2378, learning rate: 0.0003\n",
      "2018-05-26 16:14:34,206 Epoch 33, Average loss: 25.1273, learning rate: 0.0003\n",
      "2018-05-26 16:14:56,117 Epoch 34, Average loss: 23.4189, learning rate: 0.0003\n",
      "2018-05-26 16:15:18,455 Epoch 35, Average loss: 27.3160, learning rate: 0.0003\n",
      "2018-05-26 16:15:41,415 Epoch 36, Average loss: 17.0287, learning rate: 0.0003\n",
      "2018-05-26 16:16:04,013 Epoch 37, Average loss: 14.7826, learning rate: 0.0003\n",
      "2018-05-26 16:16:27,105 Epoch 38, Average loss: 21.0110, learning rate: 0.0003\n",
      "2018-05-26 16:16:51,734 Epoch 39, Average loss: 17.0381, learning rate: 0.0003\n",
      "2018-05-26 16:17:16,420 Epoch 40, Average loss: 14.7290, learning rate: 0.0003\n",
      "2018-05-26 16:17:40,782 Epoch 41, Average loss: 14.0945, learning rate: 0.0003\n",
      "2018-05-26 16:18:04,955 Epoch 42, Average loss: 20.1233, learning rate: 0.0003\n",
      "2018-05-26 16:18:30,236 Epoch 43, Average loss: 13.0409, learning rate: 0.0003\n",
      "2018-05-26 16:18:56,102 Epoch 44, Average loss: 11.4918, learning rate: 0.0003\n",
      "2018-05-26 16:19:21,286 Epoch 45, Average loss: 9.7660, learning rate: 0.0003\n",
      "2018-05-26 16:19:47,024 Epoch 46, Average loss: 10.2533, learning rate: 0.0003\n",
      "2018-05-26 16:20:12,829 Epoch 47, Average loss: 9.9796, learning rate: 0.0003\n",
      "2018-05-26 16:20:38,940 Epoch 48, Average loss: 15.6312, learning rate: 0.0003\n",
      "2018-05-26 16:21:05,414 Epoch 49, Average loss: 13.3804, learning rate: 0.0003\n",
      "2018-05-26 16:21:32,772 Epoch 50, Average loss: 12.7204, learning rate: 0.0003\n",
      "2018-05-26 16:22:00,186 Epoch 51, Average loss: 13.1253, learning rate: 0.0003\n",
      "2018-05-26 16:22:28,498 Epoch 52, Average loss: 15.2974, learning rate: 0.0003\n",
      "2018-05-26 16:22:56,926 Epoch 53, Average loss: 14.7365, learning rate: 0.0003\n",
      "2018-05-26 16:23:25,556 Epoch 54, Average loss: 11.2636, learning rate: 0.0003\n",
      "2018-05-26 16:23:54,677 Epoch 55, Average loss: 23.7275, learning rate: 0.0003\n",
      "2018-05-26 16:24:23,967 Epoch 56, Average loss: 15.7752, learning rate: 0.0003\n",
      "2018-05-26 16:24:53,179 Epoch 57, Average loss: 13.7387, learning rate: 0.0003\n",
      "2018-05-26 16:25:22,299 Epoch 58, Average loss: 18.2151, learning rate: 0.0003\n",
      "2018-05-26 16:25:52,001 Epoch 59, Average loss: 9.3928, learning rate: 0.0003\n",
      "2018-05-26 16:26:21,965 Epoch 60, Average loss: 18.8474, learning rate: 0.0003\n",
      "2018-05-26 16:26:52,448 Epoch 61, Average loss: 17.9016, learning rate: 0.0003\n",
      "2018-05-26 16:27:23,761 Epoch 62, Average loss: 13.3654, learning rate: 0.0003\n",
      "2018-05-26 16:27:55,317 Epoch 63, Average loss: 12.6614, learning rate: 0.0003\n",
      "2018-05-26 16:28:26,901 Epoch 64, Average loss: 26.5515, learning rate: 0.0003\n",
      "2018-05-26 16:28:58,404 Epoch 65, Average loss: 13.1155, learning rate: 0.0003\n",
      "2018-05-26 16:29:30,188 Epoch 66, Average loss: 10.2987, learning rate: 0.0003\n",
      "2018-05-26 16:30:02,417 Epoch 67, Average loss: 11.1277, learning rate: 0.0003\n",
      "2018-05-26 16:30:35,272 Epoch 68, Average loss: 10.1863, learning rate: 0.0003\n",
      "2018-05-26 16:31:08,674 Epoch 69, Average loss: 16.3155, learning rate: 0.0003\n",
      "2018-05-26 16:31:42,870 Epoch 70, Average loss: 11.2474, learning rate: 0.0003\n",
      "2018-05-26 16:32:17,709 Epoch 71, Average loss: 8.0638, learning rate: 0.0003\n",
      "2018-05-26 16:32:51,741 Epoch 72, Average loss: 19.0945, learning rate: 0.0003\n",
      "2018-05-26 16:33:25,821 Epoch 73, Average loss: 11.8037, learning rate: 0.0003\n",
      "2018-05-26 16:34:00,546 Epoch 74, Average loss: 15.2953, learning rate: 0.0003\n",
      "2018-05-26 16:34:35,379 Epoch 75, Average loss: 13.3573, learning rate: 0.0003\n",
      "2018-05-26 16:35:10,662 Epoch 76, Average loss: 13.8484, learning rate: 0.0003\n",
      "2018-05-26 16:35:46,500 Epoch 77, Average loss: 11.3151, learning rate: 0.0003\n",
      "2018-05-26 16:36:23,261 Epoch 78, Average loss: 11.3062, learning rate: 0.0003\n",
      "2018-05-26 16:36:59,801 Epoch 79, Average loss: 10.3922, learning rate: 0.0003\n",
      "2018-05-26 16:37:36,482 Epoch 80, Average loss: 9.7560, learning rate: 0.0003\n",
      "2018-05-26 16:38:13,206 Epoch 81, Average loss: 12.2347, learning rate: 0.0003\n",
      "2018-05-26 16:38:50,291 Epoch 82, Average loss: 9.6808, learning rate: 0.0003\n",
      "2018-05-26 16:39:28,275 Epoch 83, Average loss: 10.9099, learning rate: 0.0003\n",
      "2018-05-26 16:40:06,281 Epoch 84, Average loss: 13.6024, learning rate: 0.0003\n",
      "2018-05-26 16:40:44,712 Epoch 85, Average loss: 8.9707, learning rate: 0.0003\n",
      "2018-05-26 16:41:23,570 Epoch 86, Average loss: 14.5302, learning rate: 0.0003\n",
      "2018-05-26 16:42:02,820 Epoch 87, Average loss: 10.2939, learning rate: 0.0003\n",
      "2018-05-26 16:42:42,048 Epoch 88, Average loss: 10.8950, learning rate: 0.0003\n",
      "2018-05-26 16:43:22,435 Epoch 89, Average loss: 15.1170, learning rate: 0.0003\n",
      "2018-05-26 16:44:03,358 Epoch 90, Average loss: 9.3915, learning rate: 0.0003\n",
      "2018-05-26 16:44:43,882 Epoch 91, Average loss: 14.4497, learning rate: 0.0003\n",
      "2018-05-26 16:45:25,251 Epoch 92, Average loss: 12.0709, learning rate: 0.0003\n",
      "2018-05-26 16:46:06,221 Epoch 93, Average loss: 15.8225, learning rate: 0.0003\n",
      "2018-05-26 16:46:47,975 Epoch 94, Average loss: 10.7652, learning rate: 0.0003\n",
      "2018-05-26 16:47:29,925 Epoch 95, Average loss: 19.9802, learning rate: 0.0003\n",
      "2018-05-26 16:48:11,818 Epoch 96, Average loss: 7.4132, learning rate: 0.0003\n",
      "2018-05-26 16:48:53,953 Epoch 97, Average loss: 8.9979, learning rate: 0.0003\n",
      "2018-05-26 16:49:36,973 Epoch 98, Average loss: 12.3235, learning rate: 0.0003\n",
      "2018-05-26 16:50:19,778 Epoch 99, Average loss: 10.5345, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 16:50:25,461 Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from tf_unet import unet, util, image_util\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_iters = [20]\n",
    "num_epochs = [100]\n",
    "feat_roots = [32]\n",
    "num_layers = 3\n",
    "class_weight_tumor = [1]\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = [1]\n",
    "optimizer = \"adam\"\n",
    "\n",
    "\n",
    "reg_lambda = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "n_classes = 1\n",
    "\n",
    "#preparing data loading\n",
    "#data_provider = image_util.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/img/reduce_data/*\", data_suffix = \"t1ce.jpg\", mask_suffix = \"seg.jpg\")\n",
    "data_provider = image_util.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/reduced_flairdata/*\", data_suffix = \"flair_.jpg\", mask_suffix = \"seg_.jpg\")\n",
    "for reg_lambda_i in reg_lambda:\n",
    "    for feat_roots_i in feat_roots:\n",
    "        for batch_size_i in batch_size:\n",
    "            for train_iters_i in train_iters: \n",
    "                for num_epochs_i in num_epochs:\n",
    "                    output_path = \"./outputs/appro100/output{}-#epoch{}-#iter{}-lambda{}\".format(datetime.now(),num_epochs_i, train_iters_i, reg_lambda_i)\n",
    "                    #output_path = \"./outputs/output-fr{}-bs{}-lambda{}-clswei{}\".format(feat_roots_i,batch_size_i,reg_lambda_i, class_weight_tumor)\n",
    "                    print('Approach-- #epoch{}-#iter{}-lambda{}'.format(num_epochs_i, train_iters_i, reg_lambda_i))\n",
    "                    net = unet.Unet(layers=num_layers, features_root=feat_roots_i, channels=1, n_class=n_classes,  \n",
    "                                    cost = \"cross_entropy\", cost_kwargs=dict(regularizer=reg_lambda_i, \n",
    "                                                                             class_weights=[0.2,0.8]))\n",
    "                    trainer = unet.Trainer(net, optimizer=optimizer, batch_size = batch_size_i)\n",
    "                    trainer.train(data_provider, output_path, training_iters=train_iters_i, epochs=num_epochs_i, restore=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files used: 5000\n",
      "Approach-- #lambda0.001\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 22:27:37,172 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 22:27:33.604396-lambda0.001'\n",
      "2018-05-27 22:27:37,174 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 22:27:33.604396-lambda0.001'\n",
      "2018-05-27 22:27:37,673 Start optimization\n",
      "2018-05-27 22:27:47,898 Epoch 0, Average loss: 36.3210, learning rate: 0.0003\n",
      "2018-05-27 22:27:48,129 Verification error= 100.0%, loss= 27.7008\n",
      "2018-05-27 22:27:48,256 Iter 20, Minibatch Loss= 27.7008, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:28:00,777 Epoch 1, Average loss: 39.4439, learning rate: 0.0003\n",
      "2018-05-27 22:28:00,926 Verification error= 100.0%, loss= 25.1833\n",
      "2018-05-27 22:28:01,007 Iter 40, Minibatch Loss= 25.1833, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:28:14,131 Epoch 2, Average loss: 37.6694, learning rate: 0.0003\n",
      "2018-05-27 22:28:14,281 Verification error= 100.0%, loss= 21.9661\n",
      "2018-05-27 22:28:14,368 Iter 60, Minibatch Loss= 21.9661, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:28:27,793 Epoch 3, Average loss: 35.0550, learning rate: 0.0003\n",
      "2018-05-27 22:28:27,948 Verification error= 100.0%, loss= 19.5075\n",
      "2018-05-27 22:28:28,031 Iter 80, Minibatch Loss= 19.5075, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:28:43,936 Epoch 4, Average loss: 29.6020, learning rate: 0.0003\n",
      "2018-05-27 22:28:44,097 Verification error= 100.0%, loss= 11.3973\n",
      "2018-05-27 22:28:44,188 Iter 100, Minibatch Loss= 11.3973, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:28:58,867 Epoch 5, Average loss: 30.6446, learning rate: 0.0003\n",
      "2018-05-27 22:28:59,016 Verification error= 100.0%, loss= 24.0273\n",
      "2018-05-27 22:28:59,097 Iter 120, Minibatch Loss= 24.0273, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:29:13,956 Epoch 6, Average loss: 16.4097, learning rate: 0.0003\n",
      "2018-05-27 22:29:14,108 Verification error= 100.0%, loss= 6.1058\n",
      "2018-05-27 22:29:14,189 Iter 140, Minibatch Loss= 6.1058, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:29:29,234 Epoch 7, Average loss: 20.8936, learning rate: 0.0003\n",
      "2018-05-27 22:29:29,384 Verification error= 100.0%, loss= 9.3952\n",
      "2018-05-27 22:29:29,465 Iter 160, Minibatch Loss= 9.3952, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:29:44,660 Epoch 8, Average loss: 18.2719, learning rate: 0.0003\n",
      "2018-05-27 22:29:44,810 Verification error= 100.0%, loss= 8.6494\n",
      "2018-05-27 22:29:44,891 Iter 180, Minibatch Loss= 8.6494, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:30:00,326 Epoch 9, Average loss: 25.1639, learning rate: 0.0003\n",
      "2018-05-27 22:30:00,476 Verification error= 100.0%, loss= 5.4744\n",
      "2018-05-27 22:30:00,558 Iter 200, Minibatch Loss= 5.4744, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:30:16,230 Epoch 10, Average loss: 23.2907, learning rate: 0.0003\n",
      "2018-05-27 22:30:16,380 Verification error= 100.0%, loss= 5.9388\n",
      "2018-05-27 22:30:16,461 Iter 220, Minibatch Loss= 5.9388, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:30:32,240 Epoch 11, Average loss: 20.9151, learning rate: 0.0003\n",
      "2018-05-27 22:30:32,391 Verification error= 100.0%, loss= 4.3703\n",
      "2018-05-27 22:30:32,471 Iter 240, Minibatch Loss= 4.3703, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:30:48,722 Epoch 12, Average loss: 11.5386, learning rate: 0.0003\n",
      "2018-05-27 22:30:48,873 Verification error= 100.0%, loss= 3.2913\n",
      "2018-05-27 22:30:48,952 Iter 260, Minibatch Loss= 3.2913, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:31:05,307 Epoch 13, Average loss: 12.4788, learning rate: 0.0003\n",
      "2018-05-27 22:31:05,456 Verification error= 100.0%, loss= 3.7202\n",
      "2018-05-27 22:31:05,535 Iter 280, Minibatch Loss= 3.7202, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:31:22,194 Epoch 14, Average loss: 13.2224, learning rate: 0.0003\n",
      "2018-05-27 22:31:22,342 Verification error= 100.0%, loss= 3.0089\n",
      "2018-05-27 22:31:22,424 Iter 300, Minibatch Loss= 3.0089, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:31:39,350 Epoch 15, Average loss: 11.8133, learning rate: 0.0003\n",
      "2018-05-27 22:31:39,502 Verification error= 100.0%, loss= 5.1976\n",
      "2018-05-27 22:31:39,584 Iter 320, Minibatch Loss= 5.1976, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:31:56,695 Epoch 16, Average loss: 10.4445, learning rate: 0.0003\n",
      "2018-05-27 22:31:56,841 Verification error= 100.0%, loss= 5.3859\n",
      "2018-05-27 22:31:56,922 Iter 340, Minibatch Loss= 5.3859, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:32:14,855 Epoch 17, Average loss: 12.0334, learning rate: 0.0003\n",
      "2018-05-27 22:32:15,006 Verification error= 100.0%, loss= 5.6589\n",
      "2018-05-27 22:32:15,087 Iter 360, Minibatch Loss= 5.6589, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:32:32,441 Epoch 18, Average loss: 14.3079, learning rate: 0.0003\n",
      "2018-05-27 22:32:32,561 Verification error= 100.0%, loss= 4.0887\n",
      "2018-05-27 22:32:32,628 Iter 380, Minibatch Loss= 4.0887, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:32:49,739 Epoch 19, Average loss: 15.4397, learning rate: 0.0003\n",
      "2018-05-27 22:32:49,858 Verification error= 100.0%, loss= 4.4740\n",
      "2018-05-27 22:32:49,923 Iter 400, Minibatch Loss= 4.4740, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:33:06,963 Epoch 20, Average loss: 10.0970, learning rate: 0.0003\n",
      "2018-05-27 22:33:07,085 Verification error= 100.0%, loss= 4.6429\n",
      "2018-05-27 22:33:07,152 Iter 420, Minibatch Loss= 4.6429, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:33:24,620 Epoch 21, Average loss: 10.9226, learning rate: 0.0003\n",
      "2018-05-27 22:33:24,739 Verification error= 100.0%, loss= 3.2457\n",
      "2018-05-27 22:33:24,804 Iter 440, Minibatch Loss= 3.2457, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:33:43,251 Epoch 22, Average loss: 12.9798, learning rate: 0.0003\n",
      "2018-05-27 22:33:43,375 Verification error= 100.0%, loss= 3.5594\n",
      "2018-05-27 22:33:43,443 Iter 460, Minibatch Loss= 3.5594, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:34:02,085 Epoch 23, Average loss: 22.0217, learning rate: 0.0003\n",
      "2018-05-27 22:34:02,201 Verification error= 100.0%, loss= 4.9319\n",
      "2018-05-27 22:34:02,268 Iter 480, Minibatch Loss= 4.9319, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:34:21,137 Epoch 24, Average loss: 10.2353, learning rate: 0.0003\n",
      "2018-05-27 22:34:21,262 Verification error= 100.0%, loss= 2.9401\n",
      "2018-05-27 22:34:21,333 Iter 500, Minibatch Loss= 2.9401, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:34:40,388 Epoch 25, Average loss: 22.1909, learning rate: 0.0003\n",
      "2018-05-27 22:34:40,507 Verification error= 100.0%, loss= 4.5988\n",
      "2018-05-27 22:34:40,571 Iter 520, Minibatch Loss= 4.5988, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:34:59,708 Epoch 26, Average loss: 12.8502, learning rate: 0.0003\n",
      "2018-05-27 22:34:59,826 Verification error= 100.0%, loss= 7.6354\n",
      "2018-05-27 22:34:59,890 Iter 540, Minibatch Loss= 7.6354, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:35:19,553 Epoch 27, Average loss: 13.3073, learning rate: 0.0003\n",
      "2018-05-27 22:35:19,674 Verification error= 100.0%, loss= 3.7727\n",
      "2018-05-27 22:35:19,741 Iter 560, Minibatch Loss= 3.7727, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:35:39,614 Epoch 28, Average loss: 21.7522, learning rate: 0.0003\n",
      "2018-05-27 22:35:39,732 Verification error= 100.0%, loss= 9.9128\n",
      "2018-05-27 22:35:39,797 Iter 580, Minibatch Loss= 9.9128, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:36:00,733 Epoch 29, Average loss: 18.5238, learning rate: 0.0003\n",
      "2018-05-27 22:36:00,850 Verification error= 100.0%, loss= 3.4171\n",
      "2018-05-27 22:36:00,912 Iter 600, Minibatch Loss= 3.4171, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:36:22,198 Epoch 30, Average loss: 7.2635, learning rate: 0.0003\n",
      "2018-05-27 22:36:22,324 Verification error= 100.0%, loss= 2.7179\n",
      "2018-05-27 22:36:22,390 Iter 620, Minibatch Loss= 2.7179, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:36:43,844 Epoch 31, Average loss: 14.7631, learning rate: 0.0003\n",
      "2018-05-27 22:36:43,970 Verification error= 100.0%, loss= 3.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 22:36:44,037 Iter 640, Minibatch Loss= 3.9923, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:37:05,877 Epoch 32, Average loss: 15.7441, learning rate: 0.0003\n",
      "2018-05-27 22:37:05,994 Verification error= 100.0%, loss= 3.3227\n",
      "2018-05-27 22:37:06,058 Iter 660, Minibatch Loss= 3.3227, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:37:27,472 Epoch 33, Average loss: 18.6816, learning rate: 0.0003\n",
      "2018-05-27 22:37:27,589 Verification error= 100.0%, loss= 3.7640\n",
      "2018-05-27 22:37:27,653 Iter 680, Minibatch Loss= 3.7640, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:37:50,681 Epoch 34, Average loss: 21.5337, learning rate: 0.0003\n",
      "2018-05-27 22:37:50,805 Verification error= 100.0%, loss= 5.5318\n",
      "2018-05-27 22:37:50,872 Iter 700, Minibatch Loss= 5.5318, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:38:13,286 Epoch 35, Average loss: 15.6196, learning rate: 0.0003\n",
      "2018-05-27 22:38:13,405 Verification error= 100.0%, loss= 5.8330\n",
      "2018-05-27 22:38:13,473 Iter 720, Minibatch Loss= 5.8330, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:38:36,194 Epoch 36, Average loss: 10.3955, learning rate: 0.0003\n",
      "2018-05-27 22:38:36,315 Verification error= 100.0%, loss= 5.2375\n",
      "2018-05-27 22:38:36,380 Iter 740, Minibatch Loss= 5.2375, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:38:59,299 Epoch 37, Average loss: 9.3247, learning rate: 0.0003\n",
      "2018-05-27 22:38:59,423 Verification error= 100.0%, loss= 5.0984\n",
      "2018-05-27 22:38:59,490 Iter 760, Minibatch Loss= 5.0984, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:39:23,576 Epoch 38, Average loss: 15.0617, learning rate: 0.0003\n",
      "2018-05-27 22:39:23,694 Verification error= 100.0%, loss= 5.1049\n",
      "2018-05-27 22:39:23,759 Iter 780, Minibatch Loss= 5.1049, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:39:47,353 Epoch 39, Average loss: 14.6608, learning rate: 0.0003\n",
      "2018-05-27 22:39:47,477 Verification error= 100.0%, loss= 6.6175\n",
      "2018-05-27 22:39:47,542 Iter 800, Minibatch Loss= 6.6175, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:40:11,760 Epoch 40, Average loss: 12.9062, learning rate: 0.0003\n",
      "2018-05-27 22:40:11,879 Verification error= 100.0%, loss= 2.9763\n",
      "2018-05-27 22:40:11,946 Iter 820, Minibatch Loss= 2.9763, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:40:36,709 Epoch 41, Average loss: 12.5146, learning rate: 0.0003\n",
      "2018-05-27 22:40:36,829 Verification error= 100.0%, loss= 7.8127\n",
      "2018-05-27 22:40:36,895 Iter 840, Minibatch Loss= 7.8127, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:41:01,780 Epoch 42, Average loss: 16.4185, learning rate: 0.0003\n",
      "2018-05-27 22:41:01,897 Verification error= 100.0%, loss= 3.2829\n",
      "2018-05-27 22:41:01,963 Iter 860, Minibatch Loss= 3.2829, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:41:26,916 Epoch 43, Average loss: 9.6214, learning rate: 0.0003\n",
      "2018-05-27 22:41:27,036 Verification error= 100.0%, loss= 9.4980\n",
      "2018-05-27 22:41:27,100 Iter 880, Minibatch Loss= 9.4980, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:41:52,344 Epoch 44, Average loss: 10.0814, learning rate: 0.0003\n",
      "2018-05-27 22:41:52,462 Verification error= 100.0%, loss= 3.3366\n",
      "2018-05-27 22:41:52,528 Iter 900, Minibatch Loss= 3.3366, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:42:17,921 Epoch 45, Average loss: 9.7590, learning rate: 0.0003\n",
      "2018-05-27 22:42:18,043 Verification error= 100.0%, loss= 2.7858\n",
      "2018-05-27 22:42:18,106 Iter 920, Minibatch Loss= 2.7858, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:42:45,150 Epoch 46, Average loss: 10.5456, learning rate: 0.0003\n",
      "2018-05-27 22:42:45,273 Verification error= 100.0%, loss= 5.2674\n",
      "2018-05-27 22:42:45,342 Iter 940, Minibatch Loss= 5.2674, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:43:12,021 Epoch 47, Average loss: 11.4058, learning rate: 0.0003\n",
      "2018-05-27 22:43:12,142 Verification error= 100.0%, loss= 6.4432\n",
      "2018-05-27 22:43:12,211 Iter 960, Minibatch Loss= 6.4432, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:43:39,191 Epoch 48, Average loss: 14.8423, learning rate: 0.0003\n",
      "2018-05-27 22:43:39,313 Verification error= 100.0%, loss= 2.9268\n",
      "2018-05-27 22:43:39,382 Iter 980, Minibatch Loss= 2.9268, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:44:06,813 Epoch 49, Average loss: 10.8983, learning rate: 0.0003\n",
      "2018-05-27 22:44:06,936 Verification error= 100.0%, loss= 5.9629\n",
      "2018-05-27 22:44:07,001 Iter 1000, Minibatch Loss= 5.9629, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:44:34,814 Epoch 50, Average loss: 11.6562, learning rate: 0.0003\n",
      "2018-05-27 22:44:34,933 Verification error= 100.0%, loss= 2.9541\n",
      "2018-05-27 22:44:35,001 Iter 1020, Minibatch Loss= 2.9541, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:45:02,977 Epoch 51, Average loss: 13.6217, learning rate: 0.0003\n",
      "2018-05-27 22:45:03,096 Verification error= 100.0%, loss= 2.5953\n",
      "2018-05-27 22:45:03,161 Iter 1040, Minibatch Loss= 2.5953, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:45:31,169 Epoch 52, Average loss: 18.7018, learning rate: 0.0003\n",
      "2018-05-27 22:45:31,289 Verification error= 100.0%, loss= 3.4196\n",
      "2018-05-27 22:45:31,355 Iter 1060, Minibatch Loss= 3.4196, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:45:59,963 Epoch 53, Average loss: 9.9827, learning rate: 0.0003\n",
      "2018-05-27 22:46:00,084 Verification error= 100.0%, loss= 2.8210\n",
      "2018-05-27 22:46:00,156 Iter 1080, Minibatch Loss= 2.8210, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:46:29,393 Epoch 54, Average loss: 9.7017, learning rate: 0.0003\n",
      "2018-05-27 22:46:29,521 Verification error= 100.0%, loss= 2.8248\n",
      "2018-05-27 22:46:29,590 Iter 1100, Minibatch Loss= 2.8248, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:46:59,440 Epoch 55, Average loss: 18.0391, learning rate: 0.0003\n",
      "2018-05-27 22:46:59,565 Verification error= 100.0%, loss= 2.9679\n",
      "2018-05-27 22:46:59,633 Iter 1120, Minibatch Loss= 2.9679, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:47:29,318 Epoch 56, Average loss: 10.6034, learning rate: 0.0003\n",
      "2018-05-27 22:47:29,444 Verification error= 100.0%, loss= 2.9737\n",
      "2018-05-27 22:47:29,513 Iter 1140, Minibatch Loss= 2.9737, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:47:59,869 Epoch 57, Average loss: 11.7643, learning rate: 0.0003\n",
      "2018-05-27 22:48:00,000 Verification error= 100.0%, loss= 11.0388\n",
      "2018-05-27 22:48:00,069 Iter 1160, Minibatch Loss= 11.0388, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:48:30,309 Epoch 58, Average loss: 18.0351, learning rate: 0.0003\n",
      "2018-05-27 22:48:30,428 Verification error= 100.0%, loss= 8.9392\n",
      "2018-05-27 22:48:30,491 Iter 1180, Minibatch Loss= 8.9392, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:49:00,408 Epoch 59, Average loss: 11.0257, learning rate: 0.0003\n",
      "2018-05-27 22:49:00,529 Verification error= 100.0%, loss= 4.5000\n",
      "2018-05-27 22:49:00,597 Iter 1200, Minibatch Loss= 4.5000, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:49:31,748 Epoch 60, Average loss: 18.6574, learning rate: 0.0003\n",
      "2018-05-27 22:49:31,868 Verification error= 100.0%, loss= 3.7135\n",
      "2018-05-27 22:49:31,936 Iter 1220, Minibatch Loss= 3.7135, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:50:03,413 Epoch 61, Average loss: 19.4964, learning rate: 0.0003\n",
      "2018-05-27 22:50:03,536 Verification error= 100.0%, loss= 4.7720\n",
      "2018-05-27 22:50:03,605 Iter 1240, Minibatch Loss= 4.7720, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:50:35,432 Epoch 62, Average loss: 15.0627, learning rate: 0.0003\n",
      "2018-05-27 22:50:35,556 Verification error= 100.0%, loss= 5.8045\n",
      "2018-05-27 22:50:35,624 Iter 1260, Minibatch Loss= 5.8045, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:51:07,866 Epoch 63, Average loss: 11.6399, learning rate: 0.0003\n",
      "2018-05-27 22:51:07,990 Verification error= 100.0%, loss= 2.9798\n",
      "2018-05-27 22:51:08,058 Iter 1280, Minibatch Loss= 2.9798, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:51:41,233 Epoch 64, Average loss: 26.1251, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 22:51:41,362 Verification error= 100.0%, loss= 4.1912\n",
      "2018-05-27 22:51:41,433 Iter 1300, Minibatch Loss= 4.1912, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:52:14,706 Epoch 65, Average loss: 12.2867, learning rate: 0.0003\n",
      "2018-05-27 22:52:14,824 Verification error= 100.0%, loss= 2.9339\n",
      "2018-05-27 22:52:14,890 Iter 1320, Minibatch Loss= 2.9339, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:52:47,450 Epoch 66, Average loss: 9.0058, learning rate: 0.0003\n",
      "2018-05-27 22:52:47,570 Verification error= 100.0%, loss= 2.7105\n",
      "2018-05-27 22:52:47,635 Iter 1340, Minibatch Loss= 2.7105, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:53:21,107 Epoch 67, Average loss: 12.0616, learning rate: 0.0003\n",
      "2018-05-27 22:53:21,231 Verification error= 100.0%, loss= 4.7973\n",
      "2018-05-27 22:53:21,294 Iter 1360, Minibatch Loss= 4.7973, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:53:54,041 Epoch 68, Average loss: 11.4108, learning rate: 0.0003\n",
      "2018-05-27 22:53:54,159 Verification error= 100.0%, loss= 3.0008\n",
      "2018-05-27 22:53:54,223 Iter 1380, Minibatch Loss= 3.0008, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:54:27,669 Epoch 69, Average loss: 12.2882, learning rate: 0.0003\n",
      "2018-05-27 22:54:27,787 Verification error= 100.0%, loss= 2.2957\n",
      "2018-05-27 22:54:27,852 Iter 1400, Minibatch Loss= 2.2957, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:55:01,170 Epoch 70, Average loss: 8.0604, learning rate: 0.0003\n",
      "2018-05-27 22:55:01,289 Verification error= 100.0%, loss= 4.1126\n",
      "2018-05-27 22:55:01,354 Iter 1420, Minibatch Loss= 4.1126, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:55:35,689 Epoch 71, Average loss: 7.1271, learning rate: 0.0003\n",
      "2018-05-27 22:55:35,811 Verification error= 100.0%, loss= 3.6111\n",
      "2018-05-27 22:55:35,877 Iter 1440, Minibatch Loss= 3.6111, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:56:11,577 Epoch 72, Average loss: 19.7822, learning rate: 0.0003\n",
      "2018-05-27 22:56:11,702 Verification error= 100.0%, loss= 3.3676\n",
      "2018-05-27 22:56:11,769 Iter 1460, Minibatch Loss= 3.3676, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:56:47,021 Epoch 73, Average loss: 11.2175, learning rate: 0.0003\n",
      "2018-05-27 22:56:47,139 Verification error= 100.0%, loss= 3.3015\n",
      "2018-05-27 22:56:47,205 Iter 1480, Minibatch Loss= 3.3015, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:57:22,548 Epoch 74, Average loss: 16.0016, learning rate: 0.0003\n",
      "2018-05-27 22:57:22,674 Verification error= 100.0%, loss= 3.6531\n",
      "2018-05-27 22:57:22,739 Iter 1500, Minibatch Loss= 3.6531, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:57:58,195 Epoch 75, Average loss: 16.2933, learning rate: 0.0003\n",
      "2018-05-27 22:57:58,313 Verification error= 100.0%, loss= 2.9469\n",
      "2018-05-27 22:57:58,379 Iter 1520, Minibatch Loss= 2.9469, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:58:34,125 Epoch 76, Average loss: 8.9496, learning rate: 0.0003\n",
      "2018-05-27 22:58:34,238 Verification error= 100.0%, loss= 3.7623\n",
      "2018-05-27 22:58:34,302 Iter 1540, Minibatch Loss= 3.7623, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:59:10,618 Epoch 77, Average loss: 10.6021, learning rate: 0.0003\n",
      "2018-05-27 22:59:10,747 Verification error= 100.0%, loss= 2.5935\n",
      "2018-05-27 22:59:10,816 Iter 1560, Minibatch Loss= 2.5935, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 22:59:47,402 Epoch 78, Average loss: 11.3793, learning rate: 0.0003\n",
      "2018-05-27 22:59:47,521 Verification error= 100.0%, loss= 3.7630\n",
      "2018-05-27 22:59:47,586 Iter 1580, Minibatch Loss= 3.7630, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:00:24,368 Epoch 79, Average loss: 8.5545, learning rate: 0.0003\n",
      "2018-05-27 23:00:24,488 Verification error= 100.0%, loss= 3.9248\n",
      "2018-05-27 23:00:24,552 Iter 1600, Minibatch Loss= 3.9248, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:01:02,396 Epoch 80, Average loss: 10.5715, learning rate: 0.0003\n",
      "2018-05-27 23:01:02,520 Verification error= 100.0%, loss= 2.7253\n",
      "2018-05-27 23:01:02,589 Iter 1620, Minibatch Loss= 2.7253, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:01:39,989 Epoch 81, Average loss: 10.5249, learning rate: 0.0003\n",
      "2018-05-27 23:01:40,113 Verification error= 100.0%, loss= 4.3969\n",
      "2018-05-27 23:01:40,182 Iter 1640, Minibatch Loss= 4.3969, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:02:18,825 Epoch 82, Average loss: 10.6623, learning rate: 0.0003\n",
      "2018-05-27 23:02:18,945 Verification error= 100.0%, loss= 3.9803\n",
      "2018-05-27 23:02:19,012 Iter 1660, Minibatch Loss= 3.9803, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:02:57,867 Epoch 83, Average loss: 13.6108, learning rate: 0.0003\n",
      "2018-05-27 23:02:57,991 Verification error= 100.0%, loss= 5.8444\n",
      "2018-05-27 23:02:58,058 Iter 1680, Minibatch Loss= 5.8444, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:03:36,386 Epoch 84, Average loss: 13.6003, learning rate: 0.0003\n",
      "2018-05-27 23:03:36,501 Verification error= 100.0%, loss= 2.9333\n",
      "2018-05-27 23:03:36,565 Iter 1700, Minibatch Loss= 2.9333, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:04:15,641 Epoch 85, Average loss: 7.1511, learning rate: 0.0003\n",
      "2018-05-27 23:04:15,755 Verification error= 100.0%, loss= 2.8104\n",
      "2018-05-27 23:04:15,819 Iter 1720, Minibatch Loss= 2.8104, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:04:55,498 Epoch 86, Average loss: 14.8446, learning rate: 0.0003\n",
      "2018-05-27 23:04:55,622 Verification error= 100.0%, loss= 3.3991\n",
      "2018-05-27 23:04:55,690 Iter 1740, Minibatch Loss= 3.3991, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:05:35,803 Epoch 87, Average loss: 9.1273, learning rate: 0.0003\n",
      "2018-05-27 23:05:35,925 Verification error= 100.0%, loss= 3.5658\n",
      "2018-05-27 23:05:35,993 Iter 1760, Minibatch Loss= 3.5658, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:06:15,995 Epoch 88, Average loss: 15.3529, learning rate: 0.0003\n",
      "2018-05-27 23:06:16,119 Verification error= 100.0%, loss= 2.5088\n",
      "2018-05-27 23:06:16,186 Iter 1780, Minibatch Loss= 2.5088, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:06:56,933 Epoch 89, Average loss: 8.8801, learning rate: 0.0003\n",
      "2018-05-27 23:06:57,057 Verification error= 100.0%, loss= 2.9738\n",
      "2018-05-27 23:06:57,124 Iter 1800, Minibatch Loss= 2.9738, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:07:38,027 Epoch 90, Average loss: 8.2952, learning rate: 0.0003\n",
      "2018-05-27 23:07:38,146 Verification error= 100.0%, loss= 5.5195\n",
      "2018-05-27 23:07:38,213 Iter 1820, Minibatch Loss= 5.5195, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:08:19,401 Epoch 91, Average loss: 15.6865, learning rate: 0.0003\n",
      "2018-05-27 23:08:19,526 Verification error= 100.0%, loss= 2.7608\n",
      "2018-05-27 23:08:19,592 Iter 1840, Minibatch Loss= 2.7608, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:09:00,945 Epoch 92, Average loss: 11.5564, learning rate: 0.0003\n",
      "2018-05-27 23:09:01,077 Verification error= 100.0%, loss= 2.3258\n",
      "2018-05-27 23:09:01,147 Iter 1860, Minibatch Loss= 2.3258, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:09:42,999 Epoch 93, Average loss: 15.1004, learning rate: 0.0003\n",
      "2018-05-27 23:09:43,121 Verification error= 100.0%, loss= 2.8078\n",
      "2018-05-27 23:09:43,189 Iter 1880, Minibatch Loss= 2.8078, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:10:25,866 Epoch 94, Average loss: 9.7120, learning rate: 0.0003\n",
      "2018-05-27 23:10:25,987 Verification error= 100.0%, loss= 3.1898\n",
      "2018-05-27 23:10:26,054 Iter 1900, Minibatch Loss= 3.1898, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:11:08,682 Epoch 95, Average loss: 22.9541, learning rate: 0.0003\n",
      "2018-05-27 23:11:08,805 Verification error= 100.0%, loss= 3.1684\n",
      "2018-05-27 23:11:08,871 Iter 1920, Minibatch Loss= 3.1684, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:11:51,116 Epoch 96, Average loss: 8.0362, learning rate: 0.0003\n",
      "2018-05-27 23:11:51,233 Verification error= 100.0%, loss= 2.6928\n",
      "2018-05-27 23:11:51,298 Iter 1940, Minibatch Loss= 2.6928, Training Accuracy= 1.0000, Minibatch error= 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 23:12:33,810 Epoch 97, Average loss: 8.5081, learning rate: 0.0003\n",
      "2018-05-27 23:12:33,926 Verification error= 100.0%, loss= 2.7302\n",
      "2018-05-27 23:12:33,991 Iter 1960, Minibatch Loss= 2.7302, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:13:17,257 Epoch 98, Average loss: 9.3687, learning rate: 0.0003\n",
      "2018-05-27 23:13:17,375 Verification error= 100.0%, loss= 3.2096\n",
      "2018-05-27 23:13:17,437 Iter 1980, Minibatch Loss= 3.2096, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:14:00,506 Epoch 99, Average loss: 11.2370, learning rate: 0.0003\n",
      "2018-05-27 23:14:00,622 Verification error= 100.0%, loss= 3.0038\n",
      "2018-05-27 23:14:00,687 Iter 2000, Minibatch Loss= 3.0038, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:14:06,446 Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #lambda0.0001\n",
      "1.8.0\n",
      "Track_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 23:14:10,671 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 23:14:06.982890-lambda0.0001'\n",
      "2018-05-27 23:14:10,672 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 23:14:06.982890-lambda0.0001'\n",
      "2018-05-27 23:14:11,163 Start optimization\n",
      "2018-05-27 23:14:21,519 Epoch 0, Average loss: 45.6905, learning rate: 0.0003\n",
      "2018-05-27 23:14:21,754 Verification error= 100.0%, loss= 30.7339\n",
      "2018-05-27 23:14:21,885 Iter 20, Minibatch Loss= 30.7339, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:14:33,132 Epoch 1, Average loss: 30.5554, learning rate: 0.0003\n",
      "2018-05-27 23:14:33,250 Verification error= 100.0%, loss= 24.2782\n",
      "2018-05-27 23:14:33,319 Iter 40, Minibatch Loss= 24.2782, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:14:44,634 Epoch 2, Average loss: 22.6546, learning rate: 0.0003\n",
      "2018-05-27 23:14:44,750 Verification error= 100.0%, loss= 21.1908\n",
      "2018-05-27 23:14:44,817 Iter 60, Minibatch Loss= 21.1908, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:14:56,457 Epoch 3, Average loss: 22.3177, learning rate: 0.0003\n",
      "2018-05-27 23:14:56,576 Verification error= 100.0%, loss= 17.5926\n",
      "2018-05-27 23:14:56,641 Iter 80, Minibatch Loss= 17.5926, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:15:08,565 Epoch 4, Average loss: 27.6050, learning rate: 0.0003\n",
      "2018-05-27 23:15:08,687 Verification error= 100.0%, loss= 21.7488\n",
      "2018-05-27 23:15:08,754 Iter 100, Minibatch Loss= 21.7488, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:15:20,854 Epoch 5, Average loss: 20.3888, learning rate: 0.0003\n",
      "2018-05-27 23:15:20,972 Verification error= 100.0%, loss= 16.7703\n",
      "2018-05-27 23:15:21,038 Iter 120, Minibatch Loss= 16.7703, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:15:33,512 Epoch 6, Average loss: 14.5877, learning rate: 0.0003\n",
      "2018-05-27 23:15:33,626 Verification error= 100.0%, loss= 16.5327\n",
      "2018-05-27 23:15:33,694 Iter 140, Minibatch Loss= 16.5327, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:15:46,422 Epoch 7, Average loss: 21.6668, learning rate: 0.0003\n",
      "2018-05-27 23:15:46,540 Verification error= 100.0%, loss= 19.1726\n",
      "2018-05-27 23:15:46,605 Iter 160, Minibatch Loss= 19.1726, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:16:02,281 Epoch 8, Average loss: 20.8783, learning rate: 0.0003\n",
      "2018-05-27 23:16:02,399 Verification error= 100.0%, loss= 18.4822\n",
      "2018-05-27 23:16:02,464 Iter 180, Minibatch Loss= 18.4822, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:16:17,033 Epoch 9, Average loss: 21.0728, learning rate: 0.0003\n",
      "2018-05-27 23:16:17,150 Verification error= 100.0%, loss= 14.6398\n",
      "2018-05-27 23:16:17,215 Iter 200, Minibatch Loss= 14.6398, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:16:32,721 Epoch 10, Average loss: 25.9243, learning rate: 0.0003\n",
      "2018-05-27 23:16:32,872 Verification error= 100.0%, loss= 18.7074\n",
      "2018-05-27 23:16:32,958 Iter 220, Minibatch Loss= 18.7074, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:16:49,657 Epoch 11, Average loss: 22.1760, learning rate: 0.0003\n",
      "2018-05-27 23:16:49,807 Verification error= 100.0%, loss= 16.2624\n",
      "2018-05-27 23:16:49,891 Iter 240, Minibatch Loss= 16.2624, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:17:06,988 Epoch 12, Average loss: 21.9294, learning rate: 0.0003\n",
      "2018-05-27 23:17:07,139 Verification error= 100.0%, loss= 19.6173\n",
      "2018-05-27 23:17:07,222 Iter 260, Minibatch Loss= 19.6173, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:17:24,505 Epoch 13, Average loss: 22.0863, learning rate: 0.0003\n",
      "2018-05-27 23:17:24,681 Verification error= 100.0%, loss= 18.9881\n",
      "2018-05-27 23:17:24,776 Iter 280, Minibatch Loss= 18.9881, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:17:40,790 Epoch 14, Average loss: 17.2102, learning rate: 0.0003\n",
      "2018-05-27 23:17:40,908 Verification error= 100.0%, loss= 17.7519\n",
      "2018-05-27 23:17:40,973 Iter 300, Minibatch Loss= 17.7519, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:17:57,013 Epoch 15, Average loss: 16.1165, learning rate: 0.0003\n",
      "2018-05-27 23:17:57,132 Verification error= 100.0%, loss= 18.2201\n",
      "2018-05-27 23:17:57,198 Iter 320, Minibatch Loss= 18.2201, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:18:13,731 Epoch 16, Average loss: 24.7955, learning rate: 0.0003\n",
      "2018-05-27 23:18:13,857 Verification error= 100.0%, loss= 16.6744\n",
      "2018-05-27 23:18:13,925 Iter 340, Minibatch Loss= 16.6744, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:18:31,185 Epoch 17, Average loss: 17.0028, learning rate: 0.0003\n",
      "2018-05-27 23:18:31,305 Verification error= 100.0%, loss= 18.3210\n",
      "2018-05-27 23:18:31,368 Iter 360, Minibatch Loss= 18.3210, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:18:48,159 Epoch 18, Average loss: 19.5460, learning rate: 0.0003\n",
      "2018-05-27 23:18:48,276 Verification error= 100.0%, loss= 17.2327\n",
      "2018-05-27 23:18:48,345 Iter 380, Minibatch Loss= 17.2327, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:19:06,356 Epoch 19, Average loss: 22.0587, learning rate: 0.0003\n",
      "2018-05-27 23:19:06,478 Verification error= 100.0%, loss= 14.5497\n",
      "2018-05-27 23:19:06,548 Iter 400, Minibatch Loss= 14.5497, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:19:24,880 Epoch 20, Average loss: 15.1827, learning rate: 0.0003\n",
      "2018-05-27 23:19:25,003 Verification error= 100.0%, loss= 15.0969\n",
      "2018-05-27 23:19:25,071 Iter 420, Minibatch Loss= 15.0969, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:19:43,755 Epoch 21, Average loss: 14.4985, learning rate: 0.0003\n",
      "2018-05-27 23:19:43,877 Verification error= 100.0%, loss= 9.1748\n",
      "2018-05-27 23:19:43,948 Iter 440, Minibatch Loss= 9.1748, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:20:02,603 Epoch 22, Average loss: 19.8107, learning rate: 0.0003\n",
      "2018-05-27 23:20:02,729 Verification error= 100.0%, loss= 17.4082\n",
      "2018-05-27 23:20:02,795 Iter 460, Minibatch Loss= 17.4082, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:20:22,137 Epoch 23, Average loss: 21.9277, learning rate: 0.0003\n",
      "2018-05-27 23:20:22,255 Verification error= 100.0%, loss= 16.1175\n",
      "2018-05-27 23:20:22,322 Iter 480, Minibatch Loss= 16.1175, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:20:41,801 Epoch 24, Average loss: 10.6250, learning rate: 0.0003\n",
      "2018-05-27 23:20:41,932 Verification error= 100.0%, loss= 14.4377\n",
      "2018-05-27 23:20:42,000 Iter 500, Minibatch Loss= 14.4377, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:21:01,537 Epoch 25, Average loss: 21.3518, learning rate: 0.0003\n",
      "2018-05-27 23:21:01,656 Verification error= 100.0%, loss= 14.3117\n",
      "2018-05-27 23:21:01,722 Iter 520, Minibatch Loss= 14.3117, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:21:22,210 Epoch 26, Average loss: 15.2045, learning rate: 0.0003\n",
      "2018-05-27 23:21:22,334 Verification error= 100.0%, loss= 7.9103\n",
      "2018-05-27 23:21:22,402 Iter 540, Minibatch Loss= 7.9103, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:21:43,167 Epoch 27, Average loss: 24.3729, learning rate: 0.0003\n",
      "2018-05-27 23:21:43,283 Verification error= 100.0%, loss= 18.0539\n",
      "2018-05-27 23:21:43,348 Iter 560, Minibatch Loss= 18.0539, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:22:03,517 Epoch 28, Average loss: 19.3104, learning rate: 0.0003\n",
      "2018-05-27 23:22:03,638 Verification error= 100.0%, loss= 13.6647\n",
      "2018-05-27 23:22:03,704 Iter 580, Minibatch Loss= 13.6647, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:22:24,338 Epoch 29, Average loss: 16.1073, learning rate: 0.0003\n",
      "2018-05-27 23:22:24,461 Verification error= 100.0%, loss= 6.8277\n",
      "2018-05-27 23:22:24,531 Iter 600, Minibatch Loss= 6.8277, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:22:45,805 Epoch 30, Average loss: 8.9480, learning rate: 0.0003\n",
      "2018-05-27 23:22:45,928 Verification error= 100.0%, loss= 11.9823\n",
      "2018-05-27 23:22:45,995 Iter 620, Minibatch Loss= 11.9823, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:23:07,505 Epoch 31, Average loss: 11.8406, learning rate: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 23:23:07,629 Verification error= 100.0%, loss= 8.8245\n",
      "2018-05-27 23:23:07,697 Iter 640, Minibatch Loss= 8.8245, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:23:31,417 Epoch 32, Average loss: 15.4938, learning rate: 0.0003\n",
      "2018-05-27 23:23:31,544 Verification error= 100.0%, loss= 12.9079\n",
      "2018-05-27 23:23:31,613 Iter 660, Minibatch Loss= 12.9079, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:23:55,202 Epoch 33, Average loss: 17.6076, learning rate: 0.0003\n",
      "2018-05-27 23:23:55,331 Verification error= 100.0%, loss= 16.4230\n",
      "2018-05-27 23:23:55,403 Iter 680, Minibatch Loss= 16.4230, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:24:18,718 Epoch 34, Average loss: 20.2624, learning rate: 0.0003\n",
      "2018-05-27 23:24:18,835 Verification error= 100.0%, loss= 8.3645\n",
      "2018-05-27 23:24:18,901 Iter 700, Minibatch Loss= 8.3645, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:24:43,849 Epoch 35, Average loss: 12.6315, learning rate: 0.0003\n",
      "2018-05-27 23:24:43,980 Verification error= 100.0%, loss= 6.2573\n",
      "2018-05-27 23:24:44,054 Iter 720, Minibatch Loss= 6.2573, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:25:08,045 Epoch 36, Average loss: 19.5180, learning rate: 0.0003\n",
      "2018-05-27 23:25:08,170 Verification error= 100.0%, loss= 5.3153\n",
      "2018-05-27 23:25:08,238 Iter 740, Minibatch Loss= 5.3153, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:25:32,001 Epoch 37, Average loss: 10.2469, learning rate: 0.0003\n",
      "2018-05-27 23:25:32,119 Verification error= 100.0%, loss= 3.8704\n",
      "2018-05-27 23:25:32,187 Iter 760, Minibatch Loss= 3.8704, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:25:56,078 Epoch 38, Average loss: 15.0699, learning rate: 0.0003\n",
      "2018-05-27 23:25:56,201 Verification error= 100.0%, loss= 8.2420\n",
      "2018-05-27 23:25:56,273 Iter 780, Minibatch Loss= 8.2420, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:26:20,857 Epoch 39, Average loss: 15.7327, learning rate: 0.0003\n",
      "2018-05-27 23:26:20,982 Verification error= 100.0%, loss= 7.9640\n",
      "2018-05-27 23:26:21,050 Iter 800, Minibatch Loss= 7.9640, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:26:45,561 Epoch 40, Average loss: 14.9474, learning rate: 0.0003\n",
      "2018-05-27 23:26:45,686 Verification error= 100.0%, loss= 7.3519\n",
      "2018-05-27 23:26:45,755 Iter 820, Minibatch Loss= 7.3519, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:27:10,796 Epoch 41, Average loss: 21.8586, learning rate: 0.0003\n",
      "2018-05-27 23:27:10,916 Verification error= 100.0%, loss= 14.1827\n",
      "2018-05-27 23:27:10,981 Iter 840, Minibatch Loss= 14.1827, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:27:36,177 Epoch 42, Average loss: 14.0939, learning rate: 0.0003\n",
      "2018-05-27 23:27:36,297 Verification error= 100.0%, loss= 9.5747\n",
      "2018-05-27 23:27:36,364 Iter 860, Minibatch Loss= 9.5747, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:28:01,734 Epoch 43, Average loss: 16.1948, learning rate: 0.0003\n",
      "2018-05-27 23:28:01,852 Verification error= 100.0%, loss= 5.7084\n",
      "2018-05-27 23:28:01,917 Iter 880, Minibatch Loss= 5.7084, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:28:27,394 Epoch 44, Average loss: 12.2813, learning rate: 0.0003\n",
      "2018-05-27 23:28:27,525 Verification error= 100.0%, loss= 7.0615\n",
      "2018-05-27 23:28:27,595 Iter 900, Minibatch Loss= 7.0615, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:28:53,929 Epoch 45, Average loss: 14.2528, learning rate: 0.0003\n",
      "2018-05-27 23:28:54,057 Verification error= 100.0%, loss= 6.8452\n",
      "2018-05-27 23:28:54,126 Iter 920, Minibatch Loss= 6.8452, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:29:20,637 Epoch 46, Average loss: 11.8100, learning rate: 0.0003\n",
      "2018-05-27 23:29:20,754 Verification error= 100.0%, loss= 8.5898\n",
      "2018-05-27 23:29:20,819 Iter 940, Minibatch Loss= 8.5898, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:29:47,901 Epoch 47, Average loss: 13.4012, learning rate: 0.0003\n",
      "2018-05-27 23:29:48,025 Verification error= 100.0%, loss= 3.3994\n",
      "2018-05-27 23:29:48,094 Iter 960, Minibatch Loss= 3.3994, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:30:15,750 Epoch 48, Average loss: 16.4226, learning rate: 0.0003\n",
      "2018-05-27 23:30:15,877 Verification error= 100.0%, loss= 4.6263\n",
      "2018-05-27 23:30:15,944 Iter 980, Minibatch Loss= 4.6263, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:30:43,863 Epoch 49, Average loss: 19.9084, learning rate: 0.0003\n",
      "2018-05-27 23:30:43,987 Verification error= 100.0%, loss= 5.5971\n",
      "2018-05-27 23:30:44,054 Iter 1000, Minibatch Loss= 5.5971, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:31:11,758 Epoch 50, Average loss: 9.1511, learning rate: 0.0003\n",
      "2018-05-27 23:31:11,878 Verification error= 100.0%, loss= 4.2264\n",
      "2018-05-27 23:31:11,942 Iter 1020, Minibatch Loss= 4.2264, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:31:39,850 Epoch 51, Average loss: 12.7429, learning rate: 0.0003\n",
      "2018-05-27 23:31:39,977 Verification error= 100.0%, loss= 8.3514\n",
      "2018-05-27 23:31:40,046 Iter 1040, Minibatch Loss= 8.3514, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:32:08,707 Epoch 52, Average loss: 9.8386, learning rate: 0.0003\n",
      "2018-05-27 23:32:08,828 Verification error= 100.0%, loss= 11.1666\n",
      "2018-05-27 23:32:08,896 Iter 1060, Minibatch Loss= 11.1666, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:32:37,814 Epoch 53, Average loss: 9.8466, learning rate: 0.0003\n",
      "2018-05-27 23:32:37,929 Verification error= 100.0%, loss= 11.9928\n",
      "2018-05-27 23:32:37,994 Iter 1080, Minibatch Loss= 11.9928, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:33:06,216 Epoch 54, Average loss: 10.8490, learning rate: 0.0003\n",
      "2018-05-27 23:33:06,337 Verification error= 100.0%, loss= 9.8011\n",
      "2018-05-27 23:33:06,402 Iter 1100, Minibatch Loss= 9.8011, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:33:36,462 Epoch 55, Average loss: 17.6659, learning rate: 0.0003\n",
      "2018-05-27 23:33:36,589 Verification error= 100.0%, loss= 8.3797\n",
      "2018-05-27 23:33:36,659 Iter 1120, Minibatch Loss= 8.3797, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:34:06,657 Epoch 56, Average loss: 19.9198, learning rate: 0.0003\n",
      "2018-05-27 23:34:06,776 Verification error= 100.0%, loss= 7.7405\n",
      "2018-05-27 23:34:06,842 Iter 1140, Minibatch Loss= 7.7405, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:34:37,872 Epoch 57, Average loss: 15.7910, learning rate: 0.0003\n",
      "2018-05-27 23:34:37,997 Verification error= 100.0%, loss= 6.5984\n",
      "2018-05-27 23:34:38,065 Iter 1160, Minibatch Loss= 6.5984, Training Accuracy= 1.0000, Minibatch error= 0.0%\n",
      "2018-05-27 23:35:07,865 Epoch 58, Average loss: 10.4454, learning rate: 0.0003\n",
      "2018-05-27 23:35:07,984 Verification error= 100.0%, loss= 5.2002\n",
      "2018-05-27 23:35:08,052 Iter 1180, Minibatch Loss= 5.2002, Training Accuracy= 1.0000, Minibatch error= 0.0%\n"
     ]
    }
   ],
   "source": [
    "from tf_unet import unet, util, image_util\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_iters = [20]\n",
    "num_epochs = [100]\n",
    "feat_roots = [64]\n",
    "num_layers = 3\n",
    "#class_weight_tumor = [1]\n",
    "\n",
    "reg_lambda = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "batch_size = [1]\n",
    "optimizer = \"adam\"\n",
    "\n",
    "\n",
    "\n",
    "n_classes = 1\n",
    "\n",
    "#preparing data loading\n",
    "data_provider = image_util.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/reduced_flairdata/*\", data_suffix = \"flair_.jpg\", mask_suffix = \"seg_.jpg\")\n",
    "#data_provider = image_util.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/flairdata/preprocess_Data/*\", data_suffix = \"flair_.jpg\", mask_suffix = \"seg_.jpg\")\n",
    "for reg_lambda_i in reg_lambda:\n",
    "    for feat_roots_i in feat_roots:\n",
    "        for batch_size_i in batch_size:\n",
    "            for train_iters_i in train_iters: \n",
    "                for num_epochs_i in num_epochs:\n",
    "                    output_path = \"./outputs/appro100/output{}-lambda{}\".format(datetime.now(), reg_lambda_i)\n",
    "                    #output_path = \"./outputs/output-fr{}-bs{}-lambda{}-clswei{}\".format(feat_roots_i,batch_size_i,reg_lambda_i, class_weight_tumor)\n",
    "                    print('Approach-- #lambda{}'.format(reg_lambda_i))\n",
    "                    net = unet.Unet(layers=num_layers, features_root=feat_roots_i, channels=1, n_class=n_classes,  \n",
    "                                    cost = \"cross_entropy\", cost_kwargs=dict(regularizer=reg_lambda_i, \n",
    "                                                                             class_weights=[0.2,0.8]))\n",
    "                    trainer = unet.Trainer(net, optimizer=optimizer, batch_size = batch_size_i)\n",
    "                    trainer.train(data_provider, output_path, training_iters=train_iters_i, epochs=num_epochs_i, restore=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:33:16,600 Layers 3, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files used: 5000\n",
      "Approach-- #epoch100-#iter20-lambda0.001\n",
      "WARNING:tensorflow:From /home/javi_fdez_093/deep-learn/tf_unet/unet2.py:231: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:33:17,118 From /home/javi_fdez_093/deep-learn/tf_unet/unet2.py:231: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/javi_fdez_093/deep-learn/tf_unet/layers.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:33:17,721 From /home/javi_fdez_093/deep-learn/tf_unet/layers.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2018-05-27 15:33:18,759 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 15:33:16.586372-#epoch100-#iter20-lambda0.001'\n",
      "2018-05-27 15:33:18,760 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 15:33:16.586372-#epoch100-#iter20-lambda0.001'\n",
      "2018-05-27 15:33:37,290 Verification error= 81.1%, loss= 0.5916\n",
      "2018-05-27 15:33:37,647 Start optimization\n",
      "2018-05-27 15:33:47,456 Iter 0, Minibatch Loss= 0.5916, Training Accuracy= 0.1885, Minibatch error= 81.1%\n",
      "2018-05-27 15:33:48,975 Iter 0, Minibatch Loss= 0.6617, Training Accuracy= 0.4658, Minibatch error= 53.4%\n",
      "2018-05-27 15:33:50,056 Iter 1, Minibatch Loss= 0.5887, Training Accuracy= 0.2876, Minibatch error= 71.2%\n",
      "2018-05-27 15:33:51,142 Iter 2, Minibatch Loss= 0.5141, Training Accuracy= 0.0767, Minibatch error= 92.3%\n",
      "2018-05-27 15:33:52,199 Iter 3, Minibatch Loss= 0.5793, Training Accuracy= 0.3391, Minibatch error= 66.1%\n",
      "2018-05-27 15:33:53,276 Iter 4, Minibatch Loss= 0.5772, Training Accuracy= 0.3841, Minibatch error= 61.6%\n",
      "2018-05-27 15:33:54,361 Iter 5, Minibatch Loss= 0.5550, Training Accuracy= 0.3591, Minibatch error= 64.1%\n",
      "2018-05-27 15:33:55,448 Iter 6, Minibatch Loss= 0.5358, Training Accuracy= 0.3442, Minibatch error= 65.6%\n",
      "2018-05-27 15:33:56,521 Iter 7, Minibatch Loss= 0.4647, Training Accuracy= 0.0334, Minibatch error= 96.7%\n",
      "2018-05-27 15:33:57,601 Iter 8, Minibatch Loss= 0.4727, Training Accuracy= 0.1451, Minibatch error= 85.5%\n",
      "2018-05-27 15:33:58,673 Iter 9, Minibatch Loss= 0.5096, Training Accuracy= 0.3485, Minibatch error= 65.1%\n",
      "2018-05-27 15:33:59,771 Iter 10, Minibatch Loss= 0.4324, Training Accuracy= 0.0651, Minibatch error= 93.5%\n",
      "2018-05-27 15:34:00,842 Iter 11, Minibatch Loss= 0.5023, Training Accuracy= 0.4773, Minibatch error= 52.3%\n",
      "2018-05-27 15:34:01,911 Iter 12, Minibatch Loss= 0.4015, Training Accuracy= 0.0346, Minibatch error= 96.5%\n",
      "2018-05-27 15:34:02,971 Iter 13, Minibatch Loss= 0.3949, Training Accuracy= 0.0627, Minibatch error= 93.7%\n",
      "2018-05-27 15:34:04,032 Iter 14, Minibatch Loss= 0.3751, Training Accuracy= 0.0277, Minibatch error= 97.2%\n",
      "2018-05-27 15:34:05,103 Iter 15, Minibatch Loss= 0.4530, Training Accuracy= 0.4091, Minibatch error= 59.1%\n",
      "2018-05-27 15:34:06,173 Iter 16, Minibatch Loss= 0.3674, Training Accuracy= 0.0816, Minibatch error= 91.8%\n",
      "2018-05-27 15:34:07,254 Iter 17, Minibatch Loss= 0.3432, Training Accuracy= 0.0280, Minibatch error= 97.2%\n",
      "2018-05-27 15:34:08,350 Iter 18, Minibatch Loss= 0.3341, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 15:34:09,429 Iter 19, Minibatch Loss= 0.3977, Training Accuracy= 0.9485, Minibatch error= 5.2%\n",
      "2018-05-27 15:34:09,430 Epoch 0, Average loss: 0.4896, learning rate: 0.0010\n",
      "2018-05-27 15:34:19,058 Iter 20, Minibatch Loss= 0.3567, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 15:34:20,581 Iter 20, Minibatch Loss= 0.3656, Training Accuracy= 0.4703, Minibatch error= 53.0%\n",
      "2018-05-27 15:34:21,668 Iter 21, Minibatch Loss= 0.3277, Training Accuracy= 0.3577, Minibatch error= 64.2%\n",
      "2018-05-27 15:34:22,754 Iter 22, Minibatch Loss= 0.3094, Training Accuracy= 0.3736, Minibatch error= 62.6%\n",
      "2018-05-27 15:34:23,819 Iter 23, Minibatch Loss= 0.2951, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 15:34:24,886 Iter 24, Minibatch Loss= 0.2531, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 15:34:25,960 Iter 25, Minibatch Loss= 0.2433, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 15:34:27,032 Iter 26, Minibatch Loss= 0.2743, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 15:34:28,098 Iter 27, Minibatch Loss= 0.2482, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 15:34:29,183 Iter 28, Minibatch Loss= 0.2619, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 15:34:30,266 Iter 29, Minibatch Loss= 0.2650, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 15:34:31,338 Iter 30, Minibatch Loss= 0.2484, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 15:34:32,423 Iter 31, Minibatch Loss= 0.2223, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 15:34:33,514 Iter 32, Minibatch Loss= 0.2361, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 15:34:34,598 Iter 33, Minibatch Loss= 0.1994, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 15:34:35,684 Iter 34, Minibatch Loss= 0.1788, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 15:34:36,761 Iter 35, Minibatch Loss= 0.1559, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 15:34:37,841 Iter 36, Minibatch Loss= 0.1666, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 15:34:38,927 Iter 37, Minibatch Loss= 0.1713, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 15:34:39,991 Iter 38, Minibatch Loss= 0.1327, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 15:34:41,077 Iter 39, Minibatch Loss= 0.1599, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 15:34:41,078 Epoch 1, Average loss: 0.2540, learning rate: 0.0010\n",
      "2018-05-27 15:34:50,661 Iter 40, Minibatch Loss= 0.1587, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 15:34:52,057 Iter 40, Minibatch Loss= 0.1629, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 15:34:53,144 Iter 41, Minibatch Loss= 0.1695, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 15:34:54,223 Iter 42, Minibatch Loss= 0.1781, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 15:34:55,291 Iter 43, Minibatch Loss= 0.1750, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 15:34:56,368 Iter 44, Minibatch Loss= 0.1732, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 15:34:57,449 Iter 45, Minibatch Loss= 0.1868, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:34:58,518 Iter 46, Minibatch Loss= 0.1428, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:34:59,601 Iter 47, Minibatch Loss= 0.1392, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 15:35:00,673 Iter 48, Minibatch Loss= 0.1211, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 15:35:01,742 Iter 49, Minibatch Loss= 0.1393, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 15:35:02,820 Iter 50, Minibatch Loss= 0.1341, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 15:35:03,881 Iter 51, Minibatch Loss= 0.1209, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 15:35:04,960 Iter 52, Minibatch Loss= 0.1065, Training Accuracy= 0.9984, Minibatch error= 0.2%\n",
      "2018-05-27 15:35:06,036 Iter 53, Minibatch Loss= 0.1122, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 15:35:07,100 Iter 54, Minibatch Loss= 0.1701, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 15:35:08,178 Iter 55, Minibatch Loss= 0.1083, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 15:35:09,243 Iter 56, Minibatch Loss= 0.1276, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:35:10,333 Iter 57, Minibatch Loss= 0.1319, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 15:35:11,399 Iter 58, Minibatch Loss= 0.1104, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 15:35:12,468 Iter 59, Minibatch Loss= 0.1090, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 15:35:12,469 Epoch 2, Average loss: 0.1483, learning rate: 0.0010\n",
      "2018-05-27 15:35:22,020 Iter 60, Minibatch Loss= 0.1177, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 15:35:23,472 Iter 60, Minibatch Loss= 0.1217, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 15:35:24,559 Iter 61, Minibatch Loss= 0.1204, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 15:35:25,657 Iter 62, Minibatch Loss= 0.1112, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:35:26,723 Iter 63, Minibatch Loss= 0.1133, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 15:35:27,799 Iter 64, Minibatch Loss= 0.1159, Training Accuracy= 0.9638, Minibatch error= 3.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:35:28,878 Iter 65, Minibatch Loss= 0.1145, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 15:35:29,951 Iter 66, Minibatch Loss= 0.1107, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 15:35:31,032 Iter 67, Minibatch Loss= 0.0996, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 15:35:32,132 Iter 68, Minibatch Loss= 0.1079, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 15:35:33,227 Iter 69, Minibatch Loss= 0.1005, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 15:35:34,327 Iter 70, Minibatch Loss= 0.1022, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 15:35:35,420 Iter 71, Minibatch Loss= 0.0942, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 15:35:36,514 Iter 72, Minibatch Loss= 0.0959, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 15:35:37,606 Iter 73, Minibatch Loss= 0.1015, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 15:35:38,694 Iter 74, Minibatch Loss= 0.1041, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 15:35:39,787 Iter 75, Minibatch Loss= 0.1007, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 15:35:40,880 Iter 76, Minibatch Loss= 0.0877, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 15:35:41,959 Iter 77, Minibatch Loss= 0.0855, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 15:35:43,047 Iter 78, Minibatch Loss= 0.1034, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 15:35:44,121 Iter 79, Minibatch Loss= 0.1063, Training Accuracy= 0.9533, Minibatch error= 4.7%\n",
      "2018-05-27 15:35:44,122 Epoch 3, Average loss: 0.1083, learning rate: 0.0010\n",
      "2018-05-27 15:35:53,683 Iter 80, Minibatch Loss= 0.1027, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 15:35:55,148 Iter 80, Minibatch Loss= 0.0936, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 15:35:56,233 Iter 81, Minibatch Loss= 0.1017, Training Accuracy= 0.9492, Minibatch error= 5.1%\n",
      "2018-05-27 15:35:57,342 Iter 82, Minibatch Loss= 0.0932, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 15:35:58,409 Iter 83, Minibatch Loss= 0.0858, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 15:35:59,492 Iter 84, Minibatch Loss= 0.0761, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 15:36:00,580 Iter 85, Minibatch Loss= 0.0781, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 15:36:01,675 Iter 86, Minibatch Loss= 0.0894, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:36:02,766 Iter 87, Minibatch Loss= 0.0854, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 15:36:03,839 Iter 88, Minibatch Loss= 0.1049, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 15:36:04,921 Iter 89, Minibatch Loss= 0.0718, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 15:36:06,006 Iter 90, Minibatch Loss= 0.0982, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 15:36:07,088 Iter 91, Minibatch Loss= 0.0976, Training Accuracy= 0.9378, Minibatch error= 6.2%\n",
      "2018-05-27 15:36:08,164 Iter 92, Minibatch Loss= 0.1119, Training Accuracy= 0.9229, Minibatch error= 7.7%\n",
      "2018-05-27 15:36:09,247 Iter 93, Minibatch Loss= 0.0900, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 15:36:10,348 Iter 94, Minibatch Loss= 0.0913, Training Accuracy= 0.9457, Minibatch error= 5.4%\n",
      "2018-05-27 15:36:11,436 Iter 95, Minibatch Loss= 0.0887, Training Accuracy= 0.9517, Minibatch error= 4.8%\n",
      "2018-05-27 15:36:12,522 Iter 96, Minibatch Loss= 0.0894, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 15:36:13,599 Iter 97, Minibatch Loss= 0.0881, Training Accuracy= 0.9564, Minibatch error= 4.4%\n",
      "2018-05-27 15:36:14,677 Iter 98, Minibatch Loss= 0.0744, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 15:36:15,763 Iter 99, Minibatch Loss= 0.0863, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 15:36:15,764 Epoch 4, Average loss: 0.0913, learning rate: 0.0010\n",
      "2018-05-27 15:36:25,165 Iter 100, Minibatch Loss= 0.0802, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 15:36:26,646 Iter 100, Minibatch Loss= 0.0735, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 15:36:27,750 Iter 101, Minibatch Loss= 0.0850, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:36:28,838 Iter 102, Minibatch Loss= 0.0745, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:36:29,895 Iter 103, Minibatch Loss= 0.0699, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:36:30,981 Iter 104, Minibatch Loss= 0.0915, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 15:36:32,087 Iter 105, Minibatch Loss= 0.0673, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 15:36:33,168 Iter 106, Minibatch Loss= 0.0710, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 15:36:34,255 Iter 107, Minibatch Loss= 0.0694, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 15:36:35,336 Iter 108, Minibatch Loss= 0.0840, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 15:36:36,454 Iter 109, Minibatch Loss= 0.0740, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:36:37,534 Iter 110, Minibatch Loss= 0.0678, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 15:36:38,624 Iter 111, Minibatch Loss= 0.0657, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 15:36:39,699 Iter 112, Minibatch Loss= 0.0793, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 15:36:40,792 Iter 113, Minibatch Loss= 0.0723, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 15:36:41,884 Iter 114, Minibatch Loss= 0.0650, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 15:36:42,971 Iter 115, Minibatch Loss= 0.0821, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 15:36:44,047 Iter 116, Minibatch Loss= 0.0777, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 15:36:45,154 Iter 117, Minibatch Loss= 0.0755, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 15:36:46,216 Iter 118, Minibatch Loss= 0.0656, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 15:36:47,282 Iter 119, Minibatch Loss= 0.0827, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 15:36:47,283 Epoch 5, Average loss: 0.0770, learning rate: 0.0010\n",
      "2018-05-27 15:36:56,877 Iter 120, Minibatch Loss= 0.0722, Training Accuracy= 0.9715, Minibatch error= 2.9%\n",
      "2018-05-27 15:36:58,403 Iter 120, Minibatch Loss= 0.0575, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 15:36:59,480 Iter 121, Minibatch Loss= 0.0785, Training Accuracy= 0.9517, Minibatch error= 4.8%\n",
      "2018-05-27 15:37:00,560 Iter 122, Minibatch Loss= 0.0643, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 15:37:01,691 Iter 123, Minibatch Loss= 0.0651, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 15:37:02,796 Iter 124, Minibatch Loss= 0.0644, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 15:37:03,900 Iter 125, Minibatch Loss= 0.0743, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:37:04,992 Iter 126, Minibatch Loss= 0.0775, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 15:37:06,086 Iter 127, Minibatch Loss= 0.0544, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 15:37:07,180 Iter 128, Minibatch Loss= 0.0777, Training Accuracy= 0.9456, Minibatch error= 5.4%\n",
      "2018-05-27 15:37:08,274 Iter 129, Minibatch Loss= 0.0759, Training Accuracy= 0.9515, Minibatch error= 4.8%\n",
      "2018-05-27 15:37:09,378 Iter 130, Minibatch Loss= 0.0559, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:37:10,471 Iter 131, Minibatch Loss= 0.0727, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 15:37:11,580 Iter 132, Minibatch Loss= 0.0530, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 15:37:12,684 Iter 133, Minibatch Loss= 0.0726, Training Accuracy= 0.9558, Minibatch error= 4.4%\n",
      "2018-05-27 15:37:13,773 Iter 134, Minibatch Loss= 0.0611, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 15:37:14,885 Iter 135, Minibatch Loss= 0.0761, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 15:37:15,975 Iter 136, Minibatch Loss= 0.0740, Training Accuracy= 0.9530, Minibatch error= 4.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:37:17,091 Iter 137, Minibatch Loss= 0.0825, Training Accuracy= 0.9345, Minibatch error= 6.5%\n",
      "2018-05-27 15:37:18,195 Iter 138, Minibatch Loss= 0.0762, Training Accuracy= 0.9427, Minibatch error= 5.7%\n",
      "2018-05-27 15:37:19,305 Iter 139, Minibatch Loss= 0.0765, Training Accuracy= 0.9492, Minibatch error= 5.1%\n",
      "2018-05-27 15:37:19,306 Epoch 6, Average loss: 0.0706, learning rate: 0.0010\n",
      "2018-05-27 15:37:28,911 Iter 140, Minibatch Loss= 0.0667, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 15:37:30,513 Iter 140, Minibatch Loss= 0.0696, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 15:37:31,617 Iter 141, Minibatch Loss= 0.0689, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 15:37:32,709 Iter 142, Minibatch Loss= 0.0576, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 15:37:33,809 Iter 143, Minibatch Loss= 0.0480, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 15:37:34,936 Iter 144, Minibatch Loss= 0.0683, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 15:37:36,025 Iter 145, Minibatch Loss= 0.0633, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 15:37:37,133 Iter 146, Minibatch Loss= 0.0707, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 15:37:38,226 Iter 147, Minibatch Loss= 0.0481, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 15:37:39,335 Iter 148, Minibatch Loss= 0.0594, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 15:37:40,436 Iter 149, Minibatch Loss= 0.0570, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 15:37:41,546 Iter 150, Minibatch Loss= 0.0736, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 15:37:42,634 Iter 151, Minibatch Loss= 0.0655, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 15:37:43,738 Iter 152, Minibatch Loss= 0.0646, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:37:44,850 Iter 153, Minibatch Loss= 0.0541, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 15:37:45,946 Iter 154, Minibatch Loss= 0.0542, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:37:47,072 Iter 155, Minibatch Loss= 0.0683, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 15:37:48,182 Iter 156, Minibatch Loss= 0.0684, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 15:37:49,285 Iter 157, Minibatch Loss= 0.0659, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 15:37:50,364 Iter 158, Minibatch Loss= 0.0658, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 15:37:51,474 Iter 159, Minibatch Loss= 0.0550, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 15:37:51,475 Epoch 7, Average loss: 0.0641, learning rate: 0.0010\n",
      "2018-05-27 15:38:01,090 Iter 160, Minibatch Loss= 0.0601, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:38:02,628 Iter 160, Minibatch Loss= 0.0477, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 15:38:03,731 Iter 161, Minibatch Loss= 0.0637, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:38:04,847 Iter 162, Minibatch Loss= 0.0497, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 15:38:05,954 Iter 163, Minibatch Loss= 0.0549, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 15:38:07,065 Iter 164, Minibatch Loss= 0.0668, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 15:38:08,167 Iter 165, Minibatch Loss= 0.0631, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 15:38:09,281 Iter 166, Minibatch Loss= 0.0539, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 15:38:10,398 Iter 167, Minibatch Loss= 0.0634, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:38:11,529 Iter 168, Minibatch Loss= 0.0540, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 15:38:12,625 Iter 169, Minibatch Loss= 0.0581, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 15:38:13,725 Iter 170, Minibatch Loss= 0.0487, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 15:38:14,826 Iter 171, Minibatch Loss= 0.0554, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 15:38:15,933 Iter 172, Minibatch Loss= 0.0494, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:38:17,045 Iter 173, Minibatch Loss= 0.0416, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 15:38:18,154 Iter 174, Minibatch Loss= 0.0426, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 15:38:19,270 Iter 175, Minibatch Loss= 0.0483, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:38:20,378 Iter 176, Minibatch Loss= 0.0510, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 15:38:21,505 Iter 177, Minibatch Loss= 0.0778, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 15:38:22,625 Iter 178, Minibatch Loss= 0.0739, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 15:38:23,738 Iter 179, Minibatch Loss= 0.0654, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 15:38:23,739 Epoch 8, Average loss: 0.0585, learning rate: 0.0010\n",
      "2018-05-27 15:38:33,329 Iter 180, Minibatch Loss= 0.0578, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 15:38:34,876 Iter 180, Minibatch Loss= 0.0504, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 15:38:35,985 Iter 181, Minibatch Loss= 0.0612, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 15:38:37,091 Iter 182, Minibatch Loss= 0.0599, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 15:38:38,196 Iter 183, Minibatch Loss= 0.0547, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 15:38:39,306 Iter 184, Minibatch Loss= 0.0507, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 15:38:40,409 Iter 185, Minibatch Loss= 0.0425, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 15:38:41,537 Iter 186, Minibatch Loss= 0.0650, Training Accuracy= 0.9506, Minibatch error= 4.9%\n",
      "2018-05-27 15:38:42,642 Iter 187, Minibatch Loss= 0.0551, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 15:38:43,749 Iter 188, Minibatch Loss= 0.0728, Training Accuracy= 0.9355, Minibatch error= 6.4%\n",
      "2018-05-27 15:38:44,856 Iter 189, Minibatch Loss= 0.0622, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 15:38:45,980 Iter 190, Minibatch Loss= 0.0917, Training Accuracy= 0.9534, Minibatch error= 4.7%\n",
      "2018-05-27 15:38:47,075 Iter 191, Minibatch Loss= 0.0612, Training Accuracy= 0.9552, Minibatch error= 4.5%\n",
      "2018-05-27 15:38:48,192 Iter 192, Minibatch Loss= 0.0611, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 15:38:49,304 Iter 193, Minibatch Loss= 0.0447, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 15:38:50,428 Iter 194, Minibatch Loss= 0.0678, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:38:51,539 Iter 195, Minibatch Loss= 0.0689, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 15:38:52,660 Iter 196, Minibatch Loss= 0.0632, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 15:38:53,764 Iter 197, Minibatch Loss= 0.0517, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 15:38:54,878 Iter 198, Minibatch Loss= 0.0591, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 15:38:55,979 Iter 199, Minibatch Loss= 0.0639, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 15:38:55,980 Epoch 9, Average loss: 0.0598, learning rate: 0.0010\n",
      "2018-05-27 15:39:05,638 Iter 200, Minibatch Loss= 0.0529, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 15:39:07,251 Iter 200, Minibatch Loss= 0.0363, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 15:39:08,373 Iter 201, Minibatch Loss= 0.0417, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 15:39:09,474 Iter 202, Minibatch Loss= 0.0591, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 15:39:10,596 Iter 203, Minibatch Loss= 0.0467, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 15:39:11,702 Iter 204, Minibatch Loss= 0.0602, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 15:39:12,806 Iter 205, Minibatch Loss= 0.0505, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 15:39:13,918 Iter 206, Minibatch Loss= 0.0558, Training Accuracy= 0.9652, Minibatch error= 3.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:39:15,034 Iter 207, Minibatch Loss= 0.0398, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 15:39:16,137 Iter 208, Minibatch Loss= 0.0577, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:39:17,259 Iter 209, Minibatch Loss= 0.0419, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:39:18,374 Iter 210, Minibatch Loss= 0.0421, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 15:39:19,477 Iter 211, Minibatch Loss= 0.0399, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 15:39:20,591 Iter 212, Minibatch Loss= 0.0643, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 15:39:21,708 Iter 213, Minibatch Loss= 0.0380, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 15:39:22,831 Iter 214, Minibatch Loss= 0.0376, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 15:39:23,925 Iter 215, Minibatch Loss= 0.0398, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 15:39:25,020 Iter 216, Minibatch Loss= 0.0498, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 15:39:26,127 Iter 217, Minibatch Loss= 0.0471, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 15:39:27,256 Iter 218, Minibatch Loss= 0.0595, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 15:39:28,364 Iter 219, Minibatch Loss= 0.0349, Training Accuracy= 0.9958, Minibatch error= 0.4%\n",
      "2018-05-27 15:39:28,366 Epoch 10, Average loss: 0.0484, learning rate: 0.0010\n",
      "2018-05-27 15:39:38,000 Iter 220, Minibatch Loss= 0.0504, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 15:39:39,584 Iter 220, Minibatch Loss= 0.0387, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 15:39:40,691 Iter 221, Minibatch Loss= 0.0507, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 15:39:41,803 Iter 222, Minibatch Loss= 0.0367, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 15:39:42,913 Iter 223, Minibatch Loss= 0.0384, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 15:39:44,042 Iter 224, Minibatch Loss= 0.0560, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:39:45,160 Iter 225, Minibatch Loss= 0.0386, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 15:39:46,274 Iter 226, Minibatch Loss= 0.0562, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 15:39:47,392 Iter 227, Minibatch Loss= 0.0366, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 15:39:48,520 Iter 228, Minibatch Loss= 0.0505, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 15:39:49,656 Iter 229, Minibatch Loss= 0.0347, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 15:39:50,770 Iter 230, Minibatch Loss= 0.0442, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 15:39:51,875 Iter 231, Minibatch Loss= 0.0518, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 15:39:53,001 Iter 232, Minibatch Loss= 0.0459, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 15:39:54,120 Iter 233, Minibatch Loss= 0.0401, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:39:55,238 Iter 234, Minibatch Loss= 0.0340, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 15:39:56,370 Iter 235, Minibatch Loss= 0.0524, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 15:39:57,487 Iter 236, Minibatch Loss= 0.0619, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:39:58,597 Iter 237, Minibatch Loss= 0.0560, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 15:39:59,713 Iter 238, Minibatch Loss= 0.0455, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 15:40:00,830 Iter 239, Minibatch Loss= 0.0343, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 15:40:00,831 Epoch 11, Average loss: 0.0460, learning rate: 0.0010\n",
      "2018-05-27 15:40:10,513 Iter 240, Minibatch Loss= 0.0470, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 15:40:12,149 Iter 240, Minibatch Loss= 0.0399, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 15:40:13,260 Iter 241, Minibatch Loss= 0.0389, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 15:40:14,383 Iter 242, Minibatch Loss= 0.0448, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 15:40:15,509 Iter 243, Minibatch Loss= 0.0515, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 15:40:16,622 Iter 244, Minibatch Loss= 0.0557, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 15:40:17,725 Iter 245, Minibatch Loss= 0.0407, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 15:40:18,834 Iter 246, Minibatch Loss= 0.0308, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 15:40:19,943 Iter 247, Minibatch Loss= 0.0561, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 15:40:21,076 Iter 248, Minibatch Loss= 0.0483, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 15:40:22,181 Iter 249, Minibatch Loss= 0.0542, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 15:40:23,301 Iter 250, Minibatch Loss= 0.0359, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 15:40:24,423 Iter 251, Minibatch Loss= 0.0460, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 15:40:25,542 Iter 252, Minibatch Loss= 0.0314, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 15:40:26,651 Iter 253, Minibatch Loss= 0.0551, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 15:40:27,763 Iter 254, Minibatch Loss= 0.0527, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 15:40:28,876 Iter 255, Minibatch Loss= 0.0312, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 15:40:29,986 Iter 256, Minibatch Loss= 0.0516, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 15:40:31,094 Iter 257, Minibatch Loss= 0.0408, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 15:40:32,221 Iter 258, Minibatch Loss= 0.0508, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:40:33,341 Iter 259, Minibatch Loss= 0.0404, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 15:40:33,342 Epoch 12, Average loss: 0.0459, learning rate: 0.0010\n",
      "2018-05-27 15:40:42,980 Iter 260, Minibatch Loss= 0.0448, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 15:40:44,624 Iter 260, Minibatch Loss= 0.0510, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 15:40:45,754 Iter 261, Minibatch Loss= 0.0487, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 15:40:46,886 Iter 262, Minibatch Loss= 0.0457, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 15:40:48,012 Iter 263, Minibatch Loss= 0.0527, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 15:40:49,147 Iter 264, Minibatch Loss= 0.0403, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 15:40:50,272 Iter 265, Minibatch Loss= 0.0348, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 15:40:51,398 Iter 266, Minibatch Loss= 0.0565, Training Accuracy= 0.9564, Minibatch error= 4.4%\n",
      "2018-05-27 15:40:52,524 Iter 267, Minibatch Loss= 0.0550, Training Accuracy= 0.9538, Minibatch error= 4.6%\n",
      "2018-05-27 15:40:53,649 Iter 268, Minibatch Loss= 0.0526, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 15:40:54,772 Iter 269, Minibatch Loss= 0.0525, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 15:40:55,903 Iter 270, Minibatch Loss= 0.0538, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 15:40:57,030 Iter 271, Minibatch Loss= 0.0498, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 15:40:58,157 Iter 272, Minibatch Loss= 0.0358, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 15:40:59,289 Iter 273, Minibatch Loss= 0.0513, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:41:00,409 Iter 274, Minibatch Loss= 0.0523, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 15:41:01,517 Iter 275, Minibatch Loss= 0.0477, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 15:41:02,644 Iter 276, Minibatch Loss= 0.0466, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 15:41:03,766 Iter 277, Minibatch Loss= 0.0528, Training Accuracy= 0.9526, Minibatch error= 4.7%\n",
      "2018-05-27 15:41:04,892 Iter 278, Minibatch Loss= 0.0339, Training Accuracy= 0.9866, Minibatch error= 1.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:41:06,011 Iter 279, Minibatch Loss= 0.0395, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 15:41:06,012 Epoch 13, Average loss: 0.0478, learning rate: 0.0010\n",
      "2018-05-27 15:41:15,600 Iter 280, Minibatch Loss= 0.0428, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 15:41:17,327 Iter 280, Minibatch Loss= 0.0312, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 15:41:18,451 Iter 281, Minibatch Loss= 0.0356, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 15:41:19,574 Iter 282, Minibatch Loss= 0.0488, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 15:41:20,701 Iter 283, Minibatch Loss= 0.0534, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 15:41:21,824 Iter 284, Minibatch Loss= 0.0510, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 15:41:22,948 Iter 285, Minibatch Loss= 0.0467, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 15:41:24,078 Iter 286, Minibatch Loss= 0.0524, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 15:41:25,201 Iter 287, Minibatch Loss= 0.0391, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 15:41:26,304 Iter 288, Minibatch Loss= 0.0458, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 15:41:27,441 Iter 289, Minibatch Loss= 0.0476, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 15:41:28,543 Iter 290, Minibatch Loss= 0.0437, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 15:41:29,661 Iter 291, Minibatch Loss= 0.0440, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 15:41:30,769 Iter 292, Minibatch Loss= 0.0354, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 15:41:31,882 Iter 293, Minibatch Loss= 0.0355, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 15:41:32,987 Iter 294, Minibatch Loss= 0.0530, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 15:41:34,108 Iter 295, Minibatch Loss= 0.0341, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 15:41:35,234 Iter 296, Minibatch Loss= 0.0413, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 15:41:36,355 Iter 297, Minibatch Loss= 0.0539, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 15:41:37,470 Iter 298, Minibatch Loss= 0.0360, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:41:38,604 Iter 299, Minibatch Loss= 0.0274, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 15:41:38,605 Epoch 14, Average loss: 0.0411, learning rate: 0.0010\n",
      "2018-05-27 15:41:48,120 Iter 300, Minibatch Loss= 0.0415, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 15:41:49,782 Iter 300, Minibatch Loss= 0.0484, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 15:41:50,907 Iter 301, Minibatch Loss= 0.0331, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 15:41:52,019 Iter 302, Minibatch Loss= 0.0332, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 15:41:53,134 Iter 303, Minibatch Loss= 0.0241, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 15:41:54,268 Iter 304, Minibatch Loss= 0.0358, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 15:41:55,391 Iter 305, Minibatch Loss= 0.0542, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 15:41:56,512 Iter 306, Minibatch Loss= 0.0526, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 15:41:57,638 Iter 307, Minibatch Loss= 0.0734, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 15:41:58,767 Iter 308, Minibatch Loss= 0.0420, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 15:41:59,886 Iter 309, Minibatch Loss= 0.0485, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 15:42:01,018 Iter 310, Minibatch Loss= 0.0415, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 15:42:02,144 Iter 311, Minibatch Loss= 0.0257, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 15:42:03,272 Iter 312, Minibatch Loss= 0.0333, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:42:04,411 Iter 313, Minibatch Loss= 0.0373, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 15:42:05,543 Iter 314, Minibatch Loss= 0.0466, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 15:42:06,664 Iter 315, Minibatch Loss= 0.0331, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:42:07,781 Iter 316, Minibatch Loss= 0.0335, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 15:42:08,909 Iter 317, Minibatch Loss= 0.0462, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 15:42:10,032 Iter 318, Minibatch Loss= 0.0494, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 15:42:11,151 Iter 319, Minibatch Loss= 0.0474, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 15:42:11,152 Epoch 15, Average loss: 0.0406, learning rate: 0.0010\n",
      "2018-05-27 15:42:20,728 Iter 320, Minibatch Loss= 0.0396, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 15:42:22,419 Iter 320, Minibatch Loss= 0.0447, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 15:42:23,540 Iter 321, Minibatch Loss= 0.0307, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 15:42:24,674 Iter 322, Minibatch Loss= 0.0302, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 15:42:25,786 Iter 323, Minibatch Loss= 0.0461, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 15:42:26,910 Iter 324, Minibatch Loss= 0.0338, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 15:42:28,028 Iter 325, Minibatch Loss= 0.0439, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:42:29,156 Iter 326, Minibatch Loss= 0.0476, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 15:42:30,272 Iter 327, Minibatch Loss= 0.0296, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 15:42:31,392 Iter 328, Minibatch Loss= 0.0460, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 15:42:32,540 Iter 329, Minibatch Loss= 0.0270, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 15:42:33,653 Iter 330, Minibatch Loss= 0.0464, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 15:42:34,811 Iter 331, Minibatch Loss= 0.0431, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 15:42:35,939 Iter 332, Minibatch Loss= 0.0312, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 15:42:37,069 Iter 333, Minibatch Loss= 0.0433, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 15:42:38,192 Iter 334, Minibatch Loss= 0.0431, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 15:42:39,316 Iter 335, Minibatch Loss= 0.0521, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 15:42:40,432 Iter 336, Minibatch Loss= 0.0302, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 15:42:41,563 Iter 337, Minibatch Loss= 0.0316, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 15:42:42,683 Iter 338, Minibatch Loss= 0.0302, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 15:42:43,777 Iter 339, Minibatch Loss= 0.0356, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 15:42:43,778 Epoch 16, Average loss: 0.0393, learning rate: 0.0010\n",
      "2018-05-27 15:42:53,295 Iter 340, Minibatch Loss= 0.0383, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 15:42:54,992 Iter 340, Minibatch Loss= 0.0523, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 15:42:56,115 Iter 341, Minibatch Loss= 0.0416, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 15:42:57,257 Iter 342, Minibatch Loss= 0.0428, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 15:42:58,380 Iter 343, Minibatch Loss= 0.0304, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 15:42:59,501 Iter 344, Minibatch Loss= 0.0283, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 15:43:00,631 Iter 345, Minibatch Loss= 0.0228, Training Accuracy= 0.9965, Minibatch error= 0.4%\n",
      "2018-05-27 15:43:01,736 Iter 346, Minibatch Loss= 0.0443, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 15:43:02,846 Iter 347, Minibatch Loss= 0.0316, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 15:43:03,964 Iter 348, Minibatch Loss= 0.0258, Training Accuracy= 0.9916, Minibatch error= 0.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:43:05,087 Iter 349, Minibatch Loss= 0.0297, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 15:43:06,210 Iter 350, Minibatch Loss= 0.0354, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 15:43:07,325 Iter 351, Minibatch Loss= 0.0240, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 15:43:08,459 Iter 352, Minibatch Loss= 0.0237, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 15:43:09,580 Iter 353, Minibatch Loss= 0.0478, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 15:43:10,719 Iter 354, Minibatch Loss= 0.0451, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 15:43:11,827 Iter 355, Minibatch Loss= 0.0233, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 15:43:12,969 Iter 356, Minibatch Loss= 0.0303, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 15:43:14,075 Iter 357, Minibatch Loss= 0.0220, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 15:43:15,184 Iter 358, Minibatch Loss= 0.0429, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:43:16,312 Iter 359, Minibatch Loss= 0.0501, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 15:43:16,313 Epoch 17, Average loss: 0.0352, learning rate: 0.0010\n",
      "2018-05-27 15:43:25,848 Iter 360, Minibatch Loss= 0.0417, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 15:43:27,664 Iter 360, Minibatch Loss= 0.0514, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 15:43:28,794 Iter 361, Minibatch Loss= 0.0373, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 15:43:29,943 Iter 362, Minibatch Loss= 0.0431, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 15:43:31,072 Iter 363, Minibatch Loss= 0.0408, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 15:43:32,209 Iter 364, Minibatch Loss= 0.0452, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 15:43:33,330 Iter 365, Minibatch Loss= 0.0304, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 15:43:34,458 Iter 366, Minibatch Loss= 0.0241, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 15:43:35,609 Iter 367, Minibatch Loss= 0.0492, Training Accuracy= 0.9472, Minibatch error= 5.3%\n",
      "2018-05-27 15:43:36,731 Iter 368, Minibatch Loss= 0.0519, Training Accuracy= 0.9495, Minibatch error= 5.1%\n",
      "2018-05-27 15:43:37,861 Iter 369, Minibatch Loss= 0.0494, Training Accuracy= 0.9499, Minibatch error= 5.0%\n",
      "2018-05-27 15:43:38,991 Iter 370, Minibatch Loss= 0.0349, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 15:43:40,119 Iter 371, Minibatch Loss= 0.0417, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 15:43:41,265 Iter 372, Minibatch Loss= 0.0316, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 15:43:42,398 Iter 373, Minibatch Loss= 0.0423, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:43:43,521 Iter 374, Minibatch Loss= 0.0421, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:43:44,650 Iter 375, Minibatch Loss= 0.0404, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 15:43:45,772 Iter 376, Minibatch Loss= 0.0264, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 15:43:46,931 Iter 377, Minibatch Loss= 0.0282, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 15:43:48,056 Iter 378, Minibatch Loss= 0.0304, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 15:43:49,196 Iter 379, Minibatch Loss= 0.0267, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 15:43:49,197 Epoch 18, Average loss: 0.0375, learning rate: 0.0010\n",
      "2018-05-27 15:43:58,762 Iter 380, Minibatch Loss= 0.0359, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 15:44:00,538 Iter 380, Minibatch Loss= 0.0420, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 15:44:01,650 Iter 381, Minibatch Loss= 0.0309, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 15:44:02,791 Iter 382, Minibatch Loss= 0.0318, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 15:44:03,929 Iter 383, Minibatch Loss= 0.0315, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 15:44:05,062 Iter 384, Minibatch Loss= 0.0384, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:44:06,214 Iter 385, Minibatch Loss= 0.0426, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:44:07,369 Iter 386, Minibatch Loss= 0.0412, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:44:08,524 Iter 387, Minibatch Loss= 0.0451, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 15:44:09,678 Iter 388, Minibatch Loss= 0.0468, Training Accuracy= 0.9562, Minibatch error= 4.4%\n",
      "2018-05-27 15:44:10,842 Iter 389, Minibatch Loss= 0.0373, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 15:44:11,998 Iter 390, Minibatch Loss= 0.0265, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:44:13,136 Iter 391, Minibatch Loss= 0.0315, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 15:44:14,280 Iter 392, Minibatch Loss= 0.0426, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 15:44:15,442 Iter 393, Minibatch Loss= 0.0284, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 15:44:16,570 Iter 394, Minibatch Loss= 0.0193, Training Accuracy= 0.9977, Minibatch error= 0.2%\n",
      "2018-05-27 15:44:17,702 Iter 395, Minibatch Loss= 0.0367, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 15:44:18,824 Iter 396, Minibatch Loss= 0.0339, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 15:44:19,952 Iter 397, Minibatch Loss= 0.0435, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:44:21,094 Iter 398, Minibatch Loss= 0.0282, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 15:44:22,241 Iter 399, Minibatch Loss= 0.0502, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 15:44:22,242 Epoch 19, Average loss: 0.0366, learning rate: 0.0010\n",
      "2018-05-27 15:44:31,796 Iter 400, Minibatch Loss= 0.0453, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 15:44:33,574 Iter 400, Minibatch Loss= 0.0651, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:44:34,732 Iter 401, Minibatch Loss= 0.0513, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 15:44:35,882 Iter 402, Minibatch Loss= 0.0439, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 15:44:37,030 Iter 403, Minibatch Loss= 0.0270, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 15:44:38,178 Iter 404, Minibatch Loss= 0.0388, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 15:44:39,326 Iter 405, Minibatch Loss= 0.0399, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 15:44:40,475 Iter 406, Minibatch Loss= 0.0228, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 15:44:41,644 Iter 407, Minibatch Loss= 0.0472, Training Accuracy= 0.9514, Minibatch error= 4.9%\n",
      "2018-05-27 15:44:42,792 Iter 408, Minibatch Loss= 0.0400, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 15:44:43,931 Iter 409, Minibatch Loss= 0.0313, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 15:44:45,101 Iter 410, Minibatch Loss= 0.0314, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 15:44:46,257 Iter 411, Minibatch Loss= 0.0272, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 15:44:47,409 Iter 412, Minibatch Loss= 0.0251, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 15:44:48,590 Iter 413, Minibatch Loss= 0.0367, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 15:44:49,761 Iter 414, Minibatch Loss= 0.0423, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:44:50,926 Iter 415, Minibatch Loss= 0.0425, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 15:44:52,098 Iter 416, Minibatch Loss= 0.0232, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 15:44:53,249 Iter 417, Minibatch Loss= 0.0308, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 15:44:54,409 Iter 418, Minibatch Loss= 0.0287, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 15:44:55,584 Iter 419, Minibatch Loss= 0.0232, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 15:44:55,585 Epoch 20, Average loss: 0.0356, learning rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:45:05,194 Iter 420, Minibatch Loss= 0.0340, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 15:45:07,088 Iter 420, Minibatch Loss= 0.0335, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 15:45:08,236 Iter 421, Minibatch Loss= 0.0457, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 15:45:09,363 Iter 422, Minibatch Loss= 0.0411, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 15:45:10,507 Iter 423, Minibatch Loss= 0.0325, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 15:45:11,666 Iter 424, Minibatch Loss= 0.0214, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 15:45:12,808 Iter 425, Minibatch Loss= 0.0252, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 15:45:13,950 Iter 426, Minibatch Loss= 0.0245, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 15:45:15,085 Iter 427, Minibatch Loss= 0.0412, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 15:45:16,217 Iter 428, Minibatch Loss= 0.0369, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 15:45:17,357 Iter 429, Minibatch Loss= 0.0371, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 15:45:18,525 Iter 430, Minibatch Loss= 0.0408, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 15:45:19,661 Iter 431, Minibatch Loss= 0.0392, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 15:45:20,807 Iter 432, Minibatch Loss= 0.0425, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 15:45:21,951 Iter 433, Minibatch Loss= 0.0261, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 15:45:23,084 Iter 434, Minibatch Loss= 0.0410, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 15:45:24,242 Iter 435, Minibatch Loss= 0.0258, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:45:25,396 Iter 436, Minibatch Loss= 0.0243, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 15:45:26,526 Iter 437, Minibatch Loss= 0.0376, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 15:45:27,651 Iter 438, Minibatch Loss= 0.0247, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 15:45:28,786 Iter 439, Minibatch Loss= 0.0403, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 15:45:28,787 Epoch 21, Average loss: 0.0347, learning rate: 0.0010\n",
      "2018-05-27 15:45:38,339 Iter 440, Minibatch Loss= 0.0332, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 15:45:40,127 Iter 440, Minibatch Loss= 0.0373, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 15:45:41,276 Iter 441, Minibatch Loss= 0.0340, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 15:45:42,414 Iter 442, Minibatch Loss= 0.0298, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 15:45:43,567 Iter 443, Minibatch Loss= 0.0294, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 15:45:44,707 Iter 444, Minibatch Loss= 0.0426, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 15:45:45,861 Iter 445, Minibatch Loss= 0.0366, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 15:45:47,012 Iter 446, Minibatch Loss= 0.0299, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 15:45:48,143 Iter 447, Minibatch Loss= 0.0304, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 15:45:49,285 Iter 448, Minibatch Loss= 0.0378, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 15:45:50,423 Iter 449, Minibatch Loss= 0.0391, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 15:45:51,555 Iter 450, Minibatch Loss= 0.0448, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 15:45:52,693 Iter 451, Minibatch Loss= 0.0427, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 15:45:53,821 Iter 452, Minibatch Loss= 0.0478, Training Accuracy= 0.9430, Minibatch error= 5.7%\n",
      "2018-05-27 15:45:54,962 Iter 453, Minibatch Loss= 0.0248, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 15:45:56,110 Iter 454, Minibatch Loss= 0.0405, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 15:45:57,268 Iter 455, Minibatch Loss= 0.0265, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 15:45:58,420 Iter 456, Minibatch Loss= 0.0389, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 15:45:59,568 Iter 457, Minibatch Loss= 0.0378, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 15:46:00,703 Iter 458, Minibatch Loss= 0.0396, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 15:46:01,837 Iter 459, Minibatch Loss= 0.0394, Training Accuracy= 0.9544, Minibatch error= 4.6%\n",
      "2018-05-27 15:46:01,838 Epoch 22, Average loss: 0.0367, learning rate: 0.0010\n",
      "2018-05-27 15:46:11,442 Iter 460, Minibatch Loss= 0.0321, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 15:46:13,241 Iter 460, Minibatch Loss= 0.0279, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 15:46:14,391 Iter 461, Minibatch Loss= 0.0385, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 15:46:15,540 Iter 462, Minibatch Loss= 0.0427, Training Accuracy= 0.9587, Minibatch error= 4.1%\n",
      "2018-05-27 15:46:16,688 Iter 463, Minibatch Loss= 0.0369, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 15:46:17,829 Iter 464, Minibatch Loss= 0.0437, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 15:46:18,994 Iter 465, Minibatch Loss= 0.0306, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 15:46:20,149 Iter 466, Minibatch Loss= 0.0190, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 15:46:21,285 Iter 467, Minibatch Loss= 0.0446, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 15:46:22,416 Iter 468, Minibatch Loss= 0.0258, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 15:46:23,558 Iter 469, Minibatch Loss= 0.0258, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 15:46:24,712 Iter 470, Minibatch Loss= 0.0405, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 15:46:25,854 Iter 471, Minibatch Loss= 0.0313, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 15:46:26,983 Iter 472, Minibatch Loss= 0.0197, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 15:46:28,110 Iter 473, Minibatch Loss= 0.0194, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 15:46:29,242 Iter 474, Minibatch Loss= 0.0235, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 15:46:30,378 Iter 475, Minibatch Loss= 0.0189, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 15:46:31,516 Iter 476, Minibatch Loss= 0.0249, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 15:46:32,671 Iter 477, Minibatch Loss= 0.0380, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 15:46:33,823 Iter 478, Minibatch Loss= 0.0442, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 15:46:34,985 Iter 479, Minibatch Loss= 0.0414, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 15:46:34,986 Epoch 23, Average loss: 0.0317, learning rate: 0.0010\n",
      "2018-05-27 15:46:44,574 Iter 480, Minibatch Loss= 0.0324, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 15:46:46,429 Iter 480, Minibatch Loss= 0.0459, Training Accuracy= 0.9517, Minibatch error= 4.8%\n",
      "2018-05-27 15:46:47,585 Iter 481, Minibatch Loss= 0.0630, Training Accuracy= 0.9384, Minibatch error= 6.2%\n",
      "2018-05-27 15:46:48,734 Iter 482, Minibatch Loss= 0.0454, Training Accuracy= 0.9363, Minibatch error= 6.4%\n",
      "2018-05-27 15:46:49,879 Iter 483, Minibatch Loss= 0.0379, Training Accuracy= 0.9478, Minibatch error= 5.2%\n",
      "2018-05-27 15:46:51,018 Iter 484, Minibatch Loss= 0.0212, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 15:46:52,177 Iter 485, Minibatch Loss= 0.0257, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 15:46:53,321 Iter 486, Minibatch Loss= 0.0319, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 15:46:54,463 Iter 487, Minibatch Loss= 0.0302, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 15:46:55,608 Iter 488, Minibatch Loss= 0.0233, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 15:46:56,756 Iter 489, Minibatch Loss= 0.0337, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:46:57,907 Iter 490, Minibatch Loss= 0.0258, Training Accuracy= 0.9828, Minibatch error= 1.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:46:59,070 Iter 491, Minibatch Loss= 0.0283, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 15:47:00,219 Iter 492, Minibatch Loss= 0.0307, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 15:47:01,374 Iter 493, Minibatch Loss= 0.0366, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 15:47:02,539 Iter 494, Minibatch Loss= 0.0193, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 15:47:03,700 Iter 495, Minibatch Loss= 0.0220, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 15:47:04,846 Iter 496, Minibatch Loss= 0.0176, Training Accuracy= 0.9949, Minibatch error= 0.5%\n",
      "2018-05-27 15:47:06,004 Iter 497, Minibatch Loss= 0.0337, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 15:47:07,165 Iter 498, Minibatch Loss= 0.0254, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 15:47:08,332 Iter 499, Minibatch Loss= 0.0410, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 15:47:08,333 Epoch 24, Average loss: 0.0311, learning rate: 0.0010\n",
      "2018-05-27 15:47:17,915 Iter 500, Minibatch Loss= 0.0311, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 15:47:19,766 Iter 500, Minibatch Loss= 0.0271, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 15:47:20,923 Iter 501, Minibatch Loss= 0.0376, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 15:47:22,081 Iter 502, Minibatch Loss= 0.0250, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:47:23,248 Iter 503, Minibatch Loss= 0.0354, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 15:47:24,410 Iter 504, Minibatch Loss= 0.0211, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 15:47:25,568 Iter 505, Minibatch Loss= 0.0150, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 15:47:26,714 Iter 506, Minibatch Loss= 0.0398, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 15:47:27,880 Iter 507, Minibatch Loss= 0.0365, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:47:29,031 Iter 508, Minibatch Loss= 0.0370, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 15:47:30,183 Iter 509, Minibatch Loss= 0.0354, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 15:47:31,340 Iter 510, Minibatch Loss= 0.0153, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 15:47:32,496 Iter 511, Minibatch Loss= 0.0232, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 15:47:33,633 Iter 512, Minibatch Loss= 0.0328, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 15:47:34,815 Iter 513, Minibatch Loss= 0.0317, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 15:47:35,976 Iter 514, Minibatch Loss= 0.0236, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 15:47:37,114 Iter 515, Minibatch Loss= 0.0212, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 15:47:38,272 Iter 516, Minibatch Loss= 0.0387, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 15:47:39,441 Iter 517, Minibatch Loss= 0.0252, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 15:47:40,587 Iter 518, Minibatch Loss= 0.0349, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 15:47:41,746 Iter 519, Minibatch Loss= 0.0376, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 15:47:41,747 Epoch 25, Average loss: 0.0303, learning rate: 0.0010\n",
      "2018-05-27 15:47:51,302 Iter 520, Minibatch Loss= 0.0301, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 15:47:53,200 Iter 520, Minibatch Loss= 0.0222, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:47:54,360 Iter 521, Minibatch Loss= 0.0370, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 15:47:55,522 Iter 522, Minibatch Loss= 0.0323, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:47:56,688 Iter 523, Minibatch Loss= 0.0367, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 15:47:57,834 Iter 524, Minibatch Loss= 0.0365, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 15:47:58,994 Iter 525, Minibatch Loss= 0.0324, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 15:48:00,143 Iter 526, Minibatch Loss= 0.0183, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 15:48:01,304 Iter 527, Minibatch Loss= 0.0398, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 15:48:02,483 Iter 528, Minibatch Loss= 0.0151, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 15:48:03,652 Iter 529, Minibatch Loss= 0.0334, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 15:48:04,838 Iter 530, Minibatch Loss= 0.0337, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 15:48:06,006 Iter 531, Minibatch Loss= 0.0223, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 15:48:07,201 Iter 532, Minibatch Loss= 0.0354, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 15:48:08,365 Iter 533, Minibatch Loss= 0.0202, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 15:48:09,540 Iter 534, Minibatch Loss= 0.0367, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 15:48:10,695 Iter 535, Minibatch Loss= 0.0281, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 15:48:11,850 Iter 536, Minibatch Loss= 0.0311, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 15:48:13,009 Iter 537, Minibatch Loss= 0.0279, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 15:48:14,185 Iter 538, Minibatch Loss= 0.0263, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 15:48:15,354 Iter 539, Minibatch Loss= 0.0363, Training Accuracy= 0.9504, Minibatch error= 5.0%\n",
      "2018-05-27 15:48:15,355 Epoch 26, Average loss: 0.0307, learning rate: 0.0010\n",
      "2018-05-27 15:48:24,913 Iter 540, Minibatch Loss= 0.0306, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 15:48:26,787 Iter 540, Minibatch Loss= 0.0249, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 15:48:27,954 Iter 541, Minibatch Loss= 0.0366, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 15:48:29,115 Iter 542, Minibatch Loss= 0.0258, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 15:48:30,300 Iter 543, Minibatch Loss= 0.0225, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 15:48:31,463 Iter 544, Minibatch Loss= 0.0348, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 15:48:32,631 Iter 545, Minibatch Loss= 0.0362, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 15:48:33,791 Iter 546, Minibatch Loss= 0.0348, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 15:48:34,960 Iter 547, Minibatch Loss= 0.0369, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 15:48:36,126 Iter 548, Minibatch Loss= 0.0344, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 15:48:37,354 Iter 549, Minibatch Loss= 0.0173, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 15:48:38,522 Iter 550, Minibatch Loss= 0.0249, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 15:48:39,675 Iter 551, Minibatch Loss= 0.0367, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 15:48:40,842 Iter 552, Minibatch Loss= 0.0321, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 15:48:41,998 Iter 553, Minibatch Loss= 0.0354, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 15:48:43,154 Iter 554, Minibatch Loss= 0.0253, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 15:48:44,325 Iter 555, Minibatch Loss= 0.0131, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 15:48:45,501 Iter 556, Minibatch Loss= 0.0220, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 15:48:46,685 Iter 557, Minibatch Loss= 0.0135, Training Accuracy= 0.9981, Minibatch error= 0.2%\n",
      "2018-05-27 15:48:47,863 Iter 558, Minibatch Loss= 0.0235, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 15:48:49,037 Iter 559, Minibatch Loss= 0.0338, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 15:48:49,038 Epoch 27, Average loss: 0.0283, learning rate: 0.0010\n",
      "2018-05-27 15:48:58,613 Iter 560, Minibatch Loss= 0.0288, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 15:49:00,542 Iter 560, Minibatch Loss= 0.0228, Training Accuracy= 0.9830, Minibatch error= 1.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:49:01,713 Iter 561, Minibatch Loss= 0.0407, Training Accuracy= 0.9470, Minibatch error= 5.3%\n",
      "2018-05-27 15:49:02,870 Iter 562, Minibatch Loss= 0.0257, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 15:49:04,041 Iter 563, Minibatch Loss= 0.0316, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 15:49:05,204 Iter 564, Minibatch Loss= 0.0256, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 15:49:06,390 Iter 565, Minibatch Loss= 0.0217, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 15:49:07,581 Iter 566, Minibatch Loss= 0.0230, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 15:49:08,749 Iter 567, Minibatch Loss= 0.0221, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 15:49:09,925 Iter 568, Minibatch Loss= 0.0203, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 15:49:11,103 Iter 569, Minibatch Loss= 0.0167, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 15:49:12,287 Iter 570, Minibatch Loss= 0.0394, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 15:49:13,485 Iter 571, Minibatch Loss= 0.0350, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 15:49:14,671 Iter 572, Minibatch Loss= 0.0288, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 15:49:15,845 Iter 573, Minibatch Loss= 0.0170, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 15:49:17,016 Iter 574, Minibatch Loss= 0.0217, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 15:49:18,192 Iter 575, Minibatch Loss= 0.0124, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 15:49:19,358 Iter 576, Minibatch Loss= 0.0287, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:49:20,525 Iter 577, Minibatch Loss= 0.0201, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 15:49:21,698 Iter 578, Minibatch Loss= 0.0344, Training Accuracy= 0.9555, Minibatch error= 4.5%\n",
      "2018-05-27 15:49:22,902 Iter 579, Minibatch Loss= 0.0343, Training Accuracy= 0.9573, Minibatch error= 4.3%\n",
      "2018-05-27 15:49:22,903 Epoch 28, Average loss: 0.0256, learning rate: 0.0010\n",
      "2018-05-27 15:49:32,485 Iter 580, Minibatch Loss= 0.0325, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 15:49:34,521 Iter 580, Minibatch Loss= 0.0459, Training Accuracy= 0.9328, Minibatch error= 6.7%\n",
      "2018-05-27 15:49:35,701 Iter 581, Minibatch Loss= 0.0344, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 15:49:36,874 Iter 582, Minibatch Loss= 0.0350, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 15:49:38,055 Iter 583, Minibatch Loss= 0.0369, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:49:39,264 Iter 584, Minibatch Loss= 0.0329, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 15:49:40,449 Iter 585, Minibatch Loss= 0.0388, Training Accuracy= 0.9432, Minibatch error= 5.7%\n",
      "2018-05-27 15:49:41,627 Iter 586, Minibatch Loss= 0.0224, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 15:49:42,803 Iter 587, Minibatch Loss= 0.0365, Training Accuracy= 0.9454, Minibatch error= 5.5%\n",
      "2018-05-27 15:49:43,992 Iter 588, Minibatch Loss= 0.0221, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 15:49:45,161 Iter 589, Minibatch Loss= 0.0333, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 15:49:46,333 Iter 590, Minibatch Loss= 0.0230, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 15:49:47,520 Iter 591, Minibatch Loss= 0.0272, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 15:49:48,717 Iter 592, Minibatch Loss= 0.0317, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 15:49:49,896 Iter 593, Minibatch Loss= 0.0434, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 15:49:51,061 Iter 594, Minibatch Loss= 0.0251, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:49:52,254 Iter 595, Minibatch Loss= 0.0344, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 15:49:53,421 Iter 596, Minibatch Loss= 0.0186, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 15:49:54,591 Iter 597, Minibatch Loss= 0.0365, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 15:49:55,764 Iter 598, Minibatch Loss= 0.0364, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 15:49:56,954 Iter 599, Minibatch Loss= 0.0244, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:49:56,955 Epoch 29, Average loss: 0.0327, learning rate: 0.0010\n",
      "2018-05-27 15:50:06,547 Iter 600, Minibatch Loss= 0.0289, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 15:50:08,527 Iter 600, Minibatch Loss= 0.0337, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 15:50:09,681 Iter 601, Minibatch Loss= 0.0326, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 15:50:10,844 Iter 602, Minibatch Loss= 0.0211, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 15:50:12,029 Iter 603, Minibatch Loss= 0.0282, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 15:50:13,209 Iter 604, Minibatch Loss= 0.0337, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 15:50:14,384 Iter 605, Minibatch Loss= 0.0355, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 15:50:15,569 Iter 606, Minibatch Loss= 0.0185, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 15:50:16,755 Iter 607, Minibatch Loss= 0.0263, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 15:50:17,933 Iter 608, Minibatch Loss= 0.0342, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 15:50:19,107 Iter 609, Minibatch Loss= 0.0270, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 15:50:20,312 Iter 610, Minibatch Loss= 0.0347, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:50:21,473 Iter 611, Minibatch Loss= 0.0245, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 15:50:22,662 Iter 612, Minibatch Loss= 0.0354, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 15:50:23,839 Iter 613, Minibatch Loss= 0.0388, Training Accuracy= 0.9456, Minibatch error= 5.4%\n",
      "2018-05-27 15:50:25,016 Iter 614, Minibatch Loss= 0.0175, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 15:50:26,186 Iter 615, Minibatch Loss= 0.0222, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 15:50:27,377 Iter 616, Minibatch Loss= 0.0361, Training Accuracy= 0.9471, Minibatch error= 5.3%\n",
      "2018-05-27 15:50:28,574 Iter 617, Minibatch Loss= 0.0321, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 15:50:29,754 Iter 618, Minibatch Loss= 0.0293, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:50:30,937 Iter 619, Minibatch Loss= 0.0375, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 15:50:30,939 Epoch 30, Average loss: 0.0304, learning rate: 0.0010\n",
      "2018-05-27 15:50:40,555 Iter 620, Minibatch Loss= 0.0280, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 15:50:42,550 Iter 620, Minibatch Loss= 0.0335, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 15:50:43,737 Iter 621, Minibatch Loss= 0.0243, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 15:50:44,913 Iter 622, Minibatch Loss= 0.0359, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 15:50:46,083 Iter 623, Minibatch Loss= 0.0299, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 15:50:47,272 Iter 624, Minibatch Loss= 0.0337, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:50:48,467 Iter 625, Minibatch Loss= 0.0224, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 15:50:49,647 Iter 626, Minibatch Loss= 0.0261, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 15:50:50,823 Iter 627, Minibatch Loss= 0.0376, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 15:50:52,031 Iter 628, Minibatch Loss= 0.0352, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 15:50:53,209 Iter 629, Minibatch Loss= 0.0345, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 15:50:54,388 Iter 630, Minibatch Loss= 0.0207, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 15:50:55,587 Iter 631, Minibatch Loss= 0.0360, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 15:50:56,756 Iter 632, Minibatch Loss= 0.0192, Training Accuracy= 0.9881, Minibatch error= 1.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:50:57,960 Iter 633, Minibatch Loss= 0.0469, Training Accuracy= 0.9336, Minibatch error= 6.6%\n",
      "2018-05-27 15:50:59,136 Iter 634, Minibatch Loss= 0.0394, Training Accuracy= 0.9464, Minibatch error= 5.4%\n",
      "2018-05-27 15:51:00,312 Iter 635, Minibatch Loss= 0.0177, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 15:51:01,503 Iter 636, Minibatch Loss= 0.0330, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 15:51:02,706 Iter 637, Minibatch Loss= 0.0288, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:51:03,883 Iter 638, Minibatch Loss= 0.0132, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 15:51:05,068 Iter 639, Minibatch Loss= 0.0170, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 15:51:05,069 Epoch 31, Average loss: 0.0292, learning rate: 0.0010\n",
      "2018-05-27 15:51:14,630 Iter 640, Minibatch Loss= 0.0281, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 15:51:16,658 Iter 640, Minibatch Loss= 0.0192, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:51:17,846 Iter 641, Minibatch Loss= 0.0351, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:51:19,030 Iter 642, Minibatch Loss= 0.0349, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 15:51:20,208 Iter 643, Minibatch Loss= 0.0145, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 15:51:21,382 Iter 644, Minibatch Loss= 0.0357, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 15:51:22,566 Iter 645, Minibatch Loss= 0.0282, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 15:51:23,758 Iter 646, Minibatch Loss= 0.0115, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 15:51:24,935 Iter 647, Minibatch Loss= 0.0256, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 15:51:26,135 Iter 648, Minibatch Loss= 0.0518, Training Accuracy= 0.9114, Minibatch error= 8.9%\n",
      "2018-05-27 15:51:27,314 Iter 649, Minibatch Loss= 0.0202, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 15:51:28,511 Iter 650, Minibatch Loss= 0.0374, Training Accuracy= 0.9541, Minibatch error= 4.6%\n",
      "2018-05-27 15:51:29,682 Iter 651, Minibatch Loss= 0.0318, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 15:51:30,878 Iter 652, Minibatch Loss= 0.0200, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:51:32,054 Iter 653, Minibatch Loss= 0.0333, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 15:51:33,227 Iter 654, Minibatch Loss= 0.0144, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 15:51:34,417 Iter 655, Minibatch Loss= 0.0202, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 15:51:35,593 Iter 656, Minibatch Loss= 0.0317, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 15:51:36,758 Iter 657, Minibatch Loss= 0.0326, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 15:51:37,913 Iter 658, Minibatch Loss= 0.0333, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 15:51:39,067 Iter 659, Minibatch Loss= 0.0263, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 15:51:39,068 Epoch 32, Average loss: 0.0286, learning rate: 0.0010\n",
      "2018-05-27 15:51:48,456 Iter 660, Minibatch Loss= 0.0278, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 15:51:50,526 Iter 660, Minibatch Loss= 0.0363, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 15:51:51,722 Iter 661, Minibatch Loss= 0.0227, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 15:51:52,920 Iter 662, Minibatch Loss= 0.0172, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 15:51:54,153 Iter 663, Minibatch Loss= 0.0286, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 15:51:55,360 Iter 664, Minibatch Loss= 0.0360, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 15:51:56,584 Iter 665, Minibatch Loss= 0.0196, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 15:51:57,791 Iter 666, Minibatch Loss= 0.0348, Training Accuracy= 0.9535, Minibatch error= 4.6%\n",
      "2018-05-27 15:51:58,996 Iter 667, Minibatch Loss= 0.0323, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 15:52:00,211 Iter 668, Minibatch Loss= 0.0329, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 15:52:01,421 Iter 669, Minibatch Loss= 0.0341, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:52:02,633 Iter 670, Minibatch Loss= 0.0397, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 15:52:03,848 Iter 671, Minibatch Loss= 0.0348, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 15:52:05,056 Iter 672, Minibatch Loss= 0.0264, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 15:52:06,266 Iter 673, Minibatch Loss= 0.0292, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 15:52:07,487 Iter 674, Minibatch Loss= 0.0351, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 15:52:08,707 Iter 675, Minibatch Loss= 0.0342, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 15:52:09,915 Iter 676, Minibatch Loss= 0.0251, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 15:52:11,140 Iter 677, Minibatch Loss= 0.0238, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 15:52:12,352 Iter 678, Minibatch Loss= 0.0336, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 15:52:13,563 Iter 679, Minibatch Loss= 0.0341, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 15:52:13,564 Epoch 33, Average loss: 0.0293, learning rate: 0.0010\n",
      "2018-05-27 15:52:22,999 Iter 680, Minibatch Loss= 0.0264, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 15:52:25,027 Iter 680, Minibatch Loss= 0.0194, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 15:52:26,190 Iter 681, Minibatch Loss= 0.0124, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 15:52:27,350 Iter 682, Minibatch Loss= 0.0117, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 15:52:28,509 Iter 683, Minibatch Loss= 0.0259, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 15:52:29,681 Iter 684, Minibatch Loss= 0.0426, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 15:52:30,837 Iter 685, Minibatch Loss= 0.0136, Training Accuracy= 0.9938, Minibatch error= 0.6%\n",
      "2018-05-27 15:52:32,000 Iter 686, Minibatch Loss= 0.0372, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:52:33,153 Iter 687, Minibatch Loss= 0.0142, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 15:52:34,322 Iter 688, Minibatch Loss= 0.0358, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 15:52:35,478 Iter 689, Minibatch Loss= 0.0370, Training Accuracy= 0.9412, Minibatch error= 5.9%\n",
      "2018-05-27 15:52:36,652 Iter 690, Minibatch Loss= 0.0371, Training Accuracy= 0.9360, Minibatch error= 6.4%\n",
      "2018-05-27 15:52:37,847 Iter 691, Minibatch Loss= 0.0389, Training Accuracy= 0.9320, Minibatch error= 6.8%\n",
      "2018-05-27 15:52:39,052 Iter 692, Minibatch Loss= 0.0380, Training Accuracy= 0.9387, Minibatch error= 6.1%\n",
      "2018-05-27 15:52:40,249 Iter 693, Minibatch Loss= 0.0245, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 15:52:41,467 Iter 694, Minibatch Loss= 0.0230, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 15:52:42,652 Iter 695, Minibatch Loss= 0.0317, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:52:43,846 Iter 696, Minibatch Loss= 0.0159, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 15:52:45,034 Iter 697, Minibatch Loss= 0.0285, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 15:52:46,227 Iter 698, Minibatch Loss= 0.0182, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 15:52:47,445 Iter 699, Minibatch Loss= 0.0174, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 15:52:47,446 Epoch 34, Average loss: 0.0266, learning rate: 0.0010\n",
      "2018-05-27 15:52:56,977 Iter 700, Minibatch Loss= 0.0274, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 15:52:59,009 Iter 700, Minibatch Loss= 0.0206, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 15:53:00,188 Iter 701, Minibatch Loss= 0.0329, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 15:53:01,367 Iter 702, Minibatch Loss= 0.0204, Training Accuracy= 0.9850, Minibatch error= 1.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:53:02,556 Iter 703, Minibatch Loss= 0.0277, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:53:03,746 Iter 704, Minibatch Loss= 0.0353, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 15:53:04,936 Iter 705, Minibatch Loss= 0.0312, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:53:06,124 Iter 706, Minibatch Loss= 0.0335, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 15:53:07,307 Iter 707, Minibatch Loss= 0.0327, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 15:53:08,508 Iter 708, Minibatch Loss= 0.0184, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 15:53:09,709 Iter 709, Minibatch Loss= 0.0320, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 15:53:10,895 Iter 710, Minibatch Loss= 0.0220, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 15:53:12,090 Iter 711, Minibatch Loss= 0.0363, Training Accuracy= 0.9447, Minibatch error= 5.5%\n",
      "2018-05-27 15:53:13,296 Iter 712, Minibatch Loss= 0.0258, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 15:53:14,479 Iter 713, Minibatch Loss= 0.0352, Training Accuracy= 0.9453, Minibatch error= 5.5%\n",
      "2018-05-27 15:53:15,660 Iter 714, Minibatch Loss= 0.0339, Training Accuracy= 0.9526, Minibatch error= 4.7%\n",
      "2018-05-27 15:53:16,856 Iter 715, Minibatch Loss= 0.0207, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 15:53:18,049 Iter 716, Minibatch Loss= 0.0424, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 15:53:19,254 Iter 717, Minibatch Loss= 0.0383, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 15:53:20,460 Iter 718, Minibatch Loss= 0.0227, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 15:53:21,648 Iter 719, Minibatch Loss= 0.0145, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 15:53:21,649 Epoch 35, Average loss: 0.0288, learning rate: 0.0010\n",
      "2018-05-27 15:53:31,175 Iter 720, Minibatch Loss= 0.0267, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 15:53:33,205 Iter 720, Minibatch Loss= 0.0168, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 15:53:34,393 Iter 721, Minibatch Loss= 0.0246, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 15:53:35,603 Iter 722, Minibatch Loss= 0.0181, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 15:53:36,820 Iter 723, Minibatch Loss= 0.0344, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 15:53:38,005 Iter 724, Minibatch Loss= 0.0272, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 15:53:39,210 Iter 725, Minibatch Loss= 0.0274, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 15:53:40,385 Iter 726, Minibatch Loss= 0.0299, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:53:41,582 Iter 727, Minibatch Loss= 0.0260, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 15:53:42,786 Iter 728, Minibatch Loss= 0.0317, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 15:53:43,975 Iter 729, Minibatch Loss= 0.0312, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 15:53:45,186 Iter 730, Minibatch Loss= 0.0182, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 15:53:46,375 Iter 731, Minibatch Loss= 0.0147, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 15:53:47,581 Iter 732, Minibatch Loss= 0.0305, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 15:53:48,789 Iter 733, Minibatch Loss= 0.0287, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:53:49,995 Iter 734, Minibatch Loss= 0.0309, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 15:53:51,178 Iter 735, Minibatch Loss= 0.0320, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 15:53:52,371 Iter 736, Minibatch Loss= 0.0150, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 15:53:53,585 Iter 737, Minibatch Loss= 0.0289, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:53:54,772 Iter 738, Minibatch Loss= 0.0299, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:53:56,001 Iter 739, Minibatch Loss= 0.0255, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 15:53:56,002 Epoch 36, Average loss: 0.0268, learning rate: 0.0010\n",
      "2018-05-27 15:54:05,589 Iter 740, Minibatch Loss= 0.0255, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 15:54:07,859 Iter 740, Minibatch Loss= 0.0190, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 15:54:09,069 Iter 741, Minibatch Loss= 0.0326, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 15:54:10,268 Iter 742, Minibatch Loss= 0.0193, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 15:54:11,487 Iter 743, Minibatch Loss= 0.0388, Training Accuracy= 0.9423, Minibatch error= 5.8%\n",
      "2018-05-27 15:54:12,705 Iter 744, Minibatch Loss= 0.0102, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 15:54:13,924 Iter 745, Minibatch Loss= 0.0336, Training Accuracy= 0.9521, Minibatch error= 4.8%\n",
      "2018-05-27 15:54:15,138 Iter 746, Minibatch Loss= 0.0305, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 15:54:16,338 Iter 747, Minibatch Loss= 0.0169, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 15:54:17,550 Iter 748, Minibatch Loss= 0.0214, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 15:54:18,776 Iter 749, Minibatch Loss= 0.0347, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 15:54:19,992 Iter 750, Minibatch Loss= 0.0283, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:54:21,206 Iter 751, Minibatch Loss= 0.0247, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:54:22,418 Iter 752, Minibatch Loss= 0.0334, Training Accuracy= 0.9535, Minibatch error= 4.6%\n",
      "2018-05-27 15:54:23,661 Iter 753, Minibatch Loss= 0.0189, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 15:54:24,859 Iter 754, Minibatch Loss= 0.0309, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 15:54:26,075 Iter 755, Minibatch Loss= 0.0229, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 15:54:27,290 Iter 756, Minibatch Loss= 0.0143, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 15:54:28,520 Iter 757, Minibatch Loss= 0.0316, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 15:54:29,745 Iter 758, Minibatch Loss= 0.0292, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 15:54:30,975 Iter 759, Minibatch Loss= 0.0117, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 15:54:30,976 Epoch 37, Average loss: 0.0255, learning rate: 0.0010\n",
      "2018-05-27 15:54:40,577 Iter 760, Minibatch Loss= 0.0252, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 15:54:42,689 Iter 760, Minibatch Loss= 0.0332, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 15:54:43,921 Iter 761, Minibatch Loss= 0.0328, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 15:54:45,125 Iter 762, Minibatch Loss= 0.0340, Training Accuracy= 0.9500, Minibatch error= 5.0%\n",
      "2018-05-27 15:54:46,341 Iter 763, Minibatch Loss= 0.0383, Training Accuracy= 0.9337, Minibatch error= 6.6%\n",
      "2018-05-27 15:54:47,562 Iter 764, Minibatch Loss= 0.0203, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 15:54:48,765 Iter 765, Minibatch Loss= 0.0233, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 15:54:49,973 Iter 766, Minibatch Loss= 0.0294, Training Accuracy= 0.9665, Minibatch error= 3.4%\n",
      "2018-05-27 15:54:51,200 Iter 767, Minibatch Loss= 0.0184, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 15:54:52,393 Iter 768, Minibatch Loss= 0.0316, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 15:54:53,586 Iter 769, Minibatch Loss= 0.0302, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 15:54:54,779 Iter 770, Minibatch Loss= 0.0314, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 15:54:56,005 Iter 771, Minibatch Loss= 0.0138, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 15:54:57,215 Iter 772, Minibatch Loss= 0.0150, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 15:54:58,414 Iter 773, Minibatch Loss= 0.0107, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 15:54:59,611 Iter 774, Minibatch Loss= 0.0146, Training Accuracy= 0.9901, Minibatch error= 1.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:55:00,821 Iter 775, Minibatch Loss= 0.0239, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 15:55:02,014 Iter 776, Minibatch Loss= 0.0284, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 15:55:03,227 Iter 777, Minibatch Loss= 0.0195, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 15:55:04,436 Iter 778, Minibatch Loss= 0.0312, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 15:55:05,653 Iter 779, Minibatch Loss= 0.0172, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 15:55:05,654 Epoch 38, Average loss: 0.0252, learning rate: 0.0010\n",
      "2018-05-27 15:55:15,264 Iter 780, Minibatch Loss= 0.0247, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 15:55:17,379 Iter 780, Minibatch Loss= 0.0306, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 15:55:18,609 Iter 781, Minibatch Loss= 0.0154, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 15:55:19,812 Iter 782, Minibatch Loss= 0.0302, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 15:55:21,023 Iter 783, Minibatch Loss= 0.0315, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 15:55:22,225 Iter 784, Minibatch Loss= 0.0160, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 15:55:23,453 Iter 785, Minibatch Loss= 0.0204, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 15:55:24,678 Iter 786, Minibatch Loss= 0.0216, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 15:55:25,901 Iter 787, Minibatch Loss= 0.0317, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 15:55:27,123 Iter 788, Minibatch Loss= 0.0170, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 15:55:28,372 Iter 789, Minibatch Loss= 0.0314, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 15:55:29,615 Iter 790, Minibatch Loss= 0.0162, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 15:55:30,826 Iter 791, Minibatch Loss= 0.0328, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 15:55:32,048 Iter 792, Minibatch Loss= 0.0213, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 15:55:33,266 Iter 793, Minibatch Loss= 0.0257, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 15:55:34,494 Iter 794, Minibatch Loss= 0.0333, Training Accuracy= 0.9538, Minibatch error= 4.6%\n",
      "2018-05-27 15:55:35,705 Iter 795, Minibatch Loss= 0.0281, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:55:36,935 Iter 796, Minibatch Loss= 0.0084, Training Accuracy= 0.9987, Minibatch error= 0.1%\n",
      "2018-05-27 15:55:38,142 Iter 797, Minibatch Loss= 0.0258, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 15:55:39,344 Iter 798, Minibatch Loss= 0.0198, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 15:55:40,581 Iter 799, Minibatch Loss= 0.0269, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 15:55:40,582 Epoch 39, Average loss: 0.0245, learning rate: 0.0010\n",
      "2018-05-27 15:55:50,124 Iter 800, Minibatch Loss= 0.0254, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 15:55:52,295 Iter 800, Minibatch Loss= 0.0330, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 15:55:53,512 Iter 801, Minibatch Loss= 0.0406, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 15:55:54,755 Iter 802, Minibatch Loss= 0.0419, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:55:55,987 Iter 803, Minibatch Loss= 0.0331, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 15:55:57,201 Iter 804, Minibatch Loss= 0.0322, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:55:58,414 Iter 805, Minibatch Loss= 0.0285, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 15:55:59,640 Iter 806, Minibatch Loss= 0.0418, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 15:56:00,855 Iter 807, Minibatch Loss= 0.0184, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 15:56:02,078 Iter 808, Minibatch Loss= 0.0264, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 15:56:03,299 Iter 809, Minibatch Loss= 0.0218, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 15:56:04,523 Iter 810, Minibatch Loss= 0.0244, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 15:56:05,739 Iter 811, Minibatch Loss= 0.0300, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 15:56:06,974 Iter 812, Minibatch Loss= 0.0158, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 15:56:08,198 Iter 813, Minibatch Loss= 0.0316, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 15:56:09,422 Iter 814, Minibatch Loss= 0.0295, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 15:56:10,644 Iter 815, Minibatch Loss= 0.0134, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 15:56:11,879 Iter 816, Minibatch Loss= 0.0248, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 15:56:13,078 Iter 817, Minibatch Loss= 0.0303, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 15:56:14,313 Iter 818, Minibatch Loss= 0.0330, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 15:56:15,548 Iter 819, Minibatch Loss= 0.0309, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 15:56:15,549 Epoch 40, Average loss: 0.0291, learning rate: 0.0010\n",
      "2018-05-27 15:56:25,159 Iter 820, Minibatch Loss= 0.0256, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 15:56:27,354 Iter 820, Minibatch Loss= 0.0289, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 15:56:28,568 Iter 821, Minibatch Loss= 0.0339, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 15:56:29,807 Iter 822, Minibatch Loss= 0.0282, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 15:56:31,041 Iter 823, Minibatch Loss= 0.0336, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 15:56:32,268 Iter 824, Minibatch Loss= 0.0098, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 15:56:33,502 Iter 825, Minibatch Loss= 0.0152, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 15:56:34,740 Iter 826, Minibatch Loss= 0.0345, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 15:56:35,975 Iter 827, Minibatch Loss= 0.0246, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 15:56:37,210 Iter 828, Minibatch Loss= 0.0312, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 15:56:38,438 Iter 829, Minibatch Loss= 0.0152, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 15:56:39,661 Iter 830, Minibatch Loss= 0.0181, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 15:56:40,882 Iter 831, Minibatch Loss= 0.0171, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 15:56:42,107 Iter 832, Minibatch Loss= 0.0334, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 15:56:43,317 Iter 833, Minibatch Loss= 0.0209, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 15:56:44,529 Iter 834, Minibatch Loss= 0.0299, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 15:56:45,751 Iter 835, Minibatch Loss= 0.0186, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 15:56:46,976 Iter 836, Minibatch Loss= 0.0298, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:56:48,219 Iter 837, Minibatch Loss= 0.0204, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 15:56:49,450 Iter 838, Minibatch Loss= 0.0210, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 15:56:50,712 Iter 839, Minibatch Loss= 0.0104, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 15:56:50,713 Epoch 41, Average loss: 0.0240, learning rate: 0.0010\n",
      "2018-05-27 15:57:00,297 Iter 840, Minibatch Loss= 0.0250, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 15:57:02,519 Iter 840, Minibatch Loss= 0.0311, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:57:03,744 Iter 841, Minibatch Loss= 0.0187, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 15:57:04,988 Iter 842, Minibatch Loss= 0.0185, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 15:57:06,215 Iter 843, Minibatch Loss= 0.0313, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 15:57:07,485 Iter 844, Minibatch Loss= 0.0147, Training Accuracy= 0.9885, Minibatch error= 1.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:57:08,724 Iter 845, Minibatch Loss= 0.0238, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 15:57:09,976 Iter 846, Minibatch Loss= 0.0339, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 15:57:11,201 Iter 847, Minibatch Loss= 0.0156, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 15:57:12,446 Iter 848, Minibatch Loss= 0.0186, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 15:57:13,670 Iter 849, Minibatch Loss= 0.0109, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 15:57:14,924 Iter 850, Minibatch Loss= 0.0189, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 15:57:16,169 Iter 851, Minibatch Loss= 0.0269, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 15:57:17,437 Iter 852, Minibatch Loss= 0.0151, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 15:57:18,697 Iter 853, Minibatch Loss= 0.0354, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 15:57:19,935 Iter 854, Minibatch Loss= 0.0300, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 15:57:21,174 Iter 855, Minibatch Loss= 0.0216, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:57:22,398 Iter 856, Minibatch Loss= 0.0117, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 15:57:23,621 Iter 857, Minibatch Loss= 0.0305, Training Accuracy= 0.9531, Minibatch error= 4.7%\n",
      "2018-05-27 15:57:24,868 Iter 858, Minibatch Loss= 0.0242, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 15:57:26,084 Iter 859, Minibatch Loss= 0.0116, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 15:57:26,085 Epoch 42, Average loss: 0.0228, learning rate: 0.0010\n",
      "2018-05-27 15:57:35,704 Iter 860, Minibatch Loss= 0.0241, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 15:57:37,995 Iter 860, Minibatch Loss= 0.0252, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 15:57:39,244 Iter 861, Minibatch Loss= 0.0171, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 15:57:40,488 Iter 862, Minibatch Loss= 0.0181, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 15:57:41,728 Iter 863, Minibatch Loss= 0.0324, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 15:57:42,974 Iter 864, Minibatch Loss= 0.0147, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 15:57:44,221 Iter 865, Minibatch Loss= 0.0138, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 15:57:45,469 Iter 866, Minibatch Loss= 0.0194, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 15:57:46,685 Iter 867, Minibatch Loss= 0.0313, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 15:57:47,938 Iter 868, Minibatch Loss= 0.0340, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 15:57:49,180 Iter 869, Minibatch Loss= 0.0177, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 15:57:50,416 Iter 870, Minibatch Loss= 0.0208, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 15:57:51,661 Iter 871, Minibatch Loss= 0.0302, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 15:57:52,891 Iter 872, Minibatch Loss= 0.0320, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 15:57:54,122 Iter 873, Minibatch Loss= 0.0336, Training Accuracy= 0.9509, Minibatch error= 4.9%\n",
      "2018-05-27 15:57:55,352 Iter 874, Minibatch Loss= 0.0080, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 15:57:56,610 Iter 875, Minibatch Loss= 0.0173, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 15:57:57,846 Iter 876, Minibatch Loss= 0.0138, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 15:57:59,079 Iter 877, Minibatch Loss= 0.0098, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 15:58:00,325 Iter 878, Minibatch Loss= 0.0329, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 15:58:01,583 Iter 879, Minibatch Loss= 0.0286, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 15:58:01,584 Epoch 43, Average loss: 0.0232, learning rate: 0.0010\n",
      "2018-05-27 15:58:11,169 Iter 880, Minibatch Loss= 0.0241, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 15:58:13,410 Iter 880, Minibatch Loss= 0.0094, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 15:58:14,642 Iter 881, Minibatch Loss= 0.0283, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 15:58:15,877 Iter 882, Minibatch Loss= 0.0201, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 15:58:17,124 Iter 883, Minibatch Loss= 0.0268, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 15:58:18,359 Iter 884, Minibatch Loss= 0.0260, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 15:58:19,602 Iter 885, Minibatch Loss= 0.0224, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 15:58:20,850 Iter 886, Minibatch Loss= 0.0326, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 15:58:22,086 Iter 887, Minibatch Loss= 0.0288, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:58:23,321 Iter 888, Minibatch Loss= 0.0161, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 15:58:24,555 Iter 889, Minibatch Loss= 0.0162, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 15:58:25,782 Iter 890, Minibatch Loss= 0.0326, Training Accuracy= 0.9521, Minibatch error= 4.8%\n",
      "2018-05-27 15:58:27,026 Iter 891, Minibatch Loss= 0.0207, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 15:58:28,265 Iter 892, Minibatch Loss= 0.0323, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 15:58:29,527 Iter 893, Minibatch Loss= 0.0152, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 15:58:30,760 Iter 894, Minibatch Loss= 0.0172, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 15:58:32,008 Iter 895, Minibatch Loss= 0.0085, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 15:58:33,250 Iter 896, Minibatch Loss= 0.0101, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 15:58:34,496 Iter 897, Minibatch Loss= 0.0289, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 15:58:35,748 Iter 898, Minibatch Loss= 0.0219, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 15:58:36,994 Iter 899, Minibatch Loss= 0.0144, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 15:58:36,995 Epoch 44, Average loss: 0.0217, learning rate: 0.0010\n",
      "2018-05-27 15:58:46,541 Iter 900, Minibatch Loss= 0.0241, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 15:58:48,837 Iter 900, Minibatch Loss= 0.0319, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 15:58:50,075 Iter 901, Minibatch Loss= 0.0279, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 15:58:51,300 Iter 902, Minibatch Loss= 0.0101, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 15:58:52,546 Iter 903, Minibatch Loss= 0.0313, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 15:58:53,785 Iter 904, Minibatch Loss= 0.0324, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 15:58:55,007 Iter 905, Minibatch Loss= 0.0281, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 15:58:56,269 Iter 906, Minibatch Loss= 0.0252, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:58:57,509 Iter 907, Minibatch Loss= 0.0164, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 15:58:58,776 Iter 908, Minibatch Loss= 0.0289, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 15:59:00,016 Iter 909, Minibatch Loss= 0.0108, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 15:59:01,263 Iter 910, Minibatch Loss= 0.0212, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 15:59:02,517 Iter 911, Minibatch Loss= 0.0273, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 15:59:03,762 Iter 912, Minibatch Loss= 0.0302, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 15:59:04,997 Iter 913, Minibatch Loss= 0.0113, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 15:59:06,263 Iter 914, Minibatch Loss= 0.0259, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 15:59:07,530 Iter 915, Minibatch Loss= 0.0101, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 15:59:08,787 Iter 916, Minibatch Loss= 0.0291, Training Accuracy= 0.9657, Minibatch error= 3.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 15:59:10,015 Iter 917, Minibatch Loss= 0.0192, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 15:59:11,283 Iter 918, Minibatch Loss= 0.0181, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 15:59:12,531 Iter 919, Minibatch Loss= 0.0109, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 15:59:12,532 Epoch 45, Average loss: 0.0230, learning rate: 0.0010\n",
      "2018-05-27 15:59:22,102 Iter 920, Minibatch Loss= 0.0235, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 15:59:24,505 Iter 920, Minibatch Loss= 0.0221, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 15:59:25,751 Iter 921, Minibatch Loss= 0.0304, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 15:59:26,974 Iter 922, Minibatch Loss= 0.0303, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 15:59:28,218 Iter 923, Minibatch Loss= 0.0188, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 15:59:29,480 Iter 924, Minibatch Loss= 0.0081, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 15:59:30,714 Iter 925, Minibatch Loss= 0.0312, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 15:59:31,958 Iter 926, Minibatch Loss= 0.0301, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 15:59:33,211 Iter 927, Minibatch Loss= 0.0328, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 15:59:34,499 Iter 928, Minibatch Loss= 0.0258, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 15:59:35,748 Iter 929, Minibatch Loss= 0.0293, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 15:59:37,000 Iter 930, Minibatch Loss= 0.0186, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 15:59:38,243 Iter 931, Minibatch Loss= 0.0274, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 15:59:39,515 Iter 932, Minibatch Loss= 0.0231, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 15:59:40,761 Iter 933, Minibatch Loss= 0.0274, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 15:59:42,034 Iter 934, Minibatch Loss= 0.0240, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 15:59:43,288 Iter 935, Minibatch Loss= 0.0310, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 15:59:44,552 Iter 936, Minibatch Loss= 0.0292, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 15:59:45,779 Iter 937, Minibatch Loss= 0.0255, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 15:59:47,028 Iter 938, Minibatch Loss= 0.0175, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 15:59:48,269 Iter 939, Minibatch Loss= 0.0273, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 15:59:48,270 Epoch 46, Average loss: 0.0260, learning rate: 0.0010\n",
      "2018-05-27 15:59:57,829 Iter 940, Minibatch Loss= 0.0228, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 16:00:00,159 Iter 940, Minibatch Loss= 0.0200, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:00:01,423 Iter 941, Minibatch Loss= 0.0194, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:00:02,687 Iter 942, Minibatch Loss= 0.0298, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:00:03,946 Iter 943, Minibatch Loss= 0.0159, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 16:00:05,204 Iter 944, Minibatch Loss= 0.0185, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 16:00:06,468 Iter 945, Minibatch Loss= 0.0309, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:00:07,734 Iter 946, Minibatch Loss= 0.0170, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 16:00:09,003 Iter 947, Minibatch Loss= 0.0067, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 16:00:10,272 Iter 948, Minibatch Loss= 0.0291, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 16:00:11,551 Iter 949, Minibatch Loss= 0.0101, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 16:00:12,810 Iter 950, Minibatch Loss= 0.0179, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:00:14,091 Iter 951, Minibatch Loss= 0.0279, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:00:15,363 Iter 952, Minibatch Loss= 0.0357, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:00:16,640 Iter 953, Minibatch Loss= 0.0343, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:00:17,901 Iter 954, Minibatch Loss= 0.0170, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:00:19,164 Iter 955, Minibatch Loss= 0.0295, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 16:00:20,424 Iter 956, Minibatch Loss= 0.0282, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 16:00:21,695 Iter 957, Minibatch Loss= 0.0192, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 16:00:22,980 Iter 958, Minibatch Loss= 0.0241, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 16:00:24,263 Iter 959, Minibatch Loss= 0.0291, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 16:00:24,264 Epoch 47, Average loss: 0.0239, learning rate: 0.0010\n",
      "2018-05-27 16:00:33,845 Iter 960, Minibatch Loss= 0.0231, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 16:00:36,270 Iter 960, Minibatch Loss= 0.0144, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:00:37,548 Iter 961, Minibatch Loss= 0.0280, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 16:00:38,799 Iter 962, Minibatch Loss= 0.0235, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:00:40,090 Iter 963, Minibatch Loss= 0.0169, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:00:41,354 Iter 964, Minibatch Loss= 0.0209, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 16:00:42,695 Iter 965, Minibatch Loss= 0.0128, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:00:43,955 Iter 966, Minibatch Loss= 0.0306, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 16:00:45,244 Iter 967, Minibatch Loss= 0.0242, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 16:00:46,495 Iter 968, Minibatch Loss= 0.0234, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 16:00:47,786 Iter 969, Minibatch Loss= 0.0189, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 16:00:49,073 Iter 970, Minibatch Loss= 0.0121, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 16:00:50,344 Iter 971, Minibatch Loss= 0.0264, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 16:00:51,606 Iter 972, Minibatch Loss= 0.0232, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 16:00:52,883 Iter 973, Minibatch Loss= 0.0223, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 16:00:54,156 Iter 974, Minibatch Loss= 0.0197, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:00:55,472 Iter 975, Minibatch Loss= 0.0154, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:00:56,735 Iter 976, Minibatch Loss= 0.0312, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 16:00:57,984 Iter 977, Minibatch Loss= 0.0380, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 16:00:59,241 Iter 978, Minibatch Loss= 0.0347, Training Accuracy= 0.9423, Minibatch error= 5.8%\n",
      "2018-05-27 16:01:00,505 Iter 979, Minibatch Loss= 0.0242, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:01:00,506 Epoch 48, Average loss: 0.0234, learning rate: 0.0010\n",
      "2018-05-27 16:01:10,108 Iter 980, Minibatch Loss= 0.0245, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:01:12,498 Iter 980, Minibatch Loss= 0.0128, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:01:13,743 Iter 981, Minibatch Loss= 0.0280, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 16:01:15,023 Iter 982, Minibatch Loss= 0.0313, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 16:01:16,274 Iter 983, Minibatch Loss= 0.0298, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:01:17,529 Iter 984, Minibatch Loss= 0.0332, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:01:18,790 Iter 985, Minibatch Loss= 0.0309, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:01:20,032 Iter 986, Minibatch Loss= 0.0285, Training Accuracy= 0.9643, Minibatch error= 3.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:01:21,289 Iter 987, Minibatch Loss= 0.0244, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 16:01:22,540 Iter 988, Minibatch Loss= 0.0065, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 16:01:23,782 Iter 989, Minibatch Loss= 0.0207, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 16:01:25,029 Iter 990, Minibatch Loss= 0.0347, Training Accuracy= 0.9491, Minibatch error= 5.1%\n",
      "2018-05-27 16:01:26,281 Iter 991, Minibatch Loss= 0.0261, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:01:27,537 Iter 992, Minibatch Loss= 0.0320, Training Accuracy= 0.9463, Minibatch error= 5.4%\n",
      "2018-05-27 16:01:28,794 Iter 993, Minibatch Loss= 0.0269, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 16:01:30,040 Iter 994, Minibatch Loss= 0.0300, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 16:01:31,288 Iter 995, Minibatch Loss= 0.0307, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 16:01:32,556 Iter 996, Minibatch Loss= 0.0079, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 16:01:33,807 Iter 997, Minibatch Loss= 0.0158, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:01:35,056 Iter 998, Minibatch Loss= 0.0298, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:01:36,310 Iter 999, Minibatch Loss= 0.0290, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 16:01:36,311 Epoch 49, Average loss: 0.0261, learning rate: 0.0010\n",
      "2018-05-27 16:01:45,893 Iter 1000, Minibatch Loss= 0.0235, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:01:48,346 Iter 1000, Minibatch Loss= 0.0188, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:01:49,623 Iter 1001, Minibatch Loss= 0.0274, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:01:50,898 Iter 1002, Minibatch Loss= 0.0290, Training Accuracy= 0.9665, Minibatch error= 3.4%\n",
      "2018-05-27 16:01:52,160 Iter 1003, Minibatch Loss= 0.0198, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:01:53,418 Iter 1004, Minibatch Loss= 0.0156, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 16:01:54,689 Iter 1005, Minibatch Loss= 0.0290, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 16:01:55,965 Iter 1006, Minibatch Loss= 0.0299, Training Accuracy= 0.9564, Minibatch error= 4.4%\n",
      "2018-05-27 16:01:57,226 Iter 1007, Minibatch Loss= 0.0260, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 16:01:58,512 Iter 1008, Minibatch Loss= 0.0181, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:01:59,760 Iter 1009, Minibatch Loss= 0.0283, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 16:02:01,047 Iter 1010, Minibatch Loss= 0.0455, Training Accuracy= 0.9349, Minibatch error= 6.5%\n",
      "2018-05-27 16:02:02,308 Iter 1011, Minibatch Loss= 0.0303, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 16:02:03,598 Iter 1012, Minibatch Loss= 0.0158, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 16:02:04,855 Iter 1013, Minibatch Loss= 0.0298, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:02:06,155 Iter 1014, Minibatch Loss= 0.0130, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 16:02:07,418 Iter 1015, Minibatch Loss= 0.0200, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 16:02:08,659 Iter 1016, Minibatch Loss= 0.0297, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 16:02:09,923 Iter 1017, Minibatch Loss= 0.0193, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 16:02:11,201 Iter 1018, Minibatch Loss= 0.0288, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:02:12,477 Iter 1019, Minibatch Loss= 0.0321, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:02:12,478 Epoch 50, Average loss: 0.0258, learning rate: 0.0010\n",
      "2018-05-27 16:02:22,073 Iter 1020, Minibatch Loss= 0.0228, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:02:24,472 Iter 1020, Minibatch Loss= 0.0301, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:02:25,755 Iter 1021, Minibatch Loss= 0.0133, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:02:27,047 Iter 1022, Minibatch Loss= 0.0175, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:02:28,320 Iter 1023, Minibatch Loss= 0.0320, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 16:02:29,597 Iter 1024, Minibatch Loss= 0.0291, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:02:30,875 Iter 1025, Minibatch Loss= 0.0288, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 16:02:32,136 Iter 1026, Minibatch Loss= 0.0279, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:02:33,399 Iter 1027, Minibatch Loss= 0.0282, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:02:34,698 Iter 1028, Minibatch Loss= 0.0120, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 16:02:35,955 Iter 1029, Minibatch Loss= 0.0291, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:02:37,218 Iter 1030, Minibatch Loss= 0.0271, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:02:38,505 Iter 1031, Minibatch Loss= 0.0175, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:02:39,776 Iter 1032, Minibatch Loss= 0.0284, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:02:41,053 Iter 1033, Minibatch Loss= 0.0198, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:02:42,314 Iter 1034, Minibatch Loss= 0.0286, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 16:02:43,578 Iter 1035, Minibatch Loss= 0.0289, Training Accuracy= 0.9595, Minibatch error= 4.0%\n",
      "2018-05-27 16:02:44,837 Iter 1036, Minibatch Loss= 0.0262, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 16:02:46,092 Iter 1037, Minibatch Loss= 0.0087, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 16:02:47,349 Iter 1038, Minibatch Loss= 0.0299, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:02:48,635 Iter 1039, Minibatch Loss= 0.0143, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:02:48,637 Epoch 51, Average loss: 0.0245, learning rate: 0.0010\n",
      "2018-05-27 16:02:58,226 Iter 1040, Minibatch Loss= 0.0230, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 16:03:00,666 Iter 1040, Minibatch Loss= 0.0232, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:03:01,951 Iter 1041, Minibatch Loss= 0.0297, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:03:03,209 Iter 1042, Minibatch Loss= 0.0306, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 16:03:04,500 Iter 1043, Minibatch Loss= 0.0185, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:03:05,753 Iter 1044, Minibatch Loss= 0.0104, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 16:03:07,019 Iter 1045, Minibatch Loss= 0.0297, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:03:08,291 Iter 1046, Minibatch Loss= 0.0280, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:03:09,560 Iter 1047, Minibatch Loss= 0.0323, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 16:03:10,822 Iter 1048, Minibatch Loss= 0.0309, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 16:03:12,080 Iter 1049, Minibatch Loss= 0.0284, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 16:03:13,336 Iter 1050, Minibatch Loss= 0.0301, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:03:14,633 Iter 1051, Minibatch Loss= 0.0549, Training Accuracy= 0.9178, Minibatch error= 8.2%\n",
      "2018-05-27 16:03:15,903 Iter 1052, Minibatch Loss= 0.0300, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 16:03:17,175 Iter 1053, Minibatch Loss= 0.0155, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:03:18,425 Iter 1054, Minibatch Loss= 0.0395, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:03:19,690 Iter 1055, Minibatch Loss= 0.0278, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 16:03:20,971 Iter 1056, Minibatch Loss= 0.0314, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:03:22,257 Iter 1057, Minibatch Loss= 0.0216, Training Accuracy= 0.9744, Minibatch error= 2.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:03:23,507 Iter 1058, Minibatch Loss= 0.0199, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 16:03:24,780 Iter 1059, Minibatch Loss= 0.0139, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 16:03:24,782 Epoch 52, Average loss: 0.0276, learning rate: 0.0010\n",
      "2018-05-27 16:03:34,288 Iter 1060, Minibatch Loss= 0.0229, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 16:03:36,682 Iter 1060, Minibatch Loss= 0.0290, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 16:03:37,944 Iter 1061, Minibatch Loss= 0.0294, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:03:39,199 Iter 1062, Minibatch Loss= 0.0300, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:03:40,465 Iter 1063, Minibatch Loss= 0.0242, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 16:03:41,729 Iter 1064, Minibatch Loss= 0.0296, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:03:42,993 Iter 1065, Minibatch Loss= 0.0287, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:03:44,273 Iter 1066, Minibatch Loss= 0.0281, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:03:45,545 Iter 1067, Minibatch Loss= 0.0351, Training Accuracy= 0.9479, Minibatch error= 5.2%\n",
      "2018-05-27 16:03:46,859 Iter 1068, Minibatch Loss= 0.0210, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 16:03:48,114 Iter 1069, Minibatch Loss= 0.0135, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 16:03:49,388 Iter 1070, Minibatch Loss= 0.0305, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:03:50,673 Iter 1071, Minibatch Loss= 0.0169, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:03:51,928 Iter 1072, Minibatch Loss= 0.0169, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:03:53,186 Iter 1073, Minibatch Loss= 0.0259, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 16:03:54,464 Iter 1074, Minibatch Loss= 0.0287, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:03:55,737 Iter 1075, Minibatch Loss= 0.0319, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 16:03:57,048 Iter 1076, Minibatch Loss= 0.0187, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 16:03:58,318 Iter 1077, Minibatch Loss= 0.0169, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 16:03:59,631 Iter 1078, Minibatch Loss= 0.0106, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:04:00,923 Iter 1079, Minibatch Loss= 0.0230, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 16:04:00,924 Epoch 53, Average loss: 0.0249, learning rate: 0.0010\n",
      "2018-05-27 16:04:10,483 Iter 1080, Minibatch Loss= 0.0226, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:04:12,982 Iter 1080, Minibatch Loss= 0.0247, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:04:14,288 Iter 1081, Minibatch Loss= 0.0138, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:04:15,593 Iter 1082, Minibatch Loss= 0.0306, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:04:16,918 Iter 1083, Minibatch Loss= 0.0251, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 16:04:18,176 Iter 1084, Minibatch Loss= 0.0280, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:04:19,496 Iter 1085, Minibatch Loss= 0.0136, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:04:20,780 Iter 1086, Minibatch Loss= 0.0163, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 16:04:22,051 Iter 1087, Minibatch Loss= 0.0202, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 16:04:23,334 Iter 1088, Minibatch Loss= 0.0276, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:04:24,634 Iter 1089, Minibatch Loss= 0.0310, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 16:04:25,943 Iter 1090, Minibatch Loss= 0.0093, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 16:04:27,227 Iter 1091, Minibatch Loss= 0.0309, Training Accuracy= 0.9558, Minibatch error= 4.4%\n",
      "2018-05-27 16:04:28,501 Iter 1092, Minibatch Loss= 0.0336, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 16:04:29,764 Iter 1093, Minibatch Loss= 0.0139, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:04:31,080 Iter 1094, Minibatch Loss= 0.0141, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 16:04:32,372 Iter 1095, Minibatch Loss= 0.0167, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:04:33,681 Iter 1096, Minibatch Loss= 0.0277, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 16:04:34,967 Iter 1097, Minibatch Loss= 0.0302, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 16:04:36,258 Iter 1098, Minibatch Loss= 0.0275, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 16:04:37,551 Iter 1099, Minibatch Loss= 0.0280, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 16:04:37,552 Epoch 54, Average loss: 0.0237, learning rate: 0.0010\n",
      "2018-05-27 16:04:47,115 Iter 1100, Minibatch Loss= 0.0218, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 16:04:49,726 Iter 1100, Minibatch Loss= 0.0243, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:04:51,009 Iter 1101, Minibatch Loss= 0.0189, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 16:04:52,281 Iter 1102, Minibatch Loss= 0.0256, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:04:53,594 Iter 1103, Minibatch Loss= 0.0166, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:04:54,858 Iter 1104, Minibatch Loss= 0.0297, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 16:04:56,154 Iter 1105, Minibatch Loss= 0.0228, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:04:57,430 Iter 1106, Minibatch Loss= 0.0297, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 16:04:58,722 Iter 1107, Minibatch Loss= 0.0174, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:05:00,009 Iter 1108, Minibatch Loss= 0.0176, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:05:01,304 Iter 1109, Minibatch Loss= 0.0198, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:05:02,600 Iter 1110, Minibatch Loss= 0.0295, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 16:05:03,880 Iter 1111, Minibatch Loss= 0.0208, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 16:05:05,172 Iter 1112, Minibatch Loss= 0.0157, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:05:06,438 Iter 1113, Minibatch Loss= 0.0126, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 16:05:07,685 Iter 1114, Minibatch Loss= 0.0149, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:05:08,977 Iter 1115, Minibatch Loss= 0.0176, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:05:10,259 Iter 1116, Minibatch Loss= 0.0288, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:05:11,527 Iter 1117, Minibatch Loss= 0.0304, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:05:12,799 Iter 1118, Minibatch Loss= 0.0173, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:05:14,109 Iter 1119, Minibatch Loss= 0.0105, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 16:05:14,110 Epoch 55, Average loss: 0.0213, learning rate: 0.0010\n",
      "2018-05-27 16:05:23,678 Iter 1120, Minibatch Loss= 0.0226, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 16:05:26,241 Iter 1120, Minibatch Loss= 0.0103, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:05:27,571 Iter 1121, Minibatch Loss= 0.0216, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:05:28,857 Iter 1122, Minibatch Loss= 0.0275, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:05:30,155 Iter 1123, Minibatch Loss= 0.0296, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 16:05:31,484 Iter 1124, Minibatch Loss= 0.0098, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:05:32,772 Iter 1125, Minibatch Loss= 0.0296, Training Accuracy= 0.9512, Minibatch error= 4.9%\n",
      "2018-05-27 16:05:34,073 Iter 1126, Minibatch Loss= 0.0110, Training Accuracy= 0.9886, Minibatch error= 1.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:05:35,355 Iter 1127, Minibatch Loss= 0.0222, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:05:36,627 Iter 1128, Minibatch Loss= 0.0086, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 16:05:37,917 Iter 1129, Minibatch Loss= 0.0071, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 16:05:39,213 Iter 1130, Minibatch Loss= 0.0282, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 16:05:40,515 Iter 1131, Minibatch Loss= 0.0287, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:05:41,815 Iter 1132, Minibatch Loss= 0.0275, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 16:05:43,096 Iter 1133, Minibatch Loss= 0.0260, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:05:44,401 Iter 1134, Minibatch Loss= 0.0144, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:05:45,679 Iter 1135, Minibatch Loss= 0.0283, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 16:05:46,968 Iter 1136, Minibatch Loss= 0.0160, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 16:05:48,258 Iter 1137, Minibatch Loss= 0.0291, Training Accuracy= 0.9521, Minibatch error= 4.8%\n",
      "2018-05-27 16:05:49,553 Iter 1138, Minibatch Loss= 0.0281, Training Accuracy= 0.9487, Minibatch error= 5.1%\n",
      "2018-05-27 16:05:50,844 Iter 1139, Minibatch Loss= 0.0067, Training Accuracy= 0.9965, Minibatch error= 0.4%\n",
      "2018-05-27 16:05:50,845 Epoch 56, Average loss: 0.0210, learning rate: 0.0010\n",
      "2018-05-27 16:06:00,402 Iter 1140, Minibatch Loss= 0.0220, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 16:06:02,876 Iter 1140, Minibatch Loss= 0.0121, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:06:04,158 Iter 1141, Minibatch Loss= 0.0278, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:06:05,453 Iter 1142, Minibatch Loss= 0.0269, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:06:06,768 Iter 1143, Minibatch Loss= 0.0096, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 16:06:08,043 Iter 1144, Minibatch Loss= 0.0199, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:06:09,342 Iter 1145, Minibatch Loss= 0.0246, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:06:10,648 Iter 1146, Minibatch Loss= 0.0172, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:06:11,942 Iter 1147, Minibatch Loss= 0.0281, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:06:13,243 Iter 1148, Minibatch Loss= 0.0141, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 16:06:14,565 Iter 1149, Minibatch Loss= 0.0113, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 16:06:15,869 Iter 1150, Minibatch Loss= 0.0129, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 16:06:17,181 Iter 1151, Minibatch Loss= 0.0195, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 16:06:18,485 Iter 1152, Minibatch Loss= 0.0251, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:06:19,810 Iter 1153, Minibatch Loss= 0.0265, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:06:21,102 Iter 1154, Minibatch Loss= 0.0305, Training Accuracy= 0.9550, Minibatch error= 4.5%\n",
      "2018-05-27 16:06:22,408 Iter 1155, Minibatch Loss= 0.0210, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:06:23,704 Iter 1156, Minibatch Loss= 0.0202, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 16:06:24,976 Iter 1157, Minibatch Loss= 0.0221, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 16:06:26,270 Iter 1158, Minibatch Loss= 0.0153, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:06:27,570 Iter 1159, Minibatch Loss= 0.0254, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:06:27,571 Epoch 57, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 16:06:37,132 Iter 1160, Minibatch Loss= 0.0232, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:06:39,733 Iter 1160, Minibatch Loss= 0.0173, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 16:06:41,053 Iter 1161, Minibatch Loss= 0.0317, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 16:06:42,336 Iter 1162, Minibatch Loss= 0.0200, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:06:43,654 Iter 1163, Minibatch Loss= 0.0125, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:06:44,979 Iter 1164, Minibatch Loss= 0.0130, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 16:06:46,297 Iter 1165, Minibatch Loss= 0.0275, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 16:06:47,620 Iter 1166, Minibatch Loss= 0.0087, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 16:06:48,963 Iter 1167, Minibatch Loss= 0.0189, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 16:06:50,258 Iter 1168, Minibatch Loss= 0.0289, Training Accuracy= 0.9549, Minibatch error= 4.5%\n",
      "2018-05-27 16:06:51,533 Iter 1169, Minibatch Loss= 0.0272, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 16:06:52,832 Iter 1170, Minibatch Loss= 0.0309, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:06:54,110 Iter 1171, Minibatch Loss= 0.0170, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 16:06:55,433 Iter 1172, Minibatch Loss= 0.0158, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:06:56,707 Iter 1173, Minibatch Loss= 0.0113, Training Accuracy= 0.9925, Minibatch error= 0.8%\n",
      "2018-05-27 16:06:58,049 Iter 1174, Minibatch Loss= 0.0136, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 16:06:59,341 Iter 1175, Minibatch Loss= 0.0226, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 16:07:00,631 Iter 1176, Minibatch Loss= 0.0337, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:07:01,911 Iter 1177, Minibatch Loss= 0.0284, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:07:03,208 Iter 1178, Minibatch Loss= 0.0202, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 16:07:04,501 Iter 1179, Minibatch Loss= 0.0149, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 16:07:04,502 Epoch 58, Average loss: 0.0211, learning rate: 0.0010\n",
      "2018-05-27 16:07:14,028 Iter 1180, Minibatch Loss= 0.0220, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:07:16,600 Iter 1180, Minibatch Loss= 0.0300, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:07:17,882 Iter 1181, Minibatch Loss= 0.0232, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 16:07:19,196 Iter 1182, Minibatch Loss= 0.0274, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:07:20,486 Iter 1183, Minibatch Loss= 0.0071, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 16:07:21,796 Iter 1184, Minibatch Loss= 0.0204, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 16:07:23,082 Iter 1185, Minibatch Loss= 0.0217, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:07:24,373 Iter 1186, Minibatch Loss= 0.0307, Training Accuracy= 0.9552, Minibatch error= 4.5%\n",
      "2018-05-27 16:07:25,685 Iter 1187, Minibatch Loss= 0.0312, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 16:07:27,044 Iter 1188, Minibatch Loss= 0.0154, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:07:28,341 Iter 1189, Minibatch Loss= 0.0108, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 16:07:29,671 Iter 1190, Minibatch Loss= 0.0278, Training Accuracy= 0.9558, Minibatch error= 4.4%\n",
      "2018-05-27 16:07:30,966 Iter 1191, Minibatch Loss= 0.0307, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:07:32,265 Iter 1192, Minibatch Loss= 0.0150, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 16:07:33,628 Iter 1193, Minibatch Loss= 0.0132, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 16:07:34,908 Iter 1194, Minibatch Loss= 0.0138, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 16:07:36,219 Iter 1195, Minibatch Loss= 0.0301, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 16:07:37,517 Iter 1196, Minibatch Loss= 0.0181, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 16:07:38,839 Iter 1197, Minibatch Loss= 0.0080, Training Accuracy= 0.9937, Minibatch error= 0.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:07:40,130 Iter 1198, Minibatch Loss= 0.0121, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:07:41,455 Iter 1199, Minibatch Loss= 0.0274, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 16:07:41,456 Epoch 59, Average loss: 0.0214, learning rate: 0.0010\n",
      "2018-05-27 16:07:50,966 Iter 1200, Minibatch Loss= 0.0219, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:07:53,592 Iter 1200, Minibatch Loss= 0.0339, Training Accuracy= 0.9498, Minibatch error= 5.0%\n",
      "2018-05-27 16:07:54,891 Iter 1201, Minibatch Loss= 0.0283, Training Accuracy= 0.9545, Minibatch error= 4.6%\n",
      "2018-05-27 16:07:56,191 Iter 1202, Minibatch Loss= 0.0160, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:07:57,514 Iter 1203, Minibatch Loss= 0.0254, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:07:58,803 Iter 1204, Minibatch Loss= 0.0076, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 16:08:00,131 Iter 1205, Minibatch Loss= 0.0127, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:08:01,454 Iter 1206, Minibatch Loss= 0.0083, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 16:08:02,745 Iter 1207, Minibatch Loss= 0.0304, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 16:08:04,018 Iter 1208, Minibatch Loss= 0.0247, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:08:05,321 Iter 1209, Minibatch Loss= 0.0133, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:08:06,629 Iter 1210, Minibatch Loss= 0.0106, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 16:08:07,947 Iter 1211, Minibatch Loss= 0.0209, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:08:09,237 Iter 1212, Minibatch Loss= 0.0212, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 16:08:10,520 Iter 1213, Minibatch Loss= 0.0157, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:08:11,836 Iter 1214, Minibatch Loss= 0.0292, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:08:13,128 Iter 1215, Minibatch Loss= 0.0136, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 16:08:14,438 Iter 1216, Minibatch Loss= 0.0183, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:08:15,747 Iter 1217, Minibatch Loss= 0.0289, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:08:17,093 Iter 1218, Minibatch Loss= 0.0124, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 16:08:18,409 Iter 1219, Minibatch Loss= 0.0269, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 16:08:18,410 Epoch 60, Average loss: 0.0204, learning rate: 0.0010\n",
      "2018-05-27 16:08:28,051 Iter 1220, Minibatch Loss= 0.0217, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:08:30,678 Iter 1220, Minibatch Loss= 0.0154, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:08:31,984 Iter 1221, Minibatch Loss= 0.0316, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:08:33,283 Iter 1222, Minibatch Loss= 0.0298, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:08:34,599 Iter 1223, Minibatch Loss= 0.0245, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:08:35,896 Iter 1224, Minibatch Loss= 0.0191, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 16:08:37,222 Iter 1225, Minibatch Loss= 0.0283, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 16:08:38,529 Iter 1226, Minibatch Loss= 0.0289, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:08:39,822 Iter 1227, Minibatch Loss= 0.0122, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:08:41,119 Iter 1228, Minibatch Loss= 0.0280, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:08:42,449 Iter 1229, Minibatch Loss= 0.0380, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 16:08:43,744 Iter 1230, Minibatch Loss= 0.0304, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:08:45,049 Iter 1231, Minibatch Loss= 0.0280, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:08:46,366 Iter 1232, Minibatch Loss= 0.0168, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 16:08:47,723 Iter 1233, Minibatch Loss= 0.0164, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:08:49,044 Iter 1234, Minibatch Loss= 0.0152, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:08:50,352 Iter 1235, Minibatch Loss= 0.0269, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:08:51,654 Iter 1236, Minibatch Loss= 0.0269, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:08:52,940 Iter 1237, Minibatch Loss= 0.0294, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:08:54,246 Iter 1238, Minibatch Loss= 0.0230, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 16:08:55,587 Iter 1239, Minibatch Loss= 0.0262, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:08:55,588 Epoch 61, Average loss: 0.0256, learning rate: 0.0010\n",
      "2018-05-27 16:09:05,121 Iter 1240, Minibatch Loss= 0.0214, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:09:07,808 Iter 1240, Minibatch Loss= 0.0125, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 16:09:09,129 Iter 1241, Minibatch Loss= 0.0162, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:09:10,500 Iter 1242, Minibatch Loss= 0.0176, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 16:09:11,842 Iter 1243, Minibatch Loss= 0.0173, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 16:09:13,156 Iter 1244, Minibatch Loss= 0.0327, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:09:14,512 Iter 1245, Minibatch Loss= 0.0216, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 16:09:15,813 Iter 1246, Minibatch Loss= 0.0084, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 16:09:17,135 Iter 1247, Minibatch Loss= 0.0264, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 16:09:18,479 Iter 1248, Minibatch Loss= 0.0305, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:09:19,804 Iter 1249, Minibatch Loss= 0.0142, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:09:21,162 Iter 1250, Minibatch Loss= 0.0232, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 16:09:22,469 Iter 1251, Minibatch Loss= 0.0125, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 16:09:23,782 Iter 1252, Minibatch Loss= 0.0122, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 16:09:25,115 Iter 1253, Minibatch Loss= 0.0156, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 16:09:26,415 Iter 1254, Minibatch Loss= 0.0195, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 16:09:27,727 Iter 1255, Minibatch Loss= 0.0199, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 16:09:29,009 Iter 1256, Minibatch Loss= 0.0208, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 16:09:30,320 Iter 1257, Minibatch Loss= 0.0252, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:09:31,634 Iter 1258, Minibatch Loss= 0.0251, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 16:09:32,950 Iter 1259, Minibatch Loss= 0.0240, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 16:09:32,951 Epoch 62, Average loss: 0.0202, learning rate: 0.0010\n",
      "2018-05-27 16:09:42,529 Iter 1260, Minibatch Loss= 0.0215, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:09:45,191 Iter 1260, Minibatch Loss= 0.0255, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:09:46,510 Iter 1261, Minibatch Loss= 0.0308, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 16:09:47,872 Iter 1262, Minibatch Loss= 0.0140, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:09:49,182 Iter 1263, Minibatch Loss= 0.0153, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 16:09:50,509 Iter 1264, Minibatch Loss= 0.0202, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:09:51,818 Iter 1265, Minibatch Loss= 0.0308, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 16:09:53,151 Iter 1266, Minibatch Loss= 0.0087, Training Accuracy= 0.9940, Minibatch error= 0.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:09:54,453 Iter 1267, Minibatch Loss= 0.0064, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 16:09:55,756 Iter 1268, Minibatch Loss= 0.0234, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:09:57,077 Iter 1269, Minibatch Loss= 0.0153, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 16:09:58,410 Iter 1270, Minibatch Loss= 0.0263, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 16:09:59,709 Iter 1271, Minibatch Loss= 0.0260, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 16:10:01,056 Iter 1272, Minibatch Loss= 0.0112, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:10:02,367 Iter 1273, Minibatch Loss= 0.0167, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 16:10:03,679 Iter 1274, Minibatch Loss= 0.0310, Training Accuracy= 0.9532, Minibatch error= 4.7%\n",
      "2018-05-27 16:10:05,011 Iter 1275, Minibatch Loss= 0.0148, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:10:06,331 Iter 1276, Minibatch Loss= 0.0168, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:10:07,678 Iter 1277, Minibatch Loss= 0.0200, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 16:10:09,005 Iter 1278, Minibatch Loss= 0.0285, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:10:10,328 Iter 1279, Minibatch Loss= 0.0136, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 16:10:10,329 Epoch 63, Average loss: 0.0203, learning rate: 0.0010\n",
      "2018-05-27 16:10:19,884 Iter 1280, Minibatch Loss= 0.0229, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 16:10:22,658 Iter 1280, Minibatch Loss= 0.0133, Training Accuracy= 0.9895, Minibatch error= 1.1%\n",
      "2018-05-27 16:10:23,979 Iter 1281, Minibatch Loss= 0.0272, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:10:25,274 Iter 1282, Minibatch Loss= 0.0294, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 16:10:26,620 Iter 1283, Minibatch Loss= 0.0185, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 16:10:27,948 Iter 1284, Minibatch Loss= 0.0282, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:10:29,243 Iter 1285, Minibatch Loss= 0.0261, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 16:10:30,568 Iter 1286, Minibatch Loss= 0.0266, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:10:31,908 Iter 1287, Minibatch Loss= 0.0183, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 16:10:33,227 Iter 1288, Minibatch Loss= 0.0278, Training Accuracy= 0.9497, Minibatch error= 5.0%\n",
      "2018-05-27 16:10:34,570 Iter 1289, Minibatch Loss= 0.0149, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 16:10:35,870 Iter 1290, Minibatch Loss= 0.0169, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:10:37,248 Iter 1291, Minibatch Loss= 0.0271, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:10:38,569 Iter 1292, Minibatch Loss= 0.0184, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 16:10:39,932 Iter 1293, Minibatch Loss= 0.0160, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 16:10:41,281 Iter 1294, Minibatch Loss= 0.0299, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:10:42,594 Iter 1295, Minibatch Loss= 0.0242, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:10:43,930 Iter 1296, Minibatch Loss= 0.0297, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:10:45,253 Iter 1297, Minibatch Loss= 0.0247, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:10:46,582 Iter 1298, Minibatch Loss= 0.0264, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:10:47,893 Iter 1299, Minibatch Loss= 0.0286, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:10:47,894 Epoch 64, Average loss: 0.0243, learning rate: 0.0010\n",
      "2018-05-27 16:10:57,419 Iter 1300, Minibatch Loss= 0.0215, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 16:11:00,142 Iter 1300, Minibatch Loss= 0.0146, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:11:01,460 Iter 1301, Minibatch Loss= 0.0252, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 16:11:02,782 Iter 1302, Minibatch Loss= 0.0166, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:11:04,159 Iter 1303, Minibatch Loss= 0.0275, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:11:05,491 Iter 1304, Minibatch Loss= 0.0241, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 16:11:06,807 Iter 1305, Minibatch Loss= 0.0132, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:11:08,126 Iter 1306, Minibatch Loss= 0.0262, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:11:09,458 Iter 1307, Minibatch Loss= 0.0233, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:11:10,784 Iter 1308, Minibatch Loss= 0.0282, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 16:11:12,122 Iter 1309, Minibatch Loss= 0.0247, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:11:13,474 Iter 1310, Minibatch Loss= 0.0153, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:11:14,782 Iter 1311, Minibatch Loss= 0.0397, Training Accuracy= 0.9387, Minibatch error= 6.1%\n",
      "2018-05-27 16:11:16,088 Iter 1312, Minibatch Loss= 0.0140, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 16:11:17,397 Iter 1313, Minibatch Loss= 0.0290, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:11:18,707 Iter 1314, Minibatch Loss= 0.0269, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:11:20,022 Iter 1315, Minibatch Loss= 0.0261, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:11:21,317 Iter 1316, Minibatch Loss= 0.0105, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 16:11:22,619 Iter 1317, Minibatch Loss= 0.0162, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:11:23,953 Iter 1318, Minibatch Loss= 0.0284, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 16:11:25,287 Iter 1319, Minibatch Loss= 0.0086, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:11:25,288 Epoch 65, Average loss: 0.0223, learning rate: 0.0010\n",
      "2018-05-27 16:11:34,834 Iter 1320, Minibatch Loss= 0.0209, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:11:37,570 Iter 1320, Minibatch Loss= 0.0216, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:11:38,900 Iter 1321, Minibatch Loss= 0.0300, Training Accuracy= 0.9552, Minibatch error= 4.5%\n",
      "2018-05-27 16:11:40,251 Iter 1322, Minibatch Loss= 0.0188, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 16:11:41,572 Iter 1323, Minibatch Loss= 0.0145, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:11:42,901 Iter 1324, Minibatch Loss= 0.0193, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:11:44,265 Iter 1325, Minibatch Loss= 0.0087, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 16:11:45,578 Iter 1326, Minibatch Loss= 0.0313, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:11:46,919 Iter 1327, Minibatch Loss= 0.0123, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 16:11:48,265 Iter 1328, Minibatch Loss= 0.0085, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 16:11:49,600 Iter 1329, Minibatch Loss= 0.0238, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 16:11:50,919 Iter 1330, Minibatch Loss= 0.0163, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:11:52,264 Iter 1331, Minibatch Loss= 0.0273, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 16:11:53,593 Iter 1332, Minibatch Loss= 0.0090, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 16:11:54,886 Iter 1333, Minibatch Loss= 0.0307, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:11:56,202 Iter 1334, Minibatch Loss= 0.0290, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:11:57,522 Iter 1335, Minibatch Loss= 0.0222, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 16:11:58,812 Iter 1336, Minibatch Loss= 0.0315, Training Accuracy= 0.9441, Minibatch error= 5.6%\n",
      "2018-05-27 16:12:00,138 Iter 1337, Minibatch Loss= 0.0082, Training Accuracy= 0.9911, Minibatch error= 0.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:12:01,495 Iter 1338, Minibatch Loss= 0.0114, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:12:02,804 Iter 1339, Minibatch Loss= 0.0231, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:12:02,805 Epoch 66, Average loss: 0.0206, learning rate: 0.0010\n",
      "2018-05-27 16:12:12,267 Iter 1340, Minibatch Loss= 0.0214, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:12:14,988 Iter 1340, Minibatch Loss= 0.0172, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 16:12:16,322 Iter 1341, Minibatch Loss= 0.0069, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 16:12:17,721 Iter 1342, Minibatch Loss= 0.0202, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 16:12:19,058 Iter 1343, Minibatch Loss= 0.0144, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:12:20,357 Iter 1344, Minibatch Loss= 0.0181, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 16:12:21,669 Iter 1345, Minibatch Loss= 0.0312, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 16:12:22,991 Iter 1346, Minibatch Loss= 0.0238, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 16:12:24,320 Iter 1347, Minibatch Loss= 0.0308, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 16:12:25,655 Iter 1348, Minibatch Loss= 0.0148, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:12:26,999 Iter 1349, Minibatch Loss= 0.0290, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:12:28,323 Iter 1350, Minibatch Loss= 0.0293, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 16:12:29,661 Iter 1351, Minibatch Loss= 0.0166, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 16:12:30,965 Iter 1352, Minibatch Loss= 0.0272, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:12:32,290 Iter 1353, Minibatch Loss= 0.0271, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 16:12:33,600 Iter 1354, Minibatch Loss= 0.0303, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 16:12:34,935 Iter 1355, Minibatch Loss= 0.0288, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:12:36,254 Iter 1356, Minibatch Loss= 0.0211, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:12:37,590 Iter 1357, Minibatch Loss= 0.0127, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 16:12:38,934 Iter 1358, Minibatch Loss= 0.0132, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 16:12:40,253 Iter 1359, Minibatch Loss= 0.0268, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:12:40,254 Epoch 67, Average loss: 0.0222, learning rate: 0.0010\n",
      "2018-05-27 16:12:49,790 Iter 1360, Minibatch Loss= 0.0210, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:12:52,594 Iter 1360, Minibatch Loss= 0.0204, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 16:12:53,929 Iter 1361, Minibatch Loss= 0.0109, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 16:12:55,312 Iter 1362, Minibatch Loss= 0.0263, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 16:12:56,617 Iter 1363, Minibatch Loss= 0.0237, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 16:12:57,951 Iter 1364, Minibatch Loss= 0.0138, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:12:59,278 Iter 1365, Minibatch Loss= 0.0096, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 16:13:00,637 Iter 1366, Minibatch Loss= 0.0142, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 16:13:01,958 Iter 1367, Minibatch Loss= 0.0246, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:13:03,287 Iter 1368, Minibatch Loss= 0.0119, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:13:04,606 Iter 1369, Minibatch Loss= 0.0117, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 16:13:05,970 Iter 1370, Minibatch Loss= 0.0279, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 16:13:07,303 Iter 1371, Minibatch Loss= 0.0247, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:13:08,626 Iter 1372, Minibatch Loss= 0.0121, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:13:09,955 Iter 1373, Minibatch Loss= 0.0061, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 16:13:11,293 Iter 1374, Minibatch Loss= 0.0200, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 16:13:12,685 Iter 1375, Minibatch Loss= 0.0236, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 16:13:14,135 Iter 1376, Minibatch Loss= 0.0166, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:13:15,537 Iter 1377, Minibatch Loss= 0.0283, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:13:16,872 Iter 1378, Minibatch Loss= 0.0268, Training Accuracy= 0.9565, Minibatch error= 4.3%\n",
      "2018-05-27 16:13:18,206 Iter 1379, Minibatch Loss= 0.0166, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 16:13:18,207 Epoch 68, Average loss: 0.0190, learning rate: 0.0010\n",
      "2018-05-27 16:13:27,730 Iter 1380, Minibatch Loss= 0.0209, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:13:30,541 Iter 1380, Minibatch Loss= 0.0306, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:13:31,872 Iter 1381, Minibatch Loss= 0.0238, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 16:13:33,189 Iter 1382, Minibatch Loss= 0.0290, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:13:34,550 Iter 1383, Minibatch Loss= 0.0057, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 16:13:35,934 Iter 1384, Minibatch Loss= 0.0273, Training Accuracy= 0.9509, Minibatch error= 4.9%\n",
      "2018-05-27 16:13:37,297 Iter 1385, Minibatch Loss= 0.0154, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 16:13:38,650 Iter 1386, Minibatch Loss= 0.0280, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:13:40,002 Iter 1387, Minibatch Loss= 0.0290, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:13:41,346 Iter 1388, Minibatch Loss= 0.0319, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 16:13:42,748 Iter 1389, Minibatch Loss= 0.0277, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:13:44,077 Iter 1390, Minibatch Loss= 0.0197, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 16:13:45,450 Iter 1391, Minibatch Loss= 0.0234, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:13:46,811 Iter 1392, Minibatch Loss= 0.0177, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:13:48,164 Iter 1393, Minibatch Loss= 0.0321, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:13:49,520 Iter 1394, Minibatch Loss= 0.0311, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:13:50,895 Iter 1395, Minibatch Loss= 0.0265, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:13:52,288 Iter 1396, Minibatch Loss= 0.0173, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:13:53,652 Iter 1397, Minibatch Loss= 0.0256, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:13:55,003 Iter 1398, Minibatch Loss= 0.0217, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 16:13:56,387 Iter 1399, Minibatch Loss= 0.0249, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:13:56,389 Epoch 69, Average loss: 0.0251, learning rate: 0.0010\n",
      "2018-05-27 16:14:05,986 Iter 1400, Minibatch Loss= 0.0206, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 16:14:08,827 Iter 1400, Minibatch Loss= 0.0253, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:14:10,197 Iter 1401, Minibatch Loss= 0.0246, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:14:11,591 Iter 1402, Minibatch Loss= 0.0053, Training Accuracy= 0.9979, Minibatch error= 0.2%\n",
      "2018-05-27 16:14:12,982 Iter 1403, Minibatch Loss= 0.0170, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:14:14,347 Iter 1404, Minibatch Loss= 0.0285, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 16:14:15,737 Iter 1405, Minibatch Loss= 0.0098, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 16:14:17,108 Iter 1406, Minibatch Loss= 0.0152, Training Accuracy= 0.9815, Minibatch error= 1.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:14:18,471 Iter 1407, Minibatch Loss= 0.0084, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 16:14:19,835 Iter 1408, Minibatch Loss= 0.0162, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:14:21,210 Iter 1409, Minibatch Loss= 0.0228, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:14:22,596 Iter 1410, Minibatch Loss= 0.0275, Training Accuracy= 0.9529, Minibatch error= 4.7%\n",
      "2018-05-27 16:14:23,991 Iter 1411, Minibatch Loss= 0.0101, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 16:14:25,404 Iter 1412, Minibatch Loss= 0.0107, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 16:14:26,801 Iter 1413, Minibatch Loss= 0.0289, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:14:28,195 Iter 1414, Minibatch Loss= 0.0254, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:14:29,584 Iter 1415, Minibatch Loss= 0.0152, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:14:30,963 Iter 1416, Minibatch Loss= 0.0141, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 16:14:32,344 Iter 1417, Minibatch Loss= 0.0281, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:14:33,776 Iter 1418, Minibatch Loss= 0.0172, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:14:35,177 Iter 1419, Minibatch Loss= 0.0143, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 16:14:35,178 Epoch 70, Average loss: 0.0186, learning rate: 0.0010\n",
      "2018-05-27 16:14:44,759 Iter 1420, Minibatch Loss= 0.0204, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:14:47,762 Iter 1420, Minibatch Loss= 0.0051, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 16:14:49,139 Iter 1421, Minibatch Loss= 0.0262, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:14:50,541 Iter 1422, Minibatch Loss= 0.0255, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:14:51,946 Iter 1423, Minibatch Loss= 0.0130, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:14:53,348 Iter 1424, Minibatch Loss= 0.0163, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:14:54,771 Iter 1425, Minibatch Loss= 0.0263, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:14:56,150 Iter 1426, Minibatch Loss= 0.0183, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 16:14:57,510 Iter 1427, Minibatch Loss= 0.0161, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 16:14:58,856 Iter 1428, Minibatch Loss= 0.0261, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:15:00,208 Iter 1429, Minibatch Loss= 0.0097, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 16:15:01,559 Iter 1430, Minibatch Loss= 0.0069, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 16:15:02,902 Iter 1431, Minibatch Loss= 0.0140, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:15:04,263 Iter 1432, Minibatch Loss= 0.0174, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:15:05,606 Iter 1433, Minibatch Loss= 0.0117, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 16:15:06,954 Iter 1434, Minibatch Loss= 0.0260, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:15:08,339 Iter 1435, Minibatch Loss= 0.0306, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 16:15:09,698 Iter 1436, Minibatch Loss= 0.0248, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 16:15:11,099 Iter 1437, Minibatch Loss= 0.0254, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 16:15:12,455 Iter 1438, Minibatch Loss= 0.0161, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:15:13,809 Iter 1439, Minibatch Loss= 0.0086, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 16:15:13,810 Epoch 71, Average loss: 0.0188, learning rate: 0.0010\n",
      "2018-05-27 16:15:23,388 Iter 1440, Minibatch Loss= 0.0206, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:15:26,281 Iter 1440, Minibatch Loss= 0.0272, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:15:27,660 Iter 1441, Minibatch Loss= 0.0230, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 16:15:29,040 Iter 1442, Minibatch Loss= 0.0139, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:15:30,389 Iter 1443, Minibatch Loss= 0.0149, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 16:15:31,738 Iter 1444, Minibatch Loss= 0.0280, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:15:33,116 Iter 1445, Minibatch Loss= 0.0062, Training Accuracy= 0.9965, Minibatch error= 0.4%\n",
      "2018-05-27 16:15:34,465 Iter 1446, Minibatch Loss= 0.0287, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:15:35,843 Iter 1447, Minibatch Loss= 0.0115, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 16:15:37,211 Iter 1448, Minibatch Loss= 0.0214, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 16:15:38,607 Iter 1449, Minibatch Loss= 0.0152, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 16:15:39,987 Iter 1450, Minibatch Loss= 0.0289, Training Accuracy= 0.9513, Minibatch error= 4.9%\n",
      "2018-05-27 16:15:41,385 Iter 1451, Minibatch Loss= 0.0152, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:15:42,750 Iter 1452, Minibatch Loss= 0.0091, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 16:15:44,119 Iter 1453, Minibatch Loss= 0.0322, Training Accuracy= 0.9472, Minibatch error= 5.3%\n",
      "2018-05-27 16:15:45,507 Iter 1454, Minibatch Loss= 0.0146, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 16:15:46,860 Iter 1455, Minibatch Loss= 0.0286, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:15:48,237 Iter 1456, Minibatch Loss= 0.0283, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 16:15:49,611 Iter 1457, Minibatch Loss= 0.0159, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 16:15:50,987 Iter 1458, Minibatch Loss= 0.0157, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 16:15:52,350 Iter 1459, Minibatch Loss= 0.0278, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:15:52,351 Epoch 72, Average loss: 0.0209, learning rate: 0.0010\n",
      "2018-05-27 16:16:01,896 Iter 1460, Minibatch Loss= 0.0209, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 16:16:04,770 Iter 1460, Minibatch Loss= 0.0264, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:16:06,150 Iter 1461, Minibatch Loss= 0.0243, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:16:07,541 Iter 1462, Minibatch Loss= 0.0126, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 16:16:08,905 Iter 1463, Minibatch Loss= 0.0142, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 16:16:10,230 Iter 1464, Minibatch Loss= 0.0246, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:16:11,639 Iter 1465, Minibatch Loss= 0.0199, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 16:16:12,972 Iter 1466, Minibatch Loss= 0.0304, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:16:14,356 Iter 1467, Minibatch Loss= 0.0129, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:16:15,713 Iter 1468, Minibatch Loss= 0.0258, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:16:17,095 Iter 1469, Minibatch Loss= 0.0063, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 16:16:18,468 Iter 1470, Minibatch Loss= 0.0086, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 16:16:19,834 Iter 1471, Minibatch Loss= 0.0096, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 16:16:21,204 Iter 1472, Minibatch Loss= 0.0301, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 16:16:22,547 Iter 1473, Minibatch Loss= 0.0306, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:16:23,901 Iter 1474, Minibatch Loss= 0.0228, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 16:16:25,269 Iter 1475, Minibatch Loss= 0.0101, Training Accuracy= 0.9921, Minibatch error= 0.8%\n",
      "2018-05-27 16:16:26,700 Iter 1476, Minibatch Loss= 0.0124, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:16:28,055 Iter 1477, Minibatch Loss= 0.0265, Training Accuracy= 0.9623, Minibatch error= 3.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:16:29,476 Iter 1478, Minibatch Loss= 0.0133, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:16:30,879 Iter 1479, Minibatch Loss= 0.0113, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 16:16:30,880 Epoch 73, Average loss: 0.0191, learning rate: 0.0010\n",
      "2018-05-27 16:16:40,503 Iter 1480, Minibatch Loss= 0.0204, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 16:16:43,600 Iter 1480, Minibatch Loss= 0.0292, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 16:16:44,959 Iter 1481, Minibatch Loss= 0.0331, Training Accuracy= 0.9449, Minibatch error= 5.5%\n",
      "2018-05-27 16:16:46,314 Iter 1482, Minibatch Loss= 0.0051, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 16:16:47,662 Iter 1483, Minibatch Loss= 0.0267, Training Accuracy= 0.9587, Minibatch error= 4.1%\n",
      "2018-05-27 16:16:49,027 Iter 1484, Minibatch Loss= 0.0257, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 16:16:50,375 Iter 1485, Minibatch Loss= 0.0216, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:16:51,747 Iter 1486, Minibatch Loss= 0.0250, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 16:16:53,106 Iter 1487, Minibatch Loss= 0.0251, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:16:54,471 Iter 1488, Minibatch Loss= 0.0192, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 16:16:55,826 Iter 1489, Minibatch Loss= 0.0252, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:16:57,181 Iter 1490, Minibatch Loss= 0.0215, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 16:16:58,533 Iter 1491, Minibatch Loss= 0.0269, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 16:16:59,925 Iter 1492, Minibatch Loss= 0.0249, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 16:17:01,285 Iter 1493, Minibatch Loss= 0.0266, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 16:17:02,726 Iter 1494, Minibatch Loss= 0.0125, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:17:04,114 Iter 1495, Minibatch Loss= 0.0294, Training Accuracy= 0.9531, Minibatch error= 4.7%\n",
      "2018-05-27 16:17:05,483 Iter 1496, Minibatch Loss= 0.0285, Training Accuracy= 0.9509, Minibatch error= 4.9%\n",
      "2018-05-27 16:17:06,872 Iter 1497, Minibatch Loss= 0.0277, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 16:17:08,228 Iter 1498, Minibatch Loss= 0.0266, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:17:09,622 Iter 1499, Minibatch Loss= 0.0125, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:17:09,623 Epoch 74, Average loss: 0.0244, learning rate: 0.0010\n",
      "2018-05-27 16:17:19,193 Iter 1500, Minibatch Loss= 0.0204, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 16:17:22,160 Iter 1500, Minibatch Loss= 0.0171, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 16:17:23,544 Iter 1501, Minibatch Loss= 0.0258, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:17:24,908 Iter 1502, Minibatch Loss= 0.0263, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 16:17:26,313 Iter 1503, Minibatch Loss= 0.0147, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 16:17:27,726 Iter 1504, Minibatch Loss= 0.0164, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:17:29,097 Iter 1505, Minibatch Loss= 0.0313, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:17:30,495 Iter 1506, Minibatch Loss= 0.0215, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:17:31,875 Iter 1507, Minibatch Loss= 0.0162, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:17:33,244 Iter 1508, Minibatch Loss= 0.0149, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 16:17:34,620 Iter 1509, Minibatch Loss= 0.0307, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:17:36,014 Iter 1510, Minibatch Loss= 0.0171, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:17:37,380 Iter 1511, Minibatch Loss= 0.0156, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:17:38,735 Iter 1512, Minibatch Loss= 0.0133, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 16:17:40,092 Iter 1513, Minibatch Loss= 0.0118, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 16:17:41,463 Iter 1514, Minibatch Loss= 0.0147, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:17:42,844 Iter 1515, Minibatch Loss= 0.0285, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:17:44,207 Iter 1516, Minibatch Loss= 0.0279, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:17:45,565 Iter 1517, Minibatch Loss= 0.0297, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 16:17:46,940 Iter 1518, Minibatch Loss= 0.0158, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:17:48,298 Iter 1519, Minibatch Loss= 0.0151, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:17:48,299 Epoch 75, Average loss: 0.0206, learning rate: 0.0010\n",
      "2018-05-27 16:17:57,852 Iter 1520, Minibatch Loss= 0.0215, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:18:00,772 Iter 1520, Minibatch Loss= 0.0243, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 16:18:02,140 Iter 1521, Minibatch Loss= 0.0263, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 16:18:03,518 Iter 1522, Minibatch Loss= 0.0239, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:18:04,900 Iter 1523, Minibatch Loss= 0.0201, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:06,272 Iter 1524, Minibatch Loss= 0.0213, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:07,652 Iter 1525, Minibatch Loss= 0.0263, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:18:09,043 Iter 1526, Minibatch Loss= 0.0189, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 16:18:10,405 Iter 1527, Minibatch Loss= 0.0230, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:18:11,789 Iter 1528, Minibatch Loss= 0.0296, Training Accuracy= 0.9518, Minibatch error= 4.8%\n",
      "2018-05-27 16:18:13,180 Iter 1529, Minibatch Loss= 0.0049, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 16:18:14,560 Iter 1530, Minibatch Loss= 0.0279, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:18:16,008 Iter 1531, Minibatch Loss= 0.0168, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:17,441 Iter 1532, Minibatch Loss= 0.0071, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 16:18:18,845 Iter 1533, Minibatch Loss= 0.0155, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 16:18:20,247 Iter 1534, Minibatch Loss= 0.0178, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:21,648 Iter 1535, Minibatch Loss= 0.0094, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:18:23,071 Iter 1536, Minibatch Loss= 0.0114, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 16:18:24,438 Iter 1537, Minibatch Loss= 0.0289, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:18:25,849 Iter 1538, Minibatch Loss= 0.0200, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 16:18:27,219 Iter 1539, Minibatch Loss= 0.0283, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 16:18:27,220 Epoch 76, Average loss: 0.0206, learning rate: 0.0010\n",
      "2018-05-27 16:18:36,781 Iter 1540, Minibatch Loss= 0.0205, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:39,767 Iter 1540, Minibatch Loss= 0.0095, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 16:18:41,178 Iter 1541, Minibatch Loss= 0.0268, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 16:18:42,565 Iter 1542, Minibatch Loss= 0.0234, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 16:18:43,954 Iter 1543, Minibatch Loss= 0.0279, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:18:45,331 Iter 1544, Minibatch Loss= 0.0449, Training Accuracy= 0.9230, Minibatch error= 7.7%\n",
      "2018-05-27 16:18:46,695 Iter 1545, Minibatch Loss= 0.0223, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:18:48,085 Iter 1546, Minibatch Loss= 0.0213, Training Accuracy= 0.9738, Minibatch error= 2.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:18:49,485 Iter 1547, Minibatch Loss= 0.0202, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 16:18:50,853 Iter 1548, Minibatch Loss= 0.0287, Training Accuracy= 0.9665, Minibatch error= 3.4%\n",
      "2018-05-27 16:18:52,222 Iter 1549, Minibatch Loss= 0.0173, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 16:18:53,627 Iter 1550, Minibatch Loss= 0.0109, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 16:18:55,012 Iter 1551, Minibatch Loss= 0.0182, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:18:56,432 Iter 1552, Minibatch Loss= 0.0198, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 16:18:57,818 Iter 1553, Minibatch Loss= 0.0074, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 16:18:59,217 Iter 1554, Minibatch Loss= 0.0142, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:19:00,584 Iter 1555, Minibatch Loss= 0.0119, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 16:19:01,956 Iter 1556, Minibatch Loss= 0.0291, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 16:19:03,357 Iter 1557, Minibatch Loss= 0.0265, Training Accuracy= 0.9505, Minibatch error= 5.0%\n",
      "2018-05-27 16:19:04,726 Iter 1558, Minibatch Loss= 0.0104, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 16:19:06,122 Iter 1559, Minibatch Loss= 0.0251, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:19:06,123 Epoch 77, Average loss: 0.0213, learning rate: 0.0010\n",
      "2018-05-27 16:19:15,707 Iter 1560, Minibatch Loss= 0.0204, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 16:19:18,707 Iter 1560, Minibatch Loss= 0.0157, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 16:19:20,079 Iter 1561, Minibatch Loss= 0.0276, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:19:21,463 Iter 1562, Minibatch Loss= 0.0256, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:19:22,835 Iter 1563, Minibatch Loss= 0.0080, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 16:19:24,222 Iter 1564, Minibatch Loss= 0.0255, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:19:25,605 Iter 1565, Minibatch Loss= 0.0162, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:19:26,988 Iter 1566, Minibatch Loss= 0.0128, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 16:19:28,396 Iter 1567, Minibatch Loss= 0.0268, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:19:29,787 Iter 1568, Minibatch Loss= 0.0298, Training Accuracy= 0.9391, Minibatch error= 6.1%\n",
      "2018-05-27 16:19:31,198 Iter 1569, Minibatch Loss= 0.0264, Training Accuracy= 0.9493, Minibatch error= 5.1%\n",
      "2018-05-27 16:19:32,606 Iter 1570, Minibatch Loss= 0.0078, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 16:19:34,023 Iter 1571, Minibatch Loss= 0.0097, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 16:19:35,403 Iter 1572, Minibatch Loss= 0.0160, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 16:19:36,815 Iter 1573, Minibatch Loss= 0.0060, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 16:19:38,222 Iter 1574, Minibatch Loss= 0.0155, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 16:19:39,655 Iter 1575, Minibatch Loss= 0.0145, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 16:19:41,052 Iter 1576, Minibatch Loss= 0.0259, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 16:19:42,446 Iter 1577, Minibatch Loss= 0.0139, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:19:43,848 Iter 1578, Minibatch Loss= 0.0267, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:19:45,223 Iter 1579, Minibatch Loss= 0.0092, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 16:19:45,224 Epoch 78, Average loss: 0.0185, learning rate: 0.0010\n",
      "2018-05-27 16:19:54,785 Iter 1580, Minibatch Loss= 0.0210, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:19:57,774 Iter 1580, Minibatch Loss= 0.0067, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 16:19:59,184 Iter 1581, Minibatch Loss= 0.0236, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 16:20:00,571 Iter 1582, Minibatch Loss= 0.0157, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 16:20:01,983 Iter 1583, Minibatch Loss= 0.0141, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 16:20:03,367 Iter 1584, Minibatch Loss= 0.0190, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:20:04,746 Iter 1585, Minibatch Loss= 0.0154, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 16:20:06,139 Iter 1586, Minibatch Loss= 0.0256, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:20:07,564 Iter 1587, Minibatch Loss= 0.0173, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:20:08,992 Iter 1588, Minibatch Loss= 0.0290, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 16:20:10,382 Iter 1589, Minibatch Loss= 0.0251, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:20:11,797 Iter 1590, Minibatch Loss= 0.0295, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 16:20:13,188 Iter 1591, Minibatch Loss= 0.0272, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:20:14,563 Iter 1592, Minibatch Loss= 0.0249, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:20:15,972 Iter 1593, Minibatch Loss= 0.0173, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 16:20:17,344 Iter 1594, Minibatch Loss= 0.0263, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:20:18,754 Iter 1595, Minibatch Loss= 0.0137, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:20:20,139 Iter 1596, Minibatch Loss= 0.0273, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:20:21,562 Iter 1597, Minibatch Loss= 0.0213, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 16:20:22,960 Iter 1598, Minibatch Loss= 0.0265, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:20:24,388 Iter 1599, Minibatch Loss= 0.0207, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 16:20:24,389 Epoch 79, Average loss: 0.0218, learning rate: 0.0010\n",
      "2018-05-27 16:20:33,964 Iter 1600, Minibatch Loss= 0.0209, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:20:37,088 Iter 1600, Minibatch Loss= 0.0065, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 16:20:38,486 Iter 1601, Minibatch Loss= 0.0266, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:20:39,881 Iter 1602, Minibatch Loss= 0.0106, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 16:20:41,265 Iter 1603, Minibatch Loss= 0.0232, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 16:20:42,671 Iter 1604, Minibatch Loss= 0.0171, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 16:20:44,106 Iter 1605, Minibatch Loss= 0.0171, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:20:45,490 Iter 1606, Minibatch Loss= 0.0246, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:20:46,869 Iter 1607, Minibatch Loss= 0.0149, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 16:20:48,244 Iter 1608, Minibatch Loss= 0.0077, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 16:20:49,678 Iter 1609, Minibatch Loss= 0.0245, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:20:51,077 Iter 1610, Minibatch Loss= 0.0055, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 16:20:52,477 Iter 1611, Minibatch Loss= 0.0115, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 16:20:53,871 Iter 1612, Minibatch Loss= 0.0296, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:20:55,279 Iter 1613, Minibatch Loss= 0.0143, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 16:20:56,695 Iter 1614, Minibatch Loss= 0.0272, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 16:20:58,092 Iter 1615, Minibatch Loss= 0.0069, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 16:20:59,475 Iter 1616, Minibatch Loss= 0.0271, Training Accuracy= 0.9513, Minibatch error= 4.9%\n",
      "2018-05-27 16:21:00,848 Iter 1617, Minibatch Loss= 0.0070, Training Accuracy= 0.9920, Minibatch error= 0.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:21:02,250 Iter 1618, Minibatch Loss= 0.0158, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:21:03,666 Iter 1619, Minibatch Loss= 0.0140, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:21:03,667 Epoch 80, Average loss: 0.0172, learning rate: 0.0010\n",
      "2018-05-27 16:21:13,212 Iter 1620, Minibatch Loss= 0.0203, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:21:16,251 Iter 1620, Minibatch Loss= 0.0243, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:21:17,688 Iter 1621, Minibatch Loss= 0.0276, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:21:19,104 Iter 1622, Minibatch Loss= 0.0166, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:21:20,509 Iter 1623, Minibatch Loss= 0.0294, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 16:21:21,913 Iter 1624, Minibatch Loss= 0.0290, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:21:23,308 Iter 1625, Minibatch Loss= 0.0243, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:21:24,738 Iter 1626, Minibatch Loss= 0.0120, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 16:21:26,148 Iter 1627, Minibatch Loss= 0.0130, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 16:21:27,573 Iter 1628, Minibatch Loss= 0.0284, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 16:21:29,016 Iter 1629, Minibatch Loss= 0.0157, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 16:21:30,473 Iter 1630, Minibatch Loss= 0.0255, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:21:31,920 Iter 1631, Minibatch Loss= 0.0160, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 16:21:33,322 Iter 1632, Minibatch Loss= 0.0266, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:21:34,768 Iter 1633, Minibatch Loss= 0.0138, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:21:36,171 Iter 1634, Minibatch Loss= 0.0150, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 16:21:37,609 Iter 1635, Minibatch Loss= 0.0300, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 16:21:39,003 Iter 1636, Minibatch Loss= 0.0241, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:21:40,380 Iter 1637, Minibatch Loss= 0.0086, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 16:21:41,777 Iter 1638, Minibatch Loss= 0.0263, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 16:21:43,162 Iter 1639, Minibatch Loss= 0.0249, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 16:21:43,163 Epoch 81, Average loss: 0.0219, learning rate: 0.0010\n",
      "2018-05-27 16:21:52,788 Iter 1640, Minibatch Loss= 0.0200, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 16:21:55,861 Iter 1640, Minibatch Loss= 0.0221, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 16:21:57,298 Iter 1641, Minibatch Loss= 0.0121, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:21:58,697 Iter 1642, Minibatch Loss= 0.0293, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 16:22:00,145 Iter 1643, Minibatch Loss= 0.0083, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:22:01,584 Iter 1644, Minibatch Loss= 0.0248, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:22:03,025 Iter 1645, Minibatch Loss= 0.0123, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 16:22:04,432 Iter 1646, Minibatch Loss= 0.0293, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 16:22:05,830 Iter 1647, Minibatch Loss= 0.0272, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 16:22:07,268 Iter 1648, Minibatch Loss= 0.0218, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 16:22:08,683 Iter 1649, Minibatch Loss= 0.0227, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:22:10,113 Iter 1650, Minibatch Loss= 0.0278, Training Accuracy= 0.9615, Minibatch error= 3.9%\n",
      "2018-05-27 16:22:11,526 Iter 1651, Minibatch Loss= 0.0156, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:22:12,940 Iter 1652, Minibatch Loss= 0.0201, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 16:22:14,376 Iter 1653, Minibatch Loss= 0.0164, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:22:15,786 Iter 1654, Minibatch Loss= 0.0252, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:22:17,223 Iter 1655, Minibatch Loss= 0.0124, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:22:18,644 Iter 1656, Minibatch Loss= 0.0138, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 16:22:20,053 Iter 1657, Minibatch Loss= 0.0084, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 16:22:21,471 Iter 1658, Minibatch Loss= 0.0060, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 16:22:22,891 Iter 1659, Minibatch Loss= 0.0291, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 16:22:22,892 Epoch 82, Average loss: 0.0198, learning rate: 0.0010\n",
      "2018-05-27 16:22:32,512 Iter 1660, Minibatch Loss= 0.0211, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:22:35,825 Iter 1660, Minibatch Loss= 0.0097, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 16:22:37,263 Iter 1661, Minibatch Loss= 0.0248, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:22:38,704 Iter 1662, Minibatch Loss= 0.0212, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:22:40,160 Iter 1663, Minibatch Loss= 0.0299, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:22:41,583 Iter 1664, Minibatch Loss= 0.0259, Training Accuracy= 0.9446, Minibatch error= 5.5%\n",
      "2018-05-27 16:22:43,039 Iter 1665, Minibatch Loss= 0.0181, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:22:44,454 Iter 1666, Minibatch Loss= 0.0309, Training Accuracy= 0.9443, Minibatch error= 5.6%\n",
      "2018-05-27 16:22:45,868 Iter 1667, Minibatch Loss= 0.0136, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 16:22:47,286 Iter 1668, Minibatch Loss= 0.0191, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 16:22:48,744 Iter 1669, Minibatch Loss= 0.0262, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 16:22:50,194 Iter 1670, Minibatch Loss= 0.0326, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:22:51,601 Iter 1671, Minibatch Loss= 0.0269, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 16:22:52,998 Iter 1672, Minibatch Loss= 0.0248, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 16:22:54,414 Iter 1673, Minibatch Loss= 0.0121, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:22:55,862 Iter 1674, Minibatch Loss= 0.0231, Training Accuracy= 0.9685, Minibatch error= 3.1%\n",
      "2018-05-27 16:22:57,271 Iter 1675, Minibatch Loss= 0.0056, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 16:22:58,734 Iter 1676, Minibatch Loss= 0.0275, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 16:23:00,165 Iter 1677, Minibatch Loss= 0.0082, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 16:23:01,593 Iter 1678, Minibatch Loss= 0.0132, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:23:03,027 Iter 1679, Minibatch Loss= 0.0137, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:23:03,028 Epoch 83, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 16:23:12,592 Iter 1680, Minibatch Loss= 0.0202, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:23:15,780 Iter 1680, Minibatch Loss= 0.0084, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:23:17,191 Iter 1681, Minibatch Loss= 0.0094, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 16:23:18,603 Iter 1682, Minibatch Loss= 0.0281, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:23:20,000 Iter 1683, Minibatch Loss= 0.0264, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:23:21,421 Iter 1684, Minibatch Loss= 0.0264, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 16:23:22,827 Iter 1685, Minibatch Loss= 0.0277, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 16:23:24,281 Iter 1686, Minibatch Loss= 0.0046, Training Accuracy= 0.9977, Minibatch error= 0.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:23:25,725 Iter 1687, Minibatch Loss= 0.0266, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 16:23:27,137 Iter 1688, Minibatch Loss= 0.0290, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:23:28,586 Iter 1689, Minibatch Loss= 0.0140, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 16:23:30,008 Iter 1690, Minibatch Loss= 0.0175, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:23:31,420 Iter 1691, Minibatch Loss= 0.0276, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:23:32,847 Iter 1692, Minibatch Loss= 0.0216, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:23:34,294 Iter 1693, Minibatch Loss= 0.0225, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:23:35,736 Iter 1694, Minibatch Loss= 0.0262, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:23:37,151 Iter 1695, Minibatch Loss= 0.0267, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 16:23:38,616 Iter 1696, Minibatch Loss= 0.0196, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 16:23:40,039 Iter 1697, Minibatch Loss= 0.0148, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:23:41,470 Iter 1698, Minibatch Loss= 0.0282, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 16:23:42,922 Iter 1699, Minibatch Loss= 0.0172, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:23:42,924 Epoch 84, Average loss: 0.0217, learning rate: 0.0010\n",
      "2018-05-27 16:23:52,542 Iter 1700, Minibatch Loss= 0.0204, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 16:23:55,692 Iter 1700, Minibatch Loss= 0.0147, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 16:23:57,123 Iter 1701, Minibatch Loss= 0.0267, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:23:58,557 Iter 1702, Minibatch Loss= 0.0248, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:23:59,997 Iter 1703, Minibatch Loss= 0.0202, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:24:01,462 Iter 1704, Minibatch Loss= 0.0079, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 16:24:02,881 Iter 1705, Minibatch Loss= 0.0164, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 16:24:04,304 Iter 1706, Minibatch Loss= 0.0221, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 16:24:05,753 Iter 1707, Minibatch Loss= 0.0056, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 16:24:07,200 Iter 1708, Minibatch Loss= 0.0249, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:24:08,639 Iter 1709, Minibatch Loss= 0.0219, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:24:10,098 Iter 1710, Minibatch Loss= 0.0261, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:24:11,540 Iter 1711, Minibatch Loss= 0.0074, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 16:24:12,984 Iter 1712, Minibatch Loss= 0.0123, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:24:14,437 Iter 1713, Minibatch Loss= 0.0165, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 16:24:15,884 Iter 1714, Minibatch Loss= 0.0113, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:24:17,317 Iter 1715, Minibatch Loss= 0.0204, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:24:18,740 Iter 1716, Minibatch Loss= 0.0056, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 16:24:20,196 Iter 1717, Minibatch Loss= 0.0258, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 16:24:21,614 Iter 1718, Minibatch Loss= 0.0106, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:24:23,066 Iter 1719, Minibatch Loss= 0.0092, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 16:24:23,067 Epoch 85, Average loss: 0.0170, learning rate: 0.0010\n",
      "2018-05-27 16:24:32,693 Iter 1720, Minibatch Loss= 0.0197, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 16:24:35,890 Iter 1720, Minibatch Loss= 0.0243, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:24:37,348 Iter 1721, Minibatch Loss= 0.0251, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:24:38,770 Iter 1722, Minibatch Loss= 0.0276, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:24:40,235 Iter 1723, Minibatch Loss= 0.0209, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 16:24:41,676 Iter 1724, Minibatch Loss= 0.0264, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 16:24:43,134 Iter 1725, Minibatch Loss= 0.0263, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 16:24:44,522 Iter 1726, Minibatch Loss= 0.0249, Training Accuracy= 0.9538, Minibatch error= 4.6%\n",
      "2018-05-27 16:24:45,946 Iter 1727, Minibatch Loss= 0.0225, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 16:24:47,382 Iter 1728, Minibatch Loss= 0.0135, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 16:24:48,812 Iter 1729, Minibatch Loss= 0.0052, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 16:24:50,265 Iter 1730, Minibatch Loss= 0.0120, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 16:24:51,690 Iter 1731, Minibatch Loss= 0.0182, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:24:53,137 Iter 1732, Minibatch Loss= 0.0261, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 16:24:54,544 Iter 1733, Minibatch Loss= 0.0221, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 16:24:55,958 Iter 1734, Minibatch Loss= 0.0184, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:24:57,374 Iter 1735, Minibatch Loss= 0.0254, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:24:58,819 Iter 1736, Minibatch Loss= 0.0135, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:25:00,235 Iter 1737, Minibatch Loss= 0.0259, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 16:25:01,666 Iter 1738, Minibatch Loss= 0.0180, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:25:03,074 Iter 1739, Minibatch Loss= 0.0275, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 16:25:03,075 Epoch 86, Average loss: 0.0218, learning rate: 0.0010\n",
      "2018-05-27 16:25:12,553 Iter 1740, Minibatch Loss= 0.0201, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:25:15,769 Iter 1740, Minibatch Loss= 0.0092, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 16:25:17,178 Iter 1741, Minibatch Loss= 0.0283, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 16:25:18,595 Iter 1742, Minibatch Loss= 0.0276, Training Accuracy= 0.9456, Minibatch error= 5.4%\n",
      "2018-05-27 16:25:20,046 Iter 1743, Minibatch Loss= 0.0242, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 16:25:21,433 Iter 1744, Minibatch Loss= 0.0280, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:25:22,830 Iter 1745, Minibatch Loss= 0.0295, Training Accuracy= 0.9521, Minibatch error= 4.8%\n",
      "2018-05-27 16:25:24,249 Iter 1746, Minibatch Loss= 0.0279, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 16:25:25,710 Iter 1747, Minibatch Loss= 0.0288, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:25:27,131 Iter 1748, Minibatch Loss= 0.0227, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:25:28,554 Iter 1749, Minibatch Loss= 0.0140, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 16:25:29,979 Iter 1750, Minibatch Loss= 0.0262, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:25:31,407 Iter 1751, Minibatch Loss= 0.0274, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 16:25:32,841 Iter 1752, Minibatch Loss= 0.0283, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:25:34,280 Iter 1753, Minibatch Loss= 0.0120, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 16:25:35,712 Iter 1754, Minibatch Loss= 0.0164, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 16:25:37,154 Iter 1755, Minibatch Loss= 0.0315, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 16:25:38,626 Iter 1756, Minibatch Loss= 0.0121, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:25:40,056 Iter 1757, Minibatch Loss= 0.0243, Training Accuracy= 0.9699, Minibatch error= 3.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:25:41,535 Iter 1758, Minibatch Loss= 0.0106, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 16:25:43,060 Iter 1759, Minibatch Loss= 0.0284, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:25:43,061 Epoch 87, Average loss: 0.0233, learning rate: 0.0010\n",
      "2018-05-27 16:25:52,624 Iter 1760, Minibatch Loss= 0.0203, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 16:25:55,842 Iter 1760, Minibatch Loss= 0.0190, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 16:25:57,290 Iter 1761, Minibatch Loss= 0.0274, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:25:58,747 Iter 1762, Minibatch Loss= 0.0172, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 16:26:00,198 Iter 1763, Minibatch Loss= 0.0240, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:26:01,632 Iter 1764, Minibatch Loss= 0.0214, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:26:03,066 Iter 1765, Minibatch Loss= 0.0155, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 16:26:04,489 Iter 1766, Minibatch Loss= 0.0277, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 16:26:05,944 Iter 1767, Minibatch Loss= 0.0220, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:26:07,402 Iter 1768, Minibatch Loss= 0.0174, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 16:26:08,856 Iter 1769, Minibatch Loss= 0.0247, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 16:26:10,328 Iter 1770, Minibatch Loss= 0.0141, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:26:11,767 Iter 1771, Minibatch Loss= 0.0289, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:26:13,227 Iter 1772, Minibatch Loss= 0.0306, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 16:26:14,677 Iter 1773, Minibatch Loss= 0.0194, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:26:16,123 Iter 1774, Minibatch Loss= 0.0296, Training Accuracy= 0.9549, Minibatch error= 4.5%\n",
      "2018-05-27 16:26:17,560 Iter 1775, Minibatch Loss= 0.0260, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:26:19,008 Iter 1776, Minibatch Loss= 0.0148, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 16:26:20,438 Iter 1777, Minibatch Loss= 0.0271, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 16:26:21,885 Iter 1778, Minibatch Loss= 0.0125, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:26:23,324 Iter 1779, Minibatch Loss= 0.0145, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 16:26:23,325 Epoch 88, Average loss: 0.0223, learning rate: 0.0010\n",
      "2018-05-27 16:26:32,930 Iter 1780, Minibatch Loss= 0.0199, Training Accuracy= 0.9755, Minibatch error= 2.4%\n",
      "2018-05-27 16:26:36,214 Iter 1780, Minibatch Loss= 0.0292, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:26:37,658 Iter 1781, Minibatch Loss= 0.0222, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:26:39,115 Iter 1782, Minibatch Loss= 0.0248, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:26:40,564 Iter 1783, Minibatch Loss= 0.0269, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 16:26:42,047 Iter 1784, Minibatch Loss= 0.0258, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 16:26:43,497 Iter 1785, Minibatch Loss= 0.0132, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 16:26:44,921 Iter 1786, Minibatch Loss= 0.0282, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:26:46,339 Iter 1787, Minibatch Loss= 0.0276, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:26:47,764 Iter 1788, Minibatch Loss= 0.0267, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:26:49,239 Iter 1789, Minibatch Loss= 0.0061, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 16:26:50,689 Iter 1790, Minibatch Loss= 0.0153, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:26:52,190 Iter 1791, Minibatch Loss= 0.0442, Training Accuracy= 0.9346, Minibatch error= 6.5%\n",
      "2018-05-27 16:26:53,639 Iter 1792, Minibatch Loss= 0.0252, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:26:55,079 Iter 1793, Minibatch Loss= 0.0268, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:26:56,531 Iter 1794, Minibatch Loss= 0.0134, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:26:57,981 Iter 1795, Minibatch Loss= 0.0215, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 16:26:59,461 Iter 1796, Minibatch Loss= 0.0056, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 16:27:00,882 Iter 1797, Minibatch Loss= 0.0110, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 16:27:02,366 Iter 1798, Minibatch Loss= 0.0164, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 16:27:03,840 Iter 1799, Minibatch Loss= 0.0052, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 16:27:03,841 Epoch 89, Average loss: 0.0214, learning rate: 0.0010\n",
      "2018-05-27 16:27:13,438 Iter 1800, Minibatch Loss= 0.0198, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 16:27:16,798 Iter 1800, Minibatch Loss= 0.0252, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:27:18,241 Iter 1801, Minibatch Loss= 0.0277, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:27:19,698 Iter 1802, Minibatch Loss= 0.0285, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:27:21,160 Iter 1803, Minibatch Loss= 0.0165, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 16:27:22,624 Iter 1804, Minibatch Loss= 0.0278, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 16:27:24,082 Iter 1805, Minibatch Loss= 0.0296, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:27:25,558 Iter 1806, Minibatch Loss= 0.0125, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:27:27,093 Iter 1807, Minibatch Loss= 0.0135, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:27:28,529 Iter 1808, Minibatch Loss= 0.0292, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 16:27:30,013 Iter 1809, Minibatch Loss= 0.0143, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:27:31,488 Iter 1810, Minibatch Loss= 0.0090, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:27:32,921 Iter 1811, Minibatch Loss= 0.0178, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:27:34,406 Iter 1812, Minibatch Loss= 0.0216, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 16:27:35,858 Iter 1813, Minibatch Loss= 0.0306, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:27:37,363 Iter 1814, Minibatch Loss= 0.0124, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 16:27:38,820 Iter 1815, Minibatch Loss= 0.0132, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:27:40,254 Iter 1816, Minibatch Loss= 0.0262, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:27:41,737 Iter 1817, Minibatch Loss= 0.0260, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:27:43,189 Iter 1818, Minibatch Loss= 0.0191, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:27:44,645 Iter 1819, Minibatch Loss= 0.0168, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 16:27:44,646 Epoch 90, Average loss: 0.0214, learning rate: 0.0010\n",
      "2018-05-27 16:27:54,268 Iter 1820, Minibatch Loss= 0.0197, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 16:27:57,617 Iter 1820, Minibatch Loss= 0.0228, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:27:59,097 Iter 1821, Minibatch Loss= 0.0291, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 16:28:00,569 Iter 1822, Minibatch Loss= 0.0266, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 16:28:02,012 Iter 1823, Minibatch Loss= 0.0233, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:28:03,464 Iter 1824, Minibatch Loss= 0.0145, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:28:04,911 Iter 1825, Minibatch Loss= 0.0261, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:28:06,360 Iter 1826, Minibatch Loss= 0.0153, Training Accuracy= 0.9804, Minibatch error= 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:28:07,876 Iter 1827, Minibatch Loss= 0.0131, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 16:28:09,338 Iter 1828, Minibatch Loss= 0.0156, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:28:10,804 Iter 1829, Minibatch Loss= 0.0246, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:28:12,294 Iter 1830, Minibatch Loss= 0.0166, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 16:28:13,787 Iter 1831, Minibatch Loss= 0.0264, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 16:28:15,283 Iter 1832, Minibatch Loss= 0.0079, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 16:28:16,748 Iter 1833, Minibatch Loss= 0.0302, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 16:28:18,209 Iter 1834, Minibatch Loss= 0.0240, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:28:19,703 Iter 1835, Minibatch Loss= 0.0263, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:28:21,183 Iter 1836, Minibatch Loss= 0.0154, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 16:28:22,671 Iter 1837, Minibatch Loss= 0.0386, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:28:24,153 Iter 1838, Minibatch Loss= 0.0139, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:28:25,606 Iter 1839, Minibatch Loss= 0.0266, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 16:28:25,607 Epoch 91, Average loss: 0.0229, learning rate: 0.0010\n",
      "2018-05-27 16:28:35,230 Iter 1840, Minibatch Loss= 0.0197, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:28:38,544 Iter 1840, Minibatch Loss= 0.0305, Training Accuracy= 0.9456, Minibatch error= 5.4%\n",
      "2018-05-27 16:28:40,033 Iter 1841, Minibatch Loss= 0.0305, Training Accuracy= 0.9459, Minibatch error= 5.4%\n",
      "2018-05-27 16:28:41,498 Iter 1842, Minibatch Loss= 0.0241, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:28:42,998 Iter 1843, Minibatch Loss= 0.0052, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 16:28:44,440 Iter 1844, Minibatch Loss= 0.0219, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 16:28:45,930 Iter 1845, Minibatch Loss= 0.0245, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:28:47,396 Iter 1846, Minibatch Loss= 0.0075, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 16:28:48,844 Iter 1847, Minibatch Loss= 0.0277, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:28:50,324 Iter 1848, Minibatch Loss= 0.0202, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 16:28:51,799 Iter 1849, Minibatch Loss= 0.0274, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 16:28:53,250 Iter 1850, Minibatch Loss= 0.0101, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:28:54,704 Iter 1851, Minibatch Loss= 0.0159, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:28:56,192 Iter 1852, Minibatch Loss= 0.0320, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 16:28:57,645 Iter 1853, Minibatch Loss= 0.0262, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:28:59,199 Iter 1854, Minibatch Loss= 0.0122, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:29:00,688 Iter 1855, Minibatch Loss= 0.0209, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 16:29:02,136 Iter 1856, Minibatch Loss= 0.0341, Training Accuracy= 0.9437, Minibatch error= 5.6%\n",
      "2018-05-27 16:29:03,585 Iter 1857, Minibatch Loss= 0.0224, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:29:05,030 Iter 1858, Minibatch Loss= 0.0196, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:29:06,514 Iter 1859, Minibatch Loss= 0.0299, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:29:06,515 Epoch 92, Average loss: 0.0226, learning rate: 0.0010\n",
      "2018-05-27 16:29:16,104 Iter 1860, Minibatch Loss= 0.0203, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 16:29:19,613 Iter 1860, Minibatch Loss= 0.0226, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:29:21,089 Iter 1861, Minibatch Loss= 0.0253, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:29:22,560 Iter 1862, Minibatch Loss= 0.0094, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:29:24,041 Iter 1863, Minibatch Loss= 0.0291, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:29:25,485 Iter 1864, Minibatch Loss= 0.0232, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 16:29:26,983 Iter 1865, Minibatch Loss= 0.0161, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:29:28,488 Iter 1866, Minibatch Loss= 0.0131, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:29:29,958 Iter 1867, Minibatch Loss= 0.0262, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:29:31,411 Iter 1868, Minibatch Loss= 0.0207, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:29:32,873 Iter 1869, Minibatch Loss= 0.0067, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 16:29:34,392 Iter 1870, Minibatch Loss= 0.0316, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 16:29:35,894 Iter 1871, Minibatch Loss= 0.0080, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 16:29:37,339 Iter 1872, Minibatch Loss= 0.0367, Training Accuracy= 0.9418, Minibatch error= 5.8%\n",
      "2018-05-27 16:29:38,839 Iter 1873, Minibatch Loss= 0.0274, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 16:29:40,280 Iter 1874, Minibatch Loss= 0.0269, Training Accuracy= 0.9487, Minibatch error= 5.1%\n",
      "2018-05-27 16:29:41,729 Iter 1875, Minibatch Loss= 0.0259, Training Accuracy= 0.9523, Minibatch error= 4.8%\n",
      "2018-05-27 16:29:43,161 Iter 1876, Minibatch Loss= 0.0113, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:29:44,638 Iter 1877, Minibatch Loss= 0.0119, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 16:29:46,150 Iter 1878, Minibatch Loss= 0.0343, Training Accuracy= 0.9518, Minibatch error= 4.8%\n",
      "2018-05-27 16:29:47,618 Iter 1879, Minibatch Loss= 0.0148, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:29:47,619 Epoch 93, Average loss: 0.0215, learning rate: 0.0010\n",
      "2018-05-27 16:29:57,186 Iter 1880, Minibatch Loss= 0.0201, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 16:30:00,469 Iter 1880, Minibatch Loss= 0.0294, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:30:01,932 Iter 1881, Minibatch Loss= 0.0270, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 16:30:03,446 Iter 1882, Minibatch Loss= 0.0107, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 16:30:04,920 Iter 1883, Minibatch Loss= 0.0258, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:30:06,427 Iter 1884, Minibatch Loss= 0.0180, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:30:07,903 Iter 1885, Minibatch Loss= 0.0258, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:30:09,382 Iter 1886, Minibatch Loss= 0.0216, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:30:10,839 Iter 1887, Minibatch Loss= 0.0107, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:30:12,332 Iter 1888, Minibatch Loss= 0.0278, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:30:13,807 Iter 1889, Minibatch Loss= 0.0237, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 16:30:15,275 Iter 1890, Minibatch Loss= 0.0256, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:30:16,758 Iter 1891, Minibatch Loss= 0.0296, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 16:30:18,223 Iter 1892, Minibatch Loss= 0.0287, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 16:30:19,683 Iter 1893, Minibatch Loss= 0.0287, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 16:30:21,146 Iter 1894, Minibatch Loss= 0.0194, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 16:30:22,597 Iter 1895, Minibatch Loss= 0.0225, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:30:24,094 Iter 1896, Minibatch Loss= 0.0251, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:30:25,597 Iter 1897, Minibatch Loss= 0.0177, Training Accuracy= 0.9786, Minibatch error= 2.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:30:27,073 Iter 1898, Minibatch Loss= 0.0266, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 16:30:28,571 Iter 1899, Minibatch Loss= 0.0248, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 16:30:28,572 Epoch 94, Average loss: 0.0241, learning rate: 0.0010\n",
      "2018-05-27 16:30:38,189 Iter 1900, Minibatch Loss= 0.0199, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 16:30:41,589 Iter 1900, Minibatch Loss= 0.0234, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 16:30:43,080 Iter 1901, Minibatch Loss= 0.0336, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:30:44,570 Iter 1902, Minibatch Loss= 0.0106, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 16:30:46,034 Iter 1903, Minibatch Loss= 0.0175, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:30:47,479 Iter 1904, Minibatch Loss= 0.0258, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:30:48,935 Iter 1905, Minibatch Loss= 0.0225, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:30:50,400 Iter 1906, Minibatch Loss= 0.0271, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:30:51,867 Iter 1907, Minibatch Loss= 0.0263, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:30:53,320 Iter 1908, Minibatch Loss= 0.0087, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 16:30:54,818 Iter 1909, Minibatch Loss= 0.0253, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 16:30:56,304 Iter 1910, Minibatch Loss= 0.0137, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:30:57,780 Iter 1911, Minibatch Loss= 0.0038, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 16:30:59,315 Iter 1912, Minibatch Loss= 0.0236, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:31:00,767 Iter 1913, Minibatch Loss= 0.0275, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 16:31:02,256 Iter 1914, Minibatch Loss= 0.0046, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 16:31:03,722 Iter 1915, Minibatch Loss= 0.0252, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 16:31:05,226 Iter 1916, Minibatch Loss= 0.0152, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 16:31:06,680 Iter 1917, Minibatch Loss= 0.0196, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:31:08,174 Iter 1918, Minibatch Loss= 0.0143, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:31:09,621 Iter 1919, Minibatch Loss= 0.0306, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:31:09,623 Epoch 95, Average loss: 0.0205, learning rate: 0.0010\n",
      "2018-05-27 16:31:19,164 Iter 1920, Minibatch Loss= 0.0201, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:31:22,510 Iter 1920, Minibatch Loss= 0.0140, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:31:23,988 Iter 1921, Minibatch Loss= 0.0277, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:31:25,500 Iter 1922, Minibatch Loss= 0.0313, Training Accuracy= 0.9403, Minibatch error= 6.0%\n",
      "2018-05-27 16:31:26,970 Iter 1923, Minibatch Loss= 0.0255, Training Accuracy= 0.9532, Minibatch error= 4.7%\n",
      "2018-05-27 16:31:28,465 Iter 1924, Minibatch Loss= 0.0286, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:31:29,998 Iter 1925, Minibatch Loss= 0.0116, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 16:31:31,485 Iter 1926, Minibatch Loss= 0.0083, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 16:31:32,975 Iter 1927, Minibatch Loss= 0.0296, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:31:34,472 Iter 1928, Minibatch Loss= 0.0094, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 16:31:35,972 Iter 1929, Minibatch Loss= 0.0158, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 16:31:37,467 Iter 1930, Minibatch Loss= 0.0119, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 16:31:38,963 Iter 1931, Minibatch Loss= 0.0135, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 16:31:40,465 Iter 1932, Minibatch Loss= 0.0070, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 16:31:41,944 Iter 1933, Minibatch Loss= 0.0162, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:31:43,448 Iter 1934, Minibatch Loss= 0.0213, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:31:44,918 Iter 1935, Minibatch Loss= 0.0153, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 16:31:46,367 Iter 1936, Minibatch Loss= 0.0252, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 16:31:47,838 Iter 1937, Minibatch Loss= 0.0205, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 16:31:49,343 Iter 1938, Minibatch Loss= 0.0150, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:31:50,812 Iter 1939, Minibatch Loss= 0.0279, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:31:50,813 Epoch 96, Average loss: 0.0191, learning rate: 0.0010\n",
      "2018-05-27 16:32:00,416 Iter 1940, Minibatch Loss= 0.0198, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 16:32:03,833 Iter 1940, Minibatch Loss= 0.0184, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:32:05,353 Iter 1941, Minibatch Loss= 0.0140, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 16:32:06,851 Iter 1942, Minibatch Loss= 0.0268, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 16:32:08,384 Iter 1943, Minibatch Loss= 0.0136, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 16:32:09,884 Iter 1944, Minibatch Loss= 0.0080, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 16:32:11,354 Iter 1945, Minibatch Loss= 0.0262, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 16:32:12,852 Iter 1946, Minibatch Loss= 0.0127, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 16:32:14,336 Iter 1947, Minibatch Loss= 0.0183, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:32:15,817 Iter 1948, Minibatch Loss= 0.0314, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 16:32:17,290 Iter 1949, Minibatch Loss= 0.0246, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:32:18,788 Iter 1950, Minibatch Loss= 0.0073, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 16:32:20,263 Iter 1951, Minibatch Loss= 0.0159, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 16:32:21,732 Iter 1952, Minibatch Loss= 0.0199, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:32:23,192 Iter 1953, Minibatch Loss= 0.0148, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 16:32:24,697 Iter 1954, Minibatch Loss= 0.0146, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:32:26,197 Iter 1955, Minibatch Loss= 0.0294, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:32:27,703 Iter 1956, Minibatch Loss= 0.0076, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 16:32:29,242 Iter 1957, Minibatch Loss= 0.0259, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 16:32:30,760 Iter 1958, Minibatch Loss= 0.0235, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:32:32,254 Iter 1959, Minibatch Loss= 0.0255, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:32:32,255 Epoch 97, Average loss: 0.0196, learning rate: 0.0010\n",
      "2018-05-27 16:32:41,870 Iter 1960, Minibatch Loss= 0.0196, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 16:32:45,363 Iter 1960, Minibatch Loss= 0.0079, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 16:32:46,843 Iter 1961, Minibatch Loss= 0.0205, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 16:32:48,357 Iter 1962, Minibatch Loss= 0.0217, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 16:32:49,875 Iter 1963, Minibatch Loss= 0.0055, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 16:32:51,351 Iter 1964, Minibatch Loss= 0.0114, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 16:32:52,853 Iter 1965, Minibatch Loss= 0.0284, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:32:54,353 Iter 1966, Minibatch Loss= 0.0267, Training Accuracy= 0.9619, Minibatch error= 3.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:32:55,859 Iter 1967, Minibatch Loss= 0.0262, Training Accuracy= 0.9635, Minibatch error= 3.6%\n",
      "2018-05-27 16:32:57,373 Iter 1968, Minibatch Loss= 0.0171, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 16:32:58,908 Iter 1969, Minibatch Loss= 0.0254, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 16:33:00,429 Iter 1970, Minibatch Loss= 0.0125, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 16:33:01,920 Iter 1971, Minibatch Loss= 0.0254, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:33:03,455 Iter 1972, Minibatch Loss= 0.0080, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 16:33:04,984 Iter 1973, Minibatch Loss= 0.0110, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 16:33:06,470 Iter 1974, Minibatch Loss= 0.0209, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 16:33:08,012 Iter 1975, Minibatch Loss= 0.0237, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:33:09,527 Iter 1976, Minibatch Loss= 0.0257, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:33:11,041 Iter 1977, Minibatch Loss= 0.0165, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 16:33:12,543 Iter 1978, Minibatch Loss= 0.0085, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 16:33:14,038 Iter 1979, Minibatch Loss= 0.0105, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 16:33:14,039 Epoch 98, Average loss: 0.0182, learning rate: 0.0010\n",
      "2018-05-27 16:33:23,637 Iter 1980, Minibatch Loss= 0.0196, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 16:33:27,091 Iter 1980, Minibatch Loss= 0.0296, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 16:33:28,586 Iter 1981, Minibatch Loss= 0.0153, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:33:30,100 Iter 1982, Minibatch Loss= 0.0046, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 16:33:31,647 Iter 1983, Minibatch Loss= 0.0229, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 16:33:33,166 Iter 1984, Minibatch Loss= 0.0254, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 16:33:34,683 Iter 1985, Minibatch Loss= 0.0134, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 16:33:36,196 Iter 1986, Minibatch Loss= 0.0145, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 16:33:37,693 Iter 1987, Minibatch Loss= 0.0100, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 16:33:39,197 Iter 1988, Minibatch Loss= 0.0134, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 16:33:40,687 Iter 1989, Minibatch Loss= 0.0085, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 16:33:42,171 Iter 1990, Minibatch Loss= 0.0262, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:33:43,664 Iter 1991, Minibatch Loss= 0.0078, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 16:33:45,130 Iter 1992, Minibatch Loss= 0.0257, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:33:46,630 Iter 1993, Minibatch Loss= 0.0176, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 16:33:48,122 Iter 1994, Minibatch Loss= 0.0275, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 16:33:49,619 Iter 1995, Minibatch Loss= 0.0086, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 16:33:51,118 Iter 1996, Minibatch Loss= 0.0434, Training Accuracy= 0.9401, Minibatch error= 6.0%\n",
      "2018-05-27 16:33:52,619 Iter 1997, Minibatch Loss= 0.0189, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:33:54,115 Iter 1998, Minibatch Loss= 0.0253, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:33:55,598 Iter 1999, Minibatch Loss= 0.0144, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:33:55,600 Epoch 99, Average loss: 0.0192, learning rate: 0.0010\n",
      "2018-05-27 16:34:05,163 Iter 2000, Minibatch Loss= 0.0195, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:34:07,171 Optimization Finished!\n",
      "2018-05-27 16:34:07,408 Layers 3, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:34:10,009 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 16:34:07.401406-#epoch100-#iter20-lambda0.0001'\n",
      "2018-05-27 16:34:10,010 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 16:34:07.401406-#epoch100-#iter20-lambda0.0001'\n",
      "2018-05-27 16:34:29,825 Verification error= 22.4%, loss= 0.2715\n",
      "2018-05-27 16:34:30,148 Start optimization\n",
      "2018-05-27 16:34:39,914 Iter 0, Minibatch Loss= 0.2715, Training Accuracy= 0.7758, Minibatch error= 22.4%\n",
      "2018-05-27 16:34:41,465 Iter 0, Minibatch Loss= 0.3567, Training Accuracy= 0.5670, Minibatch error= 43.3%\n",
      "2018-05-27 16:34:42,570 Iter 1, Minibatch Loss= 0.3254, Training Accuracy= 0.3825, Minibatch error= 61.8%\n",
      "2018-05-27 16:34:43,671 Iter 2, Minibatch Loss= 0.2160, Training Accuracy= 0.1040, Minibatch error= 89.6%\n",
      "2018-05-27 16:34:44,770 Iter 3, Minibatch Loss= 0.2972, Training Accuracy= 0.3227, Minibatch error= 67.7%\n",
      "2018-05-27 16:34:45,838 Iter 4, Minibatch Loss= 0.2164, Training Accuracy= 0.1087, Minibatch error= 89.1%\n",
      "2018-05-27 16:34:46,911 Iter 5, Minibatch Loss= 0.2057, Training Accuracy= 0.0893, Minibatch error= 91.1%\n",
      "2018-05-27 16:34:47,992 Iter 6, Minibatch Loss= 0.1777, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 16:34:49,063 Iter 7, Minibatch Loss= 0.1785, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 16:34:50,150 Iter 8, Minibatch Loss= 0.1599, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 16:34:51,252 Iter 9, Minibatch Loss= 0.3521, Training Accuracy= 0.5727, Minibatch error= 42.7%\n",
      "2018-05-27 16:34:52,357 Iter 10, Minibatch Loss= 0.1778, Training Accuracy= 0.9492, Minibatch error= 5.1%\n",
      "2018-05-27 16:34:53,447 Iter 11, Minibatch Loss= 0.1810, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 16:34:54,558 Iter 12, Minibatch Loss= 0.3525, Training Accuracy= 0.5941, Minibatch error= 40.6%\n",
      "2018-05-27 16:34:55,624 Iter 13, Minibatch Loss= 0.1626, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:34:56,711 Iter 14, Minibatch Loss= 0.3224, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:34:57,817 Iter 15, Minibatch Loss= 0.1649, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 16:34:58,938 Iter 16, Minibatch Loss= 0.3180, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:35:00,063 Iter 17, Minibatch Loss= 0.2944, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 16:35:01,128 Iter 18, Minibatch Loss= 0.1727, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:35:02,226 Iter 19, Minibatch Loss= 0.3087, Training Accuracy= 0.7073, Minibatch error= 29.3%\n",
      "2018-05-27 16:35:02,227 Epoch 0, Average loss: 0.2540, learning rate: 0.0010\n",
      "2018-05-27 16:35:11,767 Iter 20, Minibatch Loss= 0.2316, Training Accuracy= 0.5907, Minibatch error= 40.9%\n",
      "2018-05-27 16:35:13,271 Iter 20, Minibatch Loss= 0.1696, Training Accuracy= 0.7453, Minibatch error= 25.5%\n",
      "2018-05-27 16:35:14,359 Iter 21, Minibatch Loss= 0.1557, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 16:35:15,493 Iter 22, Minibatch Loss= 0.2602, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:35:16,601 Iter 23, Minibatch Loss= 0.2711, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 16:35:17,705 Iter 24, Minibatch Loss= 0.2728, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:35:18,800 Iter 25, Minibatch Loss= 0.1394, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 16:35:19,920 Iter 26, Minibatch Loss= 0.1345, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 16:35:20,994 Iter 27, Minibatch Loss= 0.1172, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:35:22,103 Iter 28, Minibatch Loss= 0.2273, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:35:23,190 Iter 29, Minibatch Loss= 0.2028, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 16:35:24,302 Iter 30, Minibatch Loss= 0.1601, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 16:35:25,413 Iter 31, Minibatch Loss= 0.0940, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 16:35:26,501 Iter 32, Minibatch Loss= 0.0793, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 16:35:27,570 Iter 33, Minibatch Loss= 0.0781, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 16:35:28,736 Iter 34, Minibatch Loss= 0.0969, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:35:29,798 Iter 35, Minibatch Loss= 0.1041, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:35:30,866 Iter 36, Minibatch Loss= 0.0452, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 16:35:31,961 Iter 37, Minibatch Loss= 0.0663, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:35:33,076 Iter 38, Minibatch Loss= 0.0366, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 16:35:34,179 Iter 39, Minibatch Loss= 0.0651, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:35:34,180 Epoch 1, Average loss: 0.1561, learning rate: 0.0010\n",
      "2018-05-27 16:35:43,743 Iter 40, Minibatch Loss= 0.0585, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 16:35:45,282 Iter 40, Minibatch Loss= 0.0407, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 16:35:46,402 Iter 41, Minibatch Loss= 0.0406, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 16:35:47,501 Iter 42, Minibatch Loss= 0.0354, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:35:48,600 Iter 43, Minibatch Loss= 0.0296, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 16:35:49,688 Iter 44, Minibatch Loss= 0.0294, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:35:50,793 Iter 45, Minibatch Loss= 0.0491, Training Accuracy= 0.9507, Minibatch error= 4.9%\n",
      "2018-05-27 16:35:51,886 Iter 46, Minibatch Loss= 0.1306, Training Accuracy= 0.9497, Minibatch error= 5.0%\n",
      "2018-05-27 16:35:52,981 Iter 47, Minibatch Loss= 0.0264, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:35:54,063 Iter 48, Minibatch Loss= 0.0508, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:35:55,144 Iter 49, Minibatch Loss= 0.0539, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 16:35:56,254 Iter 50, Minibatch Loss= 0.0696, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:35:57,340 Iter 51, Minibatch Loss= 0.0607, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:35:58,454 Iter 52, Minibatch Loss= 0.0636, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:35:59,560 Iter 53, Minibatch Loss= 0.0333, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:36:00,627 Iter 54, Minibatch Loss= 0.0412, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 16:36:01,707 Iter 55, Minibatch Loss= 0.0484, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:36:02,800 Iter 56, Minibatch Loss= 0.0467, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:36:03,900 Iter 57, Minibatch Loss= 0.0437, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 16:36:04,981 Iter 58, Minibatch Loss= 0.0560, Training Accuracy= 0.9461, Minibatch error= 5.4%\n",
      "2018-05-27 16:36:06,086 Iter 59, Minibatch Loss= 0.0624, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 16:36:06,087 Epoch 2, Average loss: 0.0563, learning rate: 0.0010\n",
      "2018-05-27 16:36:15,680 Iter 60, Minibatch Loss= 0.0625, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 16:36:17,258 Iter 60, Minibatch Loss= 0.0518, Training Accuracy= 0.9945, Minibatch error= 0.5%\n",
      "2018-05-27 16:36:18,382 Iter 61, Minibatch Loss= 0.0529, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 16:36:19,475 Iter 62, Minibatch Loss= 0.0440, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:36:20,605 Iter 63, Minibatch Loss= 0.0436, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:36:21,725 Iter 64, Minibatch Loss= 0.0350, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:36:22,859 Iter 65, Minibatch Loss= 0.0290, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:36:23,955 Iter 66, Minibatch Loss= 0.0313, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:36:25,086 Iter 67, Minibatch Loss= 0.0269, Training Accuracy= 0.9872, Minibatch error= 1.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:36:26,238 Iter 68, Minibatch Loss= 0.0459, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:36:27,380 Iter 69, Minibatch Loss= 0.0411, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 16:36:28,537 Iter 70, Minibatch Loss= 0.0273, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 16:36:29,660 Iter 71, Minibatch Loss= 0.0282, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:36:30,774 Iter 72, Minibatch Loss= 0.0370, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:36:31,881 Iter 73, Minibatch Loss= 0.0258, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 16:36:33,017 Iter 74, Minibatch Loss= 0.0280, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:36:34,133 Iter 75, Minibatch Loss= 0.0340, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 16:36:35,249 Iter 76, Minibatch Loss= 0.0268, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 16:36:36,384 Iter 77, Minibatch Loss= 0.0275, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 16:36:37,509 Iter 78, Minibatch Loss= 0.0328, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:36:38,642 Iter 79, Minibatch Loss= 0.0447, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 16:36:38,643 Epoch 3, Average loss: 0.0379, learning rate: 0.0010\n",
      "2018-05-27 16:36:48,230 Iter 80, Minibatch Loss= 0.0348, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:36:49,753 Iter 80, Minibatch Loss= 0.0297, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 16:36:50,858 Iter 81, Minibatch Loss= 0.0309, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 16:36:51,961 Iter 82, Minibatch Loss= 0.0372, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:36:53,072 Iter 83, Minibatch Loss= 0.0411, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 16:36:54,182 Iter 84, Minibatch Loss= 0.0434, Training Accuracy= 0.9510, Minibatch error= 4.9%\n",
      "2018-05-27 16:36:55,294 Iter 85, Minibatch Loss= 0.0286, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 16:36:56,411 Iter 86, Minibatch Loss= 0.0367, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:36:57,513 Iter 87, Minibatch Loss= 0.0353, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:36:58,634 Iter 88, Minibatch Loss= 0.0341, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:36:59,740 Iter 89, Minibatch Loss= 0.0368, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:37:00,818 Iter 90, Minibatch Loss= 0.0308, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 16:37:01,930 Iter 91, Minibatch Loss= 0.0298, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:37:03,033 Iter 92, Minibatch Loss= 0.0263, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:37:04,147 Iter 93, Minibatch Loss= 0.0333, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 16:37:05,264 Iter 94, Minibatch Loss= 0.0331, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 16:37:06,369 Iter 95, Minibatch Loss= 0.0332, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 16:37:07,455 Iter 96, Minibatch Loss= 0.0195, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 16:37:08,550 Iter 97, Minibatch Loss= 0.0326, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 16:37:09,640 Iter 98, Minibatch Loss= 0.0189, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 16:37:10,739 Iter 99, Minibatch Loss= 0.0198, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 16:37:10,740 Epoch 4, Average loss: 0.0322, learning rate: 0.0010\n",
      "2018-05-27 16:37:20,329 Iter 100, Minibatch Loss= 0.0276, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:37:21,877 Iter 100, Minibatch Loss= 0.0247, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 16:37:23,005 Iter 101, Minibatch Loss= 0.0314, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:37:24,122 Iter 102, Minibatch Loss= 0.0219, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:37:25,210 Iter 103, Minibatch Loss= 0.0177, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 16:37:26,325 Iter 104, Minibatch Loss= 0.0257, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 16:37:27,427 Iter 105, Minibatch Loss= 0.0265, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:37:28,566 Iter 106, Minibatch Loss= 0.0323, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 16:37:29,687 Iter 107, Minibatch Loss= 0.0225, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 16:37:30,823 Iter 108, Minibatch Loss= 0.0367, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 16:37:31,941 Iter 109, Minibatch Loss= 0.0243, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:37:33,032 Iter 110, Minibatch Loss= 0.0220, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 16:37:34,166 Iter 111, Minibatch Loss= 0.0237, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:37:35,285 Iter 112, Minibatch Loss= 0.0184, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 16:37:36,408 Iter 113, Minibatch Loss= 0.0237, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:37:37,499 Iter 114, Minibatch Loss= 0.0193, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 16:37:38,609 Iter 115, Minibatch Loss= 0.0365, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:37:39,751 Iter 116, Minibatch Loss= 0.0367, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 16:37:40,868 Iter 117, Minibatch Loss= 0.0298, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:37:41,979 Iter 118, Minibatch Loss= 0.0277, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:37:43,067 Iter 119, Minibatch Loss= 0.0237, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:37:43,068 Epoch 5, Average loss: 0.0266, learning rate: 0.0010\n",
      "2018-05-27 16:37:52,542 Iter 120, Minibatch Loss= 0.0291, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 16:37:54,093 Iter 120, Minibatch Loss= 0.0323, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:37:55,175 Iter 121, Minibatch Loss= 0.0352, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 16:37:56,282 Iter 122, Minibatch Loss= 0.0271, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 16:37:57,414 Iter 123, Minibatch Loss= 0.0360, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 16:37:58,522 Iter 124, Minibatch Loss= 0.0261, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:37:59,627 Iter 125, Minibatch Loss= 0.0177, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 16:38:00,748 Iter 126, Minibatch Loss= 0.0233, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 16:38:01,871 Iter 127, Minibatch Loss= 0.0370, Training Accuracy= 0.9475, Minibatch error= 5.2%\n",
      "2018-05-27 16:38:02,971 Iter 128, Minibatch Loss= 0.0237, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 16:38:04,093 Iter 129, Minibatch Loss= 0.0312, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 16:38:05,193 Iter 130, Minibatch Loss= 0.0330, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:38:06,305 Iter 131, Minibatch Loss= 0.0372, Training Accuracy= 0.9494, Minibatch error= 5.1%\n",
      "2018-05-27 16:38:07,416 Iter 132, Minibatch Loss= 0.0346, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 16:38:08,530 Iter 133, Minibatch Loss= 0.0333, Training Accuracy= 0.9583, Minibatch error= 4.2%\n",
      "2018-05-27 16:38:09,680 Iter 134, Minibatch Loss= 0.0309, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 16:38:10,851 Iter 135, Minibatch Loss= 0.0346, Training Accuracy= 0.9553, Minibatch error= 4.5%\n",
      "2018-05-27 16:38:11,972 Iter 136, Minibatch Loss= 0.0321, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 16:38:13,081 Iter 137, Minibatch Loss= 0.0198, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 16:38:14,186 Iter 138, Minibatch Loss= 0.0284, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:38:15,315 Iter 139, Minibatch Loss= 0.0240, Training Accuracy= 0.9746, Minibatch error= 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:38:15,316 Epoch 6, Average loss: 0.0305, learning rate: 0.0010\n",
      "2018-05-27 16:38:24,837 Iter 140, Minibatch Loss= 0.0267, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 16:38:26,488 Iter 140, Minibatch Loss= 0.0358, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 16:38:27,605 Iter 141, Minibatch Loss= 0.0266, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:38:28,722 Iter 142, Minibatch Loss= 0.0238, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 16:38:29,836 Iter 143, Minibatch Loss= 0.0216, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 16:38:30,971 Iter 144, Minibatch Loss= 0.0401, Training Accuracy= 0.9482, Minibatch error= 5.2%\n",
      "2018-05-27 16:38:32,064 Iter 145, Minibatch Loss= 0.0119, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 16:38:33,187 Iter 146, Minibatch Loss= 0.0284, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 16:38:34,305 Iter 147, Minibatch Loss= 0.0322, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:38:35,422 Iter 148, Minibatch Loss= 0.0332, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:38:36,545 Iter 149, Minibatch Loss= 0.0236, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 16:38:37,638 Iter 150, Minibatch Loss= 0.0283, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:38:38,794 Iter 151, Minibatch Loss= 0.0312, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:38:39,904 Iter 152, Minibatch Loss= 0.0354, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 16:38:41,006 Iter 153, Minibatch Loss= 0.0283, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:38:42,120 Iter 154, Minibatch Loss= 0.0296, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:38:43,272 Iter 155, Minibatch Loss= 0.0337, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 16:38:44,388 Iter 156, Minibatch Loss= 0.0238, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 16:38:45,514 Iter 157, Minibatch Loss= 0.0245, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 16:38:46,642 Iter 158, Minibatch Loss= 0.0352, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 16:38:47,769 Iter 159, Minibatch Loss= 0.0343, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:38:47,770 Epoch 7, Average loss: 0.0289, learning rate: 0.0010\n",
      "2018-05-27 16:38:57,309 Iter 160, Minibatch Loss= 0.0261, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 16:38:58,932 Iter 160, Minibatch Loss= 0.0326, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 16:39:00,063 Iter 161, Minibatch Loss= 0.0228, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:39:01,193 Iter 162, Minibatch Loss= 0.0341, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 16:39:02,322 Iter 163, Minibatch Loss= 0.0357, Training Accuracy= 0.9516, Minibatch error= 4.8%\n",
      "2018-05-27 16:39:03,430 Iter 164, Minibatch Loss= 0.0244, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:39:04,547 Iter 165, Minibatch Loss= 0.0182, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 16:39:05,679 Iter 166, Minibatch Loss= 0.0323, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 16:39:06,790 Iter 167, Minibatch Loss= 0.0361, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 16:39:07,922 Iter 168, Minibatch Loss= 0.0326, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 16:39:09,041 Iter 169, Minibatch Loss= 0.0220, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 16:39:10,180 Iter 170, Minibatch Loss= 0.0404, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 16:39:11,318 Iter 171, Minibatch Loss= 0.0348, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:39:12,443 Iter 172, Minibatch Loss= 0.0204, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 16:39:13,603 Iter 173, Minibatch Loss= 0.0188, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 16:39:14,718 Iter 174, Minibatch Loss= 0.0190, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:39:15,850 Iter 175, Minibatch Loss= 0.0332, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:39:16,992 Iter 176, Minibatch Loss= 0.0174, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 16:39:18,113 Iter 177, Minibatch Loss= 0.0222, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:39:19,241 Iter 178, Minibatch Loss= 0.0365, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:39:20,354 Iter 179, Minibatch Loss= 0.0311, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:39:20,356 Epoch 8, Average loss: 0.0270, learning rate: 0.0010\n",
      "2018-05-27 16:39:29,920 Iter 180, Minibatch Loss= 0.0349, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:39:31,624 Iter 180, Minibatch Loss= 0.0339, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 16:39:32,751 Iter 181, Minibatch Loss= 0.0346, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 16:39:33,868 Iter 182, Minibatch Loss= 0.0209, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:39:35,003 Iter 183, Minibatch Loss= 0.0274, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 16:39:36,154 Iter 184, Minibatch Loss= 0.0297, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 16:39:37,269 Iter 185, Minibatch Loss= 0.0189, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:39:38,408 Iter 186, Minibatch Loss= 0.0318, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 16:39:39,524 Iter 187, Minibatch Loss= 0.0159, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 16:39:40,635 Iter 188, Minibatch Loss= 0.0297, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 16:39:41,743 Iter 189, Minibatch Loss= 0.0217, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:39:42,854 Iter 190, Minibatch Loss= 0.0139, Training Accuracy= 0.9918, Minibatch error= 0.8%\n",
      "2018-05-27 16:39:43,993 Iter 191, Minibatch Loss= 0.0179, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 16:39:45,094 Iter 192, Minibatch Loss= 0.0147, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:39:46,219 Iter 193, Minibatch Loss= 0.0125, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 16:39:47,331 Iter 194, Minibatch Loss= 0.0315, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:39:48,442 Iter 195, Minibatch Loss= 0.0131, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 16:39:49,568 Iter 196, Minibatch Loss= 0.0287, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 16:39:50,694 Iter 197, Minibatch Loss= 0.0316, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:39:51,809 Iter 198, Minibatch Loss= 0.0318, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:39:52,921 Iter 199, Minibatch Loss= 0.0201, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 16:39:52,922 Epoch 9, Average loss: 0.0232, learning rate: 0.0010\n",
      "2018-05-27 16:40:02,461 Iter 200, Minibatch Loss= 0.0251, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:40:04,090 Iter 200, Minibatch Loss= 0.0148, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 16:40:05,219 Iter 201, Minibatch Loss= 0.0206, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:40:06,342 Iter 202, Minibatch Loss= 0.0306, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:40:07,476 Iter 203, Minibatch Loss= 0.0208, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 16:40:08,607 Iter 204, Minibatch Loss= 0.0313, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 16:40:09,732 Iter 205, Minibatch Loss= 0.0332, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 16:40:10,882 Iter 206, Minibatch Loss= 0.0153, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 16:40:12,007 Iter 207, Minibatch Loss= 0.0188, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:40:13,133 Iter 208, Minibatch Loss= 0.0155, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 16:40:14,231 Iter 209, Minibatch Loss= 0.0181, Training Accuracy= 0.9859, Minibatch error= 1.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:40:15,352 Iter 210, Minibatch Loss= 0.0138, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 16:40:16,509 Iter 211, Minibatch Loss= 0.0143, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 16:40:17,657 Iter 212, Minibatch Loss= 0.0298, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:40:18,811 Iter 213, Minibatch Loss= 0.0315, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:40:19,950 Iter 214, Minibatch Loss= 0.0342, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 16:40:21,077 Iter 215, Minibatch Loss= 0.0347, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:40:22,216 Iter 216, Minibatch Loss= 0.0265, Training Accuracy= 0.9925, Minibatch error= 0.7%\n",
      "2018-05-27 16:40:23,367 Iter 217, Minibatch Loss= 0.0417, Training Accuracy= 0.9451, Minibatch error= 5.5%\n",
      "2018-05-27 16:40:24,539 Iter 218, Minibatch Loss= 0.0326, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:40:25,638 Iter 219, Minibatch Loss= 0.0218, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 16:40:25,639 Epoch 10, Average loss: 0.0238, learning rate: 0.0010\n",
      "2018-05-27 16:40:35,136 Iter 220, Minibatch Loss= 0.0250, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 16:40:36,808 Iter 220, Minibatch Loss= 0.0308, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:40:37,936 Iter 221, Minibatch Loss= 0.0233, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 16:40:39,080 Iter 222, Minibatch Loss= 0.0309, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 16:40:40,235 Iter 223, Minibatch Loss= 0.0314, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 16:40:41,372 Iter 224, Minibatch Loss= 0.0249, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:40:42,480 Iter 225, Minibatch Loss= 0.0265, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 16:40:43,581 Iter 226, Minibatch Loss= 0.0132, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 16:40:44,734 Iter 227, Minibatch Loss= 0.0342, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:40:45,853 Iter 228, Minibatch Loss= 0.0247, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 16:40:46,990 Iter 229, Minibatch Loss= 0.0312, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:40:48,156 Iter 230, Minibatch Loss= 0.0341, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 16:40:49,278 Iter 231, Minibatch Loss= 0.0189, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 16:40:50,405 Iter 232, Minibatch Loss= 0.0188, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 16:40:51,512 Iter 233, Minibatch Loss= 0.0129, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 16:40:52,632 Iter 234, Minibatch Loss= 0.0086, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 16:40:53,743 Iter 235, Minibatch Loss= 0.0195, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 16:40:54,864 Iter 236, Minibatch Loss= 0.0174, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 16:40:55,975 Iter 237, Minibatch Loss= 0.0398, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 16:40:57,112 Iter 238, Minibatch Loss= 0.0259, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 16:40:58,234 Iter 239, Minibatch Loss= 0.0178, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 16:40:58,236 Epoch 11, Average loss: 0.0244, learning rate: 0.0010\n",
      "2018-05-27 16:41:07,752 Iter 240, Minibatch Loss= 0.0243, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 16:41:09,409 Iter 240, Minibatch Loss= 0.0261, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 16:41:10,547 Iter 241, Minibatch Loss= 0.0328, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 16:41:11,653 Iter 242, Minibatch Loss= 0.0297, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:41:12,784 Iter 243, Minibatch Loss= 0.0212, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 16:41:13,902 Iter 244, Minibatch Loss= 0.0276, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:41:15,039 Iter 245, Minibatch Loss= 0.0287, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:41:16,173 Iter 246, Minibatch Loss= 0.0100, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 16:41:17,312 Iter 247, Minibatch Loss= 0.0360, Training Accuracy= 0.9498, Minibatch error= 5.0%\n",
      "2018-05-27 16:41:18,465 Iter 248, Minibatch Loss= 0.0186, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:41:19,573 Iter 249, Minibatch Loss= 0.0150, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 16:41:20,720 Iter 250, Minibatch Loss= 0.0317, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:41:21,871 Iter 251, Minibatch Loss= 0.0307, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:41:23,002 Iter 252, Minibatch Loss= 0.0179, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:41:24,130 Iter 253, Minibatch Loss= 0.0131, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 16:41:25,265 Iter 254, Minibatch Loss= 0.0259, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:41:26,369 Iter 255, Minibatch Loss= 0.0181, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 16:41:27,488 Iter 256, Minibatch Loss= 0.0090, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 16:41:28,605 Iter 257, Minibatch Loss= 0.0297, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:41:29,744 Iter 258, Minibatch Loss= 0.0249, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 16:41:30,846 Iter 259, Minibatch Loss= 0.0298, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:41:30,847 Epoch 12, Average loss: 0.0238, learning rate: 0.0010\n",
      "2018-05-27 16:41:40,387 Iter 260, Minibatch Loss= 0.0317, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:41:42,072 Iter 260, Minibatch Loss= 0.0399, Training Accuracy= 0.9473, Minibatch error= 5.3%\n",
      "2018-05-27 16:41:43,216 Iter 261, Minibatch Loss= 0.0345, Training Accuracy= 0.9540, Minibatch error= 4.6%\n",
      "2018-05-27 16:41:44,349 Iter 262, Minibatch Loss= 0.0333, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 16:41:45,506 Iter 263, Minibatch Loss= 0.0289, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:41:46,639 Iter 264, Minibatch Loss= 0.0218, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 16:41:47,772 Iter 265, Minibatch Loss= 0.0334, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:41:48,914 Iter 266, Minibatch Loss= 0.0297, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:41:50,068 Iter 267, Minibatch Loss= 0.0300, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:41:51,234 Iter 268, Minibatch Loss= 0.0324, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:41:52,376 Iter 269, Minibatch Loss= 0.0222, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 16:41:53,544 Iter 270, Minibatch Loss= 0.0355, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 16:41:54,683 Iter 271, Minibatch Loss= 0.0324, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:41:55,781 Iter 272, Minibatch Loss= 0.0127, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 16:41:56,914 Iter 273, Minibatch Loss= 0.0198, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:41:58,046 Iter 274, Minibatch Loss= 0.0167, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 16:41:59,191 Iter 275, Minibatch Loss= 0.0157, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 16:42:00,350 Iter 276, Minibatch Loss= 0.0295, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:01,523 Iter 277, Minibatch Loss= 0.0276, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:42:02,656 Iter 278, Minibatch Loss= 0.0288, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:03,808 Iter 279, Minibatch Loss= 0.0294, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:03,809 Epoch 13, Average loss: 0.0280, learning rate: 0.0010\n",
      "2018-05-27 16:42:13,321 Iter 280, Minibatch Loss= 0.0243, Training Accuracy= 0.9749, Minibatch error= 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:42:15,001 Iter 280, Minibatch Loss= 0.0359, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:16,120 Iter 281, Minibatch Loss= 0.0226, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 16:42:17,258 Iter 282, Minibatch Loss= 0.0319, Training Accuracy= 0.9566, Minibatch error= 4.3%\n",
      "2018-05-27 16:42:18,432 Iter 283, Minibatch Loss= 0.0341, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 16:42:19,600 Iter 284, Minibatch Loss= 0.0291, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:42:20,740 Iter 285, Minibatch Loss= 0.0301, Training Accuracy= 0.9587, Minibatch error= 4.1%\n",
      "2018-05-27 16:42:21,891 Iter 286, Minibatch Loss= 0.0285, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:23,032 Iter 287, Minibatch Loss= 0.0305, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:42:24,189 Iter 288, Minibatch Loss= 0.0325, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 16:42:25,334 Iter 289, Minibatch Loss= 0.0262, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:26,481 Iter 290, Minibatch Loss= 0.0280, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:42:27,602 Iter 291, Minibatch Loss= 0.0260, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 16:42:28,751 Iter 292, Minibatch Loss= 0.0244, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:42:29,885 Iter 293, Minibatch Loss= 0.0213, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 16:42:31,028 Iter 294, Minibatch Loss= 0.0182, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 16:42:32,174 Iter 295, Minibatch Loss= 0.0304, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 16:42:33,299 Iter 296, Minibatch Loss= 0.0166, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 16:42:34,453 Iter 297, Minibatch Loss= 0.0328, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 16:42:35,573 Iter 298, Minibatch Loss= 0.0200, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:42:36,718 Iter 299, Minibatch Loss= 0.0157, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 16:42:36,719 Epoch 14, Average loss: 0.0266, learning rate: 0.0010\n",
      "2018-05-27 16:42:46,258 Iter 300, Minibatch Loss= 0.0245, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 16:42:47,993 Iter 300, Minibatch Loss= 0.0289, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:42:49,140 Iter 301, Minibatch Loss= 0.0142, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 16:42:50,292 Iter 302, Minibatch Loss= 0.0198, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 16:42:51,437 Iter 303, Minibatch Loss= 0.0290, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:42:52,584 Iter 304, Minibatch Loss= 0.0180, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:42:53,748 Iter 305, Minibatch Loss= 0.0097, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 16:42:54,898 Iter 306, Minibatch Loss= 0.0213, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 16:42:56,033 Iter 307, Minibatch Loss= 0.0303, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:42:57,178 Iter 308, Minibatch Loss= 0.0262, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 16:42:58,325 Iter 309, Minibatch Loss= 0.0325, Training Accuracy= 0.9555, Minibatch error= 4.5%\n",
      "2018-05-27 16:42:59,461 Iter 310, Minibatch Loss= 0.0269, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 16:43:00,628 Iter 311, Minibatch Loss= 0.0305, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 16:43:01,801 Iter 312, Minibatch Loss= 0.0332, Training Accuracy= 0.9472, Minibatch error= 5.3%\n",
      "2018-05-27 16:43:02,928 Iter 313, Minibatch Loss= 0.0164, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:43:04,088 Iter 314, Minibatch Loss= 0.0103, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 16:43:05,223 Iter 315, Minibatch Loss= 0.0132, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 16:43:06,359 Iter 316, Minibatch Loss= 0.0255, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:43:07,481 Iter 317, Minibatch Loss= 0.0226, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 16:43:08,663 Iter 318, Minibatch Loss= 0.0292, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:43:09,828 Iter 319, Minibatch Loss= 0.0313, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 16:43:09,829 Epoch 15, Average loss: 0.0233, learning rate: 0.0010\n",
      "2018-05-27 16:43:19,375 Iter 320, Minibatch Loss= 0.0278, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:43:21,182 Iter 320, Minibatch Loss= 0.0336, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:43:22,346 Iter 321, Minibatch Loss= 0.0341, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 16:43:23,504 Iter 322, Minibatch Loss= 0.0223, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 16:43:24,664 Iter 323, Minibatch Loss= 0.0124, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 16:43:25,809 Iter 324, Minibatch Loss= 0.0249, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 16:43:26,975 Iter 325, Minibatch Loss= 0.0189, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 16:43:28,106 Iter 326, Minibatch Loss= 0.0286, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 16:43:29,254 Iter 327, Minibatch Loss= 0.0139, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 16:43:30,388 Iter 328, Minibatch Loss= 0.0152, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 16:43:31,524 Iter 329, Minibatch Loss= 0.0165, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 16:43:32,665 Iter 330, Minibatch Loss= 0.0323, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:43:33,831 Iter 331, Minibatch Loss= 0.0264, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:43:34,982 Iter 332, Minibatch Loss= 0.0319, Training Accuracy= 0.9558, Minibatch error= 4.4%\n",
      "2018-05-27 16:43:36,125 Iter 333, Minibatch Loss= 0.0133, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 16:43:37,281 Iter 334, Minibatch Loss= 0.0307, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:43:38,441 Iter 335, Minibatch Loss= 0.0327, Training Accuracy= 0.9486, Minibatch error= 5.1%\n",
      "2018-05-27 16:43:39,581 Iter 336, Minibatch Loss= 0.0139, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 16:43:40,714 Iter 337, Minibatch Loss= 0.0340, Training Accuracy= 0.9499, Minibatch error= 5.0%\n",
      "2018-05-27 16:43:41,862 Iter 338, Minibatch Loss= 0.0187, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:43:42,992 Iter 339, Minibatch Loss= 0.0324, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:43:42,993 Epoch 16, Average loss: 0.0239, learning rate: 0.0010\n",
      "2018-05-27 16:43:52,518 Iter 340, Minibatch Loss= 0.0234, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:43:54,294 Iter 340, Minibatch Loss= 0.0197, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:43:55,443 Iter 341, Minibatch Loss= 0.0324, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 16:43:56,614 Iter 342, Minibatch Loss= 0.0276, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:43:57,827 Iter 343, Minibatch Loss= 0.0165, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 16:43:58,998 Iter 344, Minibatch Loss= 0.0332, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 16:44:00,132 Iter 345, Minibatch Loss= 0.0339, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 16:44:01,270 Iter 346, Minibatch Loss= 0.0211, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 16:44:02,445 Iter 347, Minibatch Loss= 0.0154, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 16:44:03,564 Iter 348, Minibatch Loss= 0.0294, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:44:04,708 Iter 349, Minibatch Loss= 0.0130, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:44:05,864 Iter 350, Minibatch Loss= 0.0272, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:44:07,030 Iter 351, Minibatch Loss= 0.0202, Training Accuracy= 0.9804, Minibatch error= 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:44:08,183 Iter 352, Minibatch Loss= 0.0178, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 16:44:09,320 Iter 353, Minibatch Loss= 0.0148, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 16:44:10,462 Iter 354, Minibatch Loss= 0.0197, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:44:11,608 Iter 355, Minibatch Loss= 0.0082, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 16:44:12,792 Iter 356, Minibatch Loss= 0.0313, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 16:44:13,943 Iter 357, Minibatch Loss= 0.0182, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:44:15,152 Iter 358, Minibatch Loss= 0.0328, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 16:44:16,284 Iter 359, Minibatch Loss= 0.0167, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 16:44:16,285 Epoch 17, Average loss: 0.0218, learning rate: 0.0010\n",
      "2018-05-27 16:44:25,795 Iter 360, Minibatch Loss= 0.0256, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 16:44:27,581 Iter 360, Minibatch Loss= 0.0112, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 16:44:28,720 Iter 361, Minibatch Loss= 0.0184, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 16:44:29,863 Iter 362, Minibatch Loss= 0.0120, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:44:30,993 Iter 363, Minibatch Loss= 0.0197, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:44:32,146 Iter 364, Minibatch Loss= 0.0322, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:44:33,281 Iter 365, Minibatch Loss= 0.0229, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 16:44:34,432 Iter 366, Minibatch Loss= 0.0093, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 16:44:35,579 Iter 367, Minibatch Loss= 0.0248, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 16:44:36,746 Iter 368, Minibatch Loss= 0.0287, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:44:37,927 Iter 369, Minibatch Loss= 0.0282, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:44:39,065 Iter 370, Minibatch Loss= 0.0224, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 16:44:40,228 Iter 371, Minibatch Loss= 0.0199, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 16:44:41,371 Iter 372, Minibatch Loss= 0.0194, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 16:44:42,533 Iter 373, Minibatch Loss= 0.0204, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:44:43,694 Iter 374, Minibatch Loss= 0.0302, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:44:44,842 Iter 375, Minibatch Loss= 0.0154, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:44:45,979 Iter 376, Minibatch Loss= 0.0347, Training Accuracy= 0.9542, Minibatch error= 4.6%\n",
      "2018-05-27 16:44:47,126 Iter 377, Minibatch Loss= 0.0410, Training Accuracy= 0.9432, Minibatch error= 5.7%\n",
      "2018-05-27 16:44:48,284 Iter 378, Minibatch Loss= 0.0326, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 16:44:49,444 Iter 379, Minibatch Loss= 0.0319, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:44:49,445 Epoch 18, Average loss: 0.0222, learning rate: 0.0010\n",
      "2018-05-27 16:44:58,964 Iter 380, Minibatch Loss= 0.0238, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:00,776 Iter 380, Minibatch Loss= 0.0163, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 16:45:01,934 Iter 381, Minibatch Loss= 0.0272, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 16:45:03,093 Iter 382, Minibatch Loss= 0.0292, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:45:04,235 Iter 383, Minibatch Loss= 0.0202, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:45:05,387 Iter 384, Minibatch Loss= 0.0386, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:45:06,543 Iter 385, Minibatch Loss= 0.0637, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 16:45:07,701 Iter 386, Minibatch Loss= 0.0322, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 16:45:08,846 Iter 387, Minibatch Loss= 0.0077, Training Accuracy= 0.9987, Minibatch error= 0.1%\n",
      "2018-05-27 16:45:09,989 Iter 388, Minibatch Loss= 0.0248, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:11,139 Iter 389, Minibatch Loss= 0.0234, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 16:45:12,282 Iter 390, Minibatch Loss= 0.0164, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 16:45:13,430 Iter 391, Minibatch Loss= 0.0172, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 16:45:14,585 Iter 392, Minibatch Loss= 0.0175, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 16:45:15,743 Iter 393, Minibatch Loss= 0.0184, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:45:16,892 Iter 394, Minibatch Loss= 0.0263, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:18,082 Iter 395, Minibatch Loss= 0.0295, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:45:19,244 Iter 396, Minibatch Loss= 0.0291, Training Accuracy= 0.9585, Minibatch error= 4.2%\n",
      "2018-05-27 16:45:20,387 Iter 397, Minibatch Loss= 0.0164, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 16:45:21,554 Iter 398, Minibatch Loss= 0.0316, Training Accuracy= 0.9416, Minibatch error= 5.8%\n",
      "2018-05-27 16:45:22,718 Iter 399, Minibatch Loss= 0.0325, Training Accuracy= 0.9451, Minibatch error= 5.5%\n",
      "2018-05-27 16:45:22,719 Epoch 19, Average loss: 0.0231, learning rate: 0.0010\n",
      "2018-05-27 16:45:32,194 Iter 400, Minibatch Loss= 0.0245, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:45:34,040 Iter 400, Minibatch Loss= 0.0316, Training Accuracy= 0.9506, Minibatch error= 4.9%\n",
      "2018-05-27 16:45:35,209 Iter 401, Minibatch Loss= 0.0176, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 16:45:36,365 Iter 402, Minibatch Loss= 0.0246, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:37,554 Iter 403, Minibatch Loss= 0.0117, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 16:45:38,748 Iter 404, Minibatch Loss= 0.0273, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 16:45:39,935 Iter 405, Minibatch Loss= 0.0258, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 16:45:41,093 Iter 406, Minibatch Loss= 0.0218, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:45:42,257 Iter 407, Minibatch Loss= 0.0291, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:45:43,413 Iter 408, Minibatch Loss= 0.0245, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:45:44,600 Iter 409, Minibatch Loss= 0.0095, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 16:45:45,762 Iter 410, Minibatch Loss= 0.0178, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:45:46,932 Iter 411, Minibatch Loss= 0.0305, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:45:48,133 Iter 412, Minibatch Loss= 0.0294, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 16:45:49,310 Iter 413, Minibatch Loss= 0.0129, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 16:45:50,460 Iter 414, Minibatch Loss= 0.0289, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 16:45:51,612 Iter 415, Minibatch Loss= 0.0269, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:52,759 Iter 416, Minibatch Loss= 0.0141, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 16:45:53,932 Iter 417, Minibatch Loss= 0.0126, Training Accuracy= 0.9910, Minibatch error= 0.9%\n",
      "2018-05-27 16:45:55,098 Iter 418, Minibatch Loss= 0.0245, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:45:56,272 Iter 419, Minibatch Loss= 0.0171, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 16:45:56,273 Epoch 20, Average loss: 0.0218, learning rate: 0.0010\n",
      "2018-05-27 16:46:05,874 Iter 420, Minibatch Loss= 0.0223, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:46:07,730 Iter 420, Minibatch Loss= 0.0140, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:46:08,871 Iter 421, Minibatch Loss= 0.0207, Training Accuracy= 0.9761, Minibatch error= 2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:46:10,049 Iter 422, Minibatch Loss= 0.0345, Training Accuracy= 0.9526, Minibatch error= 4.7%\n",
      "2018-05-27 16:46:11,213 Iter 423, Minibatch Loss= 0.0243, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 16:46:12,394 Iter 424, Minibatch Loss= 0.0318, Training Accuracy= 0.9507, Minibatch error= 4.9%\n",
      "2018-05-27 16:46:13,563 Iter 425, Minibatch Loss= 0.0334, Training Accuracy= 0.9454, Minibatch error= 5.5%\n",
      "2018-05-27 16:46:14,759 Iter 426, Minibatch Loss= 0.0326, Training Accuracy= 0.9454, Minibatch error= 5.5%\n",
      "2018-05-27 16:46:15,926 Iter 427, Minibatch Loss= 0.0165, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:46:17,105 Iter 428, Minibatch Loss= 0.0186, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:46:18,280 Iter 429, Minibatch Loss= 0.0293, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 16:46:19,446 Iter 430, Minibatch Loss= 0.0221, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:46:20,652 Iter 431, Minibatch Loss= 0.0169, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 16:46:21,828 Iter 432, Minibatch Loss= 0.0279, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:46:23,036 Iter 433, Minibatch Loss= 0.0313, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:46:24,224 Iter 434, Minibatch Loss= 0.0151, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:46:25,389 Iter 435, Minibatch Loss= 0.0169, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 16:46:26,563 Iter 436, Minibatch Loss= 0.0270, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:46:27,730 Iter 437, Minibatch Loss= 0.0200, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 16:46:28,924 Iter 438, Minibatch Loss= 0.0330, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 16:46:30,122 Iter 439, Minibatch Loss= 0.0290, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 16:46:30,123 Epoch 21, Average loss: 0.0246, learning rate: 0.0010\n",
      "2018-05-27 16:46:39,599 Iter 440, Minibatch Loss= 0.0259, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 16:46:41,590 Iter 440, Minibatch Loss= 0.0318, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 16:46:42,780 Iter 441, Minibatch Loss= 0.0318, Training Accuracy= 0.9535, Minibatch error= 4.7%\n",
      "2018-05-27 16:46:43,992 Iter 442, Minibatch Loss= 0.0316, Training Accuracy= 0.9544, Minibatch error= 4.6%\n",
      "2018-05-27 16:46:45,206 Iter 443, Minibatch Loss= 0.0233, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 16:46:46,404 Iter 444, Minibatch Loss= 0.0369, Training Accuracy= 0.9438, Minibatch error= 5.6%\n",
      "2018-05-27 16:46:47,605 Iter 445, Minibatch Loss= 0.0095, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 16:46:48,804 Iter 446, Minibatch Loss= 0.0379, Training Accuracy= 0.9391, Minibatch error= 6.1%\n",
      "2018-05-27 16:46:50,015 Iter 447, Minibatch Loss= 0.0293, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:46:51,225 Iter 448, Minibatch Loss= 0.0271, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:46:52,409 Iter 449, Minibatch Loss= 0.0136, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:46:53,598 Iter 450, Minibatch Loss= 0.0164, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:46:54,778 Iter 451, Minibatch Loss= 0.0328, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 16:46:55,992 Iter 452, Minibatch Loss= 0.0289, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:46:57,207 Iter 453, Minibatch Loss= 0.0140, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 16:46:58,394 Iter 454, Minibatch Loss= 0.0128, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 16:46:59,608 Iter 455, Minibatch Loss= 0.0370, Training Accuracy= 0.9418, Minibatch error= 5.8%\n",
      "2018-05-27 16:47:00,809 Iter 456, Minibatch Loss= 0.1245, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:47:01,993 Iter 457, Minibatch Loss= 0.0347, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 16:47:03,206 Iter 458, Minibatch Loss= 0.0275, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:47:04,424 Iter 459, Minibatch Loss= 0.0281, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 16:47:04,425 Epoch 22, Average loss: 0.0240, learning rate: 0.0010\n",
      "2018-05-27 16:47:14,005 Iter 460, Minibatch Loss= 0.0220, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 16:47:16,167 Iter 460, Minibatch Loss= 0.0236, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:47:17,382 Iter 461, Minibatch Loss= 0.0311, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 16:47:18,590 Iter 462, Minibatch Loss= 0.0161, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 16:47:19,783 Iter 463, Minibatch Loss= 0.0750, Training Accuracy= 0.9155, Minibatch error= 8.4%\n",
      "2018-05-27 16:47:20,983 Iter 464, Minibatch Loss= 0.0834, Training Accuracy= 0.9533, Minibatch error= 4.7%\n",
      "2018-05-27 16:47:22,215 Iter 465, Minibatch Loss= 0.0237, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 16:47:23,455 Iter 466, Minibatch Loss= 0.0124, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:47:24,681 Iter 467, Minibatch Loss= 0.0186, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 16:47:25,888 Iter 468, Minibatch Loss= 0.0299, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:47:27,093 Iter 469, Minibatch Loss= 0.0173, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 16:47:28,297 Iter 470, Minibatch Loss= 0.0270, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:47:29,502 Iter 471, Minibatch Loss= 0.0256, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:47:30,706 Iter 472, Minibatch Loss= 0.0087, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 16:47:31,935 Iter 473, Minibatch Loss= 0.0258, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:47:33,151 Iter 474, Minibatch Loss= 0.0288, Training Accuracy= 0.9555, Minibatch error= 4.4%\n",
      "2018-05-27 16:47:34,379 Iter 475, Minibatch Loss= 0.0331, Training Accuracy= 0.9390, Minibatch error= 6.1%\n",
      "2018-05-27 16:47:35,635 Iter 476, Minibatch Loss= 0.0301, Training Accuracy= 0.9411, Minibatch error= 5.9%\n",
      "2018-05-27 16:47:36,851 Iter 477, Minibatch Loss= 0.0314, Training Accuracy= 0.9449, Minibatch error= 5.5%\n",
      "2018-05-27 16:47:38,063 Iter 478, Minibatch Loss= 0.0301, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 16:47:39,280 Iter 479, Minibatch Loss= 0.0258, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:47:39,281 Epoch 23, Average loss: 0.0243, learning rate: 0.0010\n",
      "2018-05-27 16:47:48,862 Iter 480, Minibatch Loss= 0.0230, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 16:47:50,886 Iter 480, Minibatch Loss= 0.0283, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 16:47:52,111 Iter 481, Minibatch Loss= 0.0292, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:47:53,312 Iter 482, Minibatch Loss= 0.0172, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 16:47:54,508 Iter 483, Minibatch Loss= 0.0222, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 16:47:55,711 Iter 484, Minibatch Loss= 0.0151, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 16:47:56,934 Iter 485, Minibatch Loss= 0.0116, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 16:47:58,117 Iter 486, Minibatch Loss= 0.0099, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 16:47:59,331 Iter 487, Minibatch Loss= 0.0207, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 16:48:00,564 Iter 488, Minibatch Loss= 0.0265, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:48:01,751 Iter 489, Minibatch Loss= 0.0184, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 16:48:02,916 Iter 490, Minibatch Loss= 0.0237, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 16:48:04,129 Iter 491, Minibatch Loss= 0.0308, Training Accuracy= 0.9564, Minibatch error= 4.4%\n",
      "2018-05-27 16:48:05,367 Iter 492, Minibatch Loss= 0.0306, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 16:48:06,597 Iter 493, Minibatch Loss= 0.0110, Training Accuracy= 0.9932, Minibatch error= 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:48:07,793 Iter 494, Minibatch Loss= 0.0105, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:48:08,989 Iter 495, Minibatch Loss= 0.0200, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:48:10,214 Iter 496, Minibatch Loss= 0.0199, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 16:48:11,441 Iter 497, Minibatch Loss= 0.0254, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 16:48:12,657 Iter 498, Minibatch Loss= 0.0151, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 16:48:13,901 Iter 499, Minibatch Loss= 0.0166, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:48:13,902 Epoch 24, Average loss: 0.0197, learning rate: 0.0010\n",
      "2018-05-27 16:48:23,473 Iter 500, Minibatch Loss= 0.0212, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 16:48:25,406 Iter 500, Minibatch Loss= 0.0203, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:48:26,612 Iter 501, Minibatch Loss= 0.0250, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 16:48:27,806 Iter 502, Minibatch Loss= 0.0255, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:48:28,990 Iter 503, Minibatch Loss= 0.0301, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 16:48:30,189 Iter 504, Minibatch Loss= 0.0318, Training Accuracy= 0.9369, Minibatch error= 6.3%\n",
      "2018-05-27 16:48:31,362 Iter 505, Minibatch Loss= 0.0348, Training Accuracy= 0.9498, Minibatch error= 5.0%\n",
      "2018-05-27 16:48:32,540 Iter 506, Minibatch Loss= 0.0253, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 16:48:33,714 Iter 507, Minibatch Loss= 0.0227, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:48:34,934 Iter 508, Minibatch Loss= 0.0112, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 16:48:36,152 Iter 509, Minibatch Loss= 0.0256, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 16:48:37,351 Iter 510, Minibatch Loss= 0.0264, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:48:38,562 Iter 511, Minibatch Loss= 0.0323, Training Accuracy= 0.9550, Minibatch error= 4.5%\n",
      "2018-05-27 16:48:39,723 Iter 512, Minibatch Loss= 0.0079, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 16:48:40,920 Iter 513, Minibatch Loss= 0.0284, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:48:42,102 Iter 514, Minibatch Loss= 0.0158, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 16:48:43,321 Iter 515, Minibatch Loss= 0.0145, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 16:48:44,501 Iter 516, Minibatch Loss= 0.0164, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 16:48:45,693 Iter 517, Minibatch Loss= 0.0269, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:48:46,902 Iter 518, Minibatch Loss= 0.0268, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:48:48,087 Iter 519, Minibatch Loss= 0.0171, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:48:48,089 Epoch 25, Average loss: 0.0227, learning rate: 0.0010\n",
      "2018-05-27 16:48:57,576 Iter 520, Minibatch Loss= 0.0234, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 16:48:59,626 Iter 520, Minibatch Loss= 0.0131, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 16:49:00,919 Iter 521, Minibatch Loss= 0.0210, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 16:49:02,121 Iter 522, Minibatch Loss= 0.0305, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 16:49:03,342 Iter 523, Minibatch Loss= 0.0241, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:49:04,557 Iter 524, Minibatch Loss= 0.0215, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 16:49:05,797 Iter 525, Minibatch Loss= 0.0217, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 16:49:06,994 Iter 526, Minibatch Loss= 0.0293, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:49:08,214 Iter 527, Minibatch Loss= 0.0125, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:49:09,416 Iter 528, Minibatch Loss= 0.0160, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 16:49:10,645 Iter 529, Minibatch Loss= 0.0299, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:49:11,865 Iter 530, Minibatch Loss= 0.0352, Training Accuracy= 0.9376, Minibatch error= 6.2%\n",
      "2018-05-27 16:49:13,054 Iter 531, Minibatch Loss= 0.0217, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 16:49:14,253 Iter 532, Minibatch Loss= 0.0265, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 16:49:15,469 Iter 533, Minibatch Loss= 0.0286, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 16:49:16,691 Iter 534, Minibatch Loss= 0.0275, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:49:17,900 Iter 535, Minibatch Loss= 0.0259, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:49:19,118 Iter 536, Minibatch Loss= 0.0257, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:49:20,342 Iter 537, Minibatch Loss= 0.0091, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 16:49:21,565 Iter 538, Minibatch Loss= 0.0227, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:49:22,780 Iter 539, Minibatch Loss= 0.0201, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 16:49:22,781 Epoch 26, Average loss: 0.0223, learning rate: 0.0010\n",
      "2018-05-27 16:49:32,332 Iter 540, Minibatch Loss= 0.0213, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 16:49:34,485 Iter 540, Minibatch Loss= 0.0232, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:49:35,763 Iter 541, Minibatch Loss= 0.0286, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 16:49:37,004 Iter 542, Minibatch Loss= 0.0325, Training Accuracy= 0.9388, Minibatch error= 6.1%\n",
      "2018-05-27 16:49:38,247 Iter 543, Minibatch Loss= 0.0340, Training Accuracy= 0.9343, Minibatch error= 6.6%\n",
      "2018-05-27 16:49:39,501 Iter 544, Minibatch Loss= 0.0283, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 16:49:40,718 Iter 545, Minibatch Loss= 0.0178, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 16:49:41,945 Iter 546, Minibatch Loss= 0.0284, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 16:49:43,150 Iter 547, Minibatch Loss= 0.0072, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 16:49:44,378 Iter 548, Minibatch Loss= 0.0287, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 16:49:45,610 Iter 549, Minibatch Loss= 0.0274, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 16:49:46,838 Iter 550, Minibatch Loss= 0.0263, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 16:49:48,071 Iter 551, Minibatch Loss= 0.0219, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 16:49:49,306 Iter 552, Minibatch Loss= 0.0175, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 16:49:50,538 Iter 553, Minibatch Loss= 0.0236, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:49:51,760 Iter 554, Minibatch Loss= 0.0175, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 16:49:52,989 Iter 555, Minibatch Loss= 0.0252, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:49:54,217 Iter 556, Minibatch Loss= 0.0249, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:49:55,453 Iter 557, Minibatch Loss= 0.0254, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:49:56,685 Iter 558, Minibatch Loss= 0.0283, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 16:49:57,909 Iter 559, Minibatch Loss= 0.0118, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:49:57,910 Epoch 27, Average loss: 0.0236, learning rate: 0.0010\n",
      "2018-05-27 16:50:07,478 Iter 560, Minibatch Loss= 0.0211, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 16:50:09,571 Iter 560, Minibatch Loss= 0.0225, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 16:50:10,788 Iter 561, Minibatch Loss= 0.0296, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 16:50:11,987 Iter 562, Minibatch Loss= 0.0257, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 16:50:13,193 Iter 563, Minibatch Loss= 0.0115, Training Accuracy= 0.9918, Minibatch error= 0.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:50:14,390 Iter 564, Minibatch Loss= 0.0289, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 16:50:15,596 Iter 565, Minibatch Loss= 0.0170, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:50:16,817 Iter 566, Minibatch Loss= 0.0076, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 16:50:18,049 Iter 567, Minibatch Loss= 0.0293, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 16:50:19,279 Iter 568, Minibatch Loss= 0.0284, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:50:20,492 Iter 569, Minibatch Loss= 0.0303, Training Accuracy= 0.9549, Minibatch error= 4.5%\n",
      "2018-05-27 16:50:21,663 Iter 570, Minibatch Loss= 0.0171, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 16:50:22,859 Iter 571, Minibatch Loss= 0.0103, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 16:50:24,065 Iter 572, Minibatch Loss= 0.0303, Training Accuracy= 0.9529, Minibatch error= 4.7%\n",
      "2018-05-27 16:50:25,224 Iter 573, Minibatch Loss= 0.0184, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:50:26,386 Iter 574, Minibatch Loss= 0.0097, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 16:50:27,589 Iter 575, Minibatch Loss= 0.0133, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 16:50:28,774 Iter 576, Minibatch Loss= 0.0122, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 16:50:29,970 Iter 577, Minibatch Loss= 0.0209, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:50:31,180 Iter 578, Minibatch Loss= 0.0144, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:50:32,420 Iter 579, Minibatch Loss= 0.0265, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 16:50:32,421 Epoch 28, Average loss: 0.0198, learning rate: 0.0010\n",
      "2018-05-27 16:50:41,940 Iter 580, Minibatch Loss= 0.0207, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 16:50:44,046 Iter 580, Minibatch Loss= 0.0110, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 16:50:45,274 Iter 581, Minibatch Loss= 0.0112, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 16:50:46,478 Iter 582, Minibatch Loss= 0.0094, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 16:50:47,720 Iter 583, Minibatch Loss= 0.0325, Training Accuracy= 0.9517, Minibatch error= 4.8%\n",
      "2018-05-27 16:50:48,969 Iter 584, Minibatch Loss= 0.0302, Training Accuracy= 0.9535, Minibatch error= 4.6%\n",
      "2018-05-27 16:50:50,216 Iter 585, Minibatch Loss= 0.0162, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 16:50:51,469 Iter 586, Minibatch Loss= 0.0273, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:50:52,716 Iter 587, Minibatch Loss= 0.0257, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 16:50:53,965 Iter 588, Minibatch Loss= 0.0214, Training Accuracy= 0.9715, Minibatch error= 2.9%\n",
      "2018-05-27 16:50:55,225 Iter 589, Minibatch Loss= 0.0293, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 16:50:56,462 Iter 590, Minibatch Loss= 0.0183, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 16:50:57,699 Iter 591, Minibatch Loss= 0.0252, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:50:58,906 Iter 592, Minibatch Loss= 0.0180, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 16:51:00,151 Iter 593, Minibatch Loss= 0.0264, Training Accuracy= 0.9685, Minibatch error= 3.1%\n",
      "2018-05-27 16:51:01,362 Iter 594, Minibatch Loss= 0.0280, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 16:51:02,575 Iter 595, Minibatch Loss= 0.0387, Training Accuracy= 0.9466, Minibatch error= 5.3%\n",
      "2018-05-27 16:51:03,777 Iter 596, Minibatch Loss= 0.0425, Training Accuracy= 0.9448, Minibatch error= 5.5%\n",
      "2018-05-27 16:51:05,081 Iter 597, Minibatch Loss= 0.0263, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 16:51:06,284 Iter 598, Minibatch Loss= 0.0297, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 16:51:07,500 Iter 599, Minibatch Loss= 0.0164, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 16:51:07,501 Epoch 29, Average loss: 0.0219, learning rate: 0.0010\n",
      "2018-05-27 16:51:17,010 Iter 600, Minibatch Loss= 0.0208, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 16:51:19,154 Iter 600, Minibatch Loss= 0.0174, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 16:51:20,384 Iter 601, Minibatch Loss= 0.0273, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:51:21,600 Iter 602, Minibatch Loss= 0.0087, Training Accuracy= 0.9945, Minibatch error= 0.5%\n",
      "2018-05-27 16:51:22,822 Iter 603, Minibatch Loss= 0.0204, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 16:51:24,101 Iter 604, Minibatch Loss= 0.0297, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 16:51:25,342 Iter 605, Minibatch Loss= 0.0330, Training Accuracy= 0.9396, Minibatch error= 6.0%\n",
      "2018-05-27 16:51:26,554 Iter 606, Minibatch Loss= 0.0219, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 16:51:27,793 Iter 607, Minibatch Loss= 0.0306, Training Accuracy= 0.9458, Minibatch error= 5.4%\n",
      "2018-05-27 16:51:29,045 Iter 608, Minibatch Loss= 0.0297, Training Accuracy= 0.9542, Minibatch error= 4.6%\n",
      "2018-05-27 16:51:30,249 Iter 609, Minibatch Loss= 0.0174, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 16:51:31,499 Iter 610, Minibatch Loss= 0.0158, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:51:32,718 Iter 611, Minibatch Loss= 0.0078, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 16:51:33,951 Iter 612, Minibatch Loss= 0.0229, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 16:51:35,160 Iter 613, Minibatch Loss= 0.0225, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 16:51:36,395 Iter 614, Minibatch Loss= 0.0162, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 16:51:37,639 Iter 615, Minibatch Loss= 0.0271, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 16:51:38,869 Iter 616, Minibatch Loss= 0.0272, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 16:51:40,052 Iter 617, Minibatch Loss= 0.0160, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 16:51:41,270 Iter 618, Minibatch Loss= 0.0196, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 16:51:42,509 Iter 619, Minibatch Loss= 0.0278, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 16:51:42,510 Epoch 30, Average loss: 0.0206, learning rate: 0.0010\n",
      "2018-05-27 16:51:52,147 Iter 620, Minibatch Loss= 0.0228, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:51:54,420 Iter 620, Minibatch Loss= 0.0252, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 16:51:55,642 Iter 621, Minibatch Loss= 0.0110, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 16:51:56,888 Iter 622, Minibatch Loss= 0.0268, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:51:58,107 Iter 623, Minibatch Loss= 0.0270, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 16:51:59,341 Iter 624, Minibatch Loss= 0.0262, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 16:52:00,563 Iter 625, Minibatch Loss= 0.0241, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:52:01,779 Iter 626, Minibatch Loss= 0.0207, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 16:52:03,026 Iter 627, Minibatch Loss= 0.0254, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 16:52:04,316 Iter 628, Minibatch Loss= 0.0273, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 16:52:05,587 Iter 629, Minibatch Loss= 0.0296, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 16:52:06,776 Iter 630, Minibatch Loss= 0.0286, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 16:52:07,976 Iter 631, Minibatch Loss= 0.0184, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:52:09,187 Iter 632, Minibatch Loss= 0.0279, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 16:52:10,390 Iter 633, Minibatch Loss= 0.0208, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 16:52:11,629 Iter 634, Minibatch Loss= 0.0278, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 16:52:12,875 Iter 635, Minibatch Loss= 0.0257, Training Accuracy= 0.9664, Minibatch error= 3.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:52:14,105 Iter 636, Minibatch Loss= 0.0197, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:52:15,359 Iter 637, Minibatch Loss= 0.0269, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:52:16,634 Iter 638, Minibatch Loss= 0.0298, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 16:52:17,878 Iter 639, Minibatch Loss= 0.0166, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 16:52:17,879 Epoch 31, Average loss: 0.0236, learning rate: 0.0010\n",
      "2018-05-27 16:52:27,416 Iter 640, Minibatch Loss= 0.0220, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 16:52:29,625 Iter 640, Minibatch Loss= 0.0287, Training Accuracy= 0.9595, Minibatch error= 4.0%\n",
      "2018-05-27 16:52:30,873 Iter 641, Minibatch Loss= 0.0180, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 16:52:32,149 Iter 642, Minibatch Loss= 0.0295, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 16:52:33,423 Iter 643, Minibatch Loss= 0.0233, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 16:52:34,716 Iter 644, Minibatch Loss= 0.0259, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:52:35,974 Iter 645, Minibatch Loss= 0.0246, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 16:52:37,234 Iter 646, Minibatch Loss= 0.0147, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:52:38,495 Iter 647, Minibatch Loss= 0.0298, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:52:39,736 Iter 648, Minibatch Loss= 0.0226, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 16:52:40,999 Iter 649, Minibatch Loss= 0.0265, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 16:52:42,274 Iter 650, Minibatch Loss= 0.0357, Training Accuracy= 0.9393, Minibatch error= 6.1%\n",
      "2018-05-27 16:52:43,526 Iter 651, Minibatch Loss= 0.0326, Training Accuracy= 0.9398, Minibatch error= 6.0%\n",
      "2018-05-27 16:52:44,802 Iter 652, Minibatch Loss= 0.0149, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 16:52:46,065 Iter 653, Minibatch Loss= 0.0141, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 16:52:47,325 Iter 654, Minibatch Loss= 0.0097, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 16:52:48,578 Iter 655, Minibatch Loss= 0.0244, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:52:49,848 Iter 656, Minibatch Loss= 0.0307, Training Accuracy= 0.9552, Minibatch error= 4.5%\n",
      "2018-05-27 16:52:51,093 Iter 657, Minibatch Loss= 0.0451, Training Accuracy= 0.9431, Minibatch error= 5.7%\n",
      "2018-05-27 16:52:52,360 Iter 658, Minibatch Loss= 0.0845, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 16:52:53,600 Iter 659, Minibatch Loss= 0.0299, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 16:52:53,601 Epoch 32, Average loss: 0.0233, learning rate: 0.0010\n",
      "2018-05-27 16:53:03,146 Iter 660, Minibatch Loss= 0.0225, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 16:53:05,418 Iter 660, Minibatch Loss= 0.0258, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 16:53:06,663 Iter 661, Minibatch Loss= 0.0196, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 16:53:07,915 Iter 662, Minibatch Loss= 0.0301, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:53:09,174 Iter 663, Minibatch Loss= 0.0285, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:53:10,422 Iter 664, Minibatch Loss= 0.0177, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 16:53:11,682 Iter 665, Minibatch Loss= 0.0107, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 16:53:12,930 Iter 666, Minibatch Loss= 0.0166, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 16:53:14,164 Iter 667, Minibatch Loss= 0.0106, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 16:53:15,431 Iter 668, Minibatch Loss= 0.0266, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 16:53:16,676 Iter 669, Minibatch Loss= 0.0155, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 16:53:17,905 Iter 670, Minibatch Loss= 0.0151, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:53:19,168 Iter 671, Minibatch Loss= 0.0119, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 16:53:20,372 Iter 672, Minibatch Loss= 0.0130, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 16:53:21,591 Iter 673, Minibatch Loss= 0.0099, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 16:53:22,804 Iter 674, Minibatch Loss= 0.0107, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 16:53:24,047 Iter 675, Minibatch Loss= 0.0172, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 16:53:25,237 Iter 676, Minibatch Loss= 0.0155, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 16:53:26,466 Iter 677, Minibatch Loss= 0.0183, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 16:53:27,705 Iter 678, Minibatch Loss= 0.0178, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 16:53:28,942 Iter 679, Minibatch Loss= 0.0267, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:53:28,943 Epoch 33, Average loss: 0.0185, learning rate: 0.0010\n",
      "2018-05-27 16:53:38,453 Iter 680, Minibatch Loss= 0.0202, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 16:53:40,716 Iter 680, Minibatch Loss= 0.0247, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 16:53:42,000 Iter 681, Minibatch Loss= 0.0303, Training Accuracy= 0.9496, Minibatch error= 5.0%\n",
      "2018-05-27 16:53:43,244 Iter 682, Minibatch Loss= 0.0269, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 16:53:44,536 Iter 683, Minibatch Loss= 0.0085, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 16:53:45,799 Iter 684, Minibatch Loss= 0.0123, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 16:53:47,061 Iter 685, Minibatch Loss= 0.0273, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 16:53:48,314 Iter 686, Minibatch Loss= 0.0090, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 16:53:49,572 Iter 687, Minibatch Loss= 0.0110, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 16:53:50,817 Iter 688, Minibatch Loss= 0.0283, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 16:53:52,076 Iter 689, Minibatch Loss= 0.0230, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:53:53,353 Iter 690, Minibatch Loss= 0.0084, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 16:53:54,616 Iter 691, Minibatch Loss= 0.0230, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 16:53:55,868 Iter 692, Minibatch Loss= 0.0171, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 16:53:57,156 Iter 693, Minibatch Loss= 0.0182, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 16:53:58,419 Iter 694, Minibatch Loss= 0.0187, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 16:53:59,670 Iter 695, Minibatch Loss= 0.0197, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 16:54:00,934 Iter 696, Minibatch Loss= 0.0201, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:54:02,229 Iter 697, Minibatch Loss= 0.0258, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:54:03,526 Iter 698, Minibatch Loss= 0.0246, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:54:04,815 Iter 699, Minibatch Loss= 0.0195, Training Accuracy= 0.9755, Minibatch error= 2.4%\n",
      "2018-05-27 16:54:04,816 Epoch 34, Average loss: 0.0195, learning rate: 0.0010\n",
      "2018-05-27 16:54:14,444 Iter 700, Minibatch Loss= 0.0214, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 16:54:16,576 Iter 700, Minibatch Loss= 0.0219, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 16:54:17,808 Iter 701, Minibatch Loss= 0.0286, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 16:54:19,038 Iter 702, Minibatch Loss= 0.0201, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:54:20,248 Iter 703, Minibatch Loss= 0.0067, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 16:54:21,481 Iter 704, Minibatch Loss= 0.0235, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:54:22,724 Iter 705, Minibatch Loss= 0.0162, Training Accuracy= 0.9844, Minibatch error= 1.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:54:23,937 Iter 706, Minibatch Loss= 0.0112, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 16:54:25,193 Iter 707, Minibatch Loss= 0.0187, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 16:54:26,440 Iter 708, Minibatch Loss= 0.0268, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:54:27,649 Iter 709, Minibatch Loss= 0.0132, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 16:54:28,877 Iter 710, Minibatch Loss= 0.0171, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:54:30,128 Iter 711, Minibatch Loss= 0.0319, Training Accuracy= 0.9435, Minibatch error= 5.7%\n",
      "2018-05-27 16:54:31,341 Iter 712, Minibatch Loss= 0.0254, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 16:54:32,572 Iter 713, Minibatch Loss= 0.0310, Training Accuracy= 0.9503, Minibatch error= 5.0%\n",
      "2018-05-27 16:54:33,812 Iter 714, Minibatch Loss= 0.0133, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 16:54:35,050 Iter 715, Minibatch Loss= 0.0259, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:54:36,285 Iter 716, Minibatch Loss= 0.0192, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 16:54:37,509 Iter 717, Minibatch Loss= 0.0208, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 16:54:38,707 Iter 718, Minibatch Loss= 0.0141, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:54:39,959 Iter 719, Minibatch Loss= 0.0272, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:54:39,961 Epoch 35, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 16:54:49,476 Iter 720, Minibatch Loss= 0.0217, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:54:51,685 Iter 720, Minibatch Loss= 0.0255, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 16:54:52,964 Iter 721, Minibatch Loss= 0.0149, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 16:54:54,192 Iter 722, Minibatch Loss= 0.0292, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 16:54:55,440 Iter 723, Minibatch Loss= 0.0166, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 16:54:56,696 Iter 724, Minibatch Loss= 0.0091, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 16:54:57,953 Iter 725, Minibatch Loss= 0.0232, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 16:54:59,239 Iter 726, Minibatch Loss= 0.0231, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 16:55:00,524 Iter 727, Minibatch Loss= 0.0095, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 16:55:01,782 Iter 728, Minibatch Loss= 0.0163, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 16:55:03,054 Iter 729, Minibatch Loss= 0.0262, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 16:55:04,316 Iter 730, Minibatch Loss= 0.0097, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 16:55:05,576 Iter 731, Minibatch Loss= 0.0269, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 16:55:06,844 Iter 732, Minibatch Loss= 0.0288, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 16:55:08,096 Iter 733, Minibatch Loss= 0.0190, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 16:55:09,367 Iter 734, Minibatch Loss= 0.0096, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 16:55:10,634 Iter 735, Minibatch Loss= 0.0194, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 16:55:11,906 Iter 736, Minibatch Loss= 0.0246, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:55:13,159 Iter 737, Minibatch Loss= 0.0098, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 16:55:14,442 Iter 738, Minibatch Loss= 0.0157, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 16:55:15,749 Iter 739, Minibatch Loss= 0.0309, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 16:55:15,750 Epoch 36, Average loss: 0.0195, learning rate: 0.0010\n",
      "2018-05-27 16:55:25,283 Iter 740, Minibatch Loss= 0.0202, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 16:55:27,645 Iter 740, Minibatch Loss= 0.0193, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 16:55:28,934 Iter 741, Minibatch Loss= 0.0159, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 16:55:30,251 Iter 742, Minibatch Loss= 0.0099, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 16:55:31,540 Iter 743, Minibatch Loss= 0.0064, Training Accuracy= 0.9981, Minibatch error= 0.2%\n",
      "2018-05-27 16:55:32,844 Iter 744, Minibatch Loss= 0.0270, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:55:34,095 Iter 745, Minibatch Loss= 0.0110, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 16:55:35,362 Iter 746, Minibatch Loss= 0.0222, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 16:55:36,631 Iter 747, Minibatch Loss= 0.0117, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 16:55:37,900 Iter 748, Minibatch Loss= 0.0209, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:55:39,163 Iter 749, Minibatch Loss= 0.0160, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 16:55:40,447 Iter 750, Minibatch Loss= 0.0269, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 16:55:41,678 Iter 751, Minibatch Loss= 0.0190, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 16:55:42,921 Iter 752, Minibatch Loss= 0.0291, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 16:55:44,173 Iter 753, Minibatch Loss= 0.0262, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 16:55:45,434 Iter 754, Minibatch Loss= 0.0095, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 16:55:46,704 Iter 755, Minibatch Loss= 0.0195, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 16:55:47,972 Iter 756, Minibatch Loss= 0.0245, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 16:55:49,262 Iter 757, Minibatch Loss= 0.0238, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:55:50,518 Iter 758, Minibatch Loss= 0.0238, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 16:55:51,769 Iter 759, Minibatch Loss= 0.0152, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 16:55:51,770 Epoch 37, Average loss: 0.0188, learning rate: 0.0010\n",
      "2018-05-27 16:56:01,165 Iter 760, Minibatch Loss= 0.0211, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 16:56:03,499 Iter 760, Minibatch Loss= 0.0288, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 16:56:04,737 Iter 761, Minibatch Loss= 0.0135, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 16:56:05,994 Iter 762, Minibatch Loss= 0.0195, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 16:56:07,207 Iter 763, Minibatch Loss= 0.0172, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 16:56:08,419 Iter 764, Minibatch Loss= 0.0224, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 16:56:09,664 Iter 765, Minibatch Loss= 0.0183, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 16:56:10,909 Iter 766, Minibatch Loss= 0.0255, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:56:12,146 Iter 767, Minibatch Loss= 0.0156, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:56:13,396 Iter 768, Minibatch Loss= 0.0159, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 16:56:14,641 Iter 769, Minibatch Loss= 0.0186, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 16:56:15,867 Iter 770, Minibatch Loss= 0.0126, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 16:56:17,174 Iter 771, Minibatch Loss= 0.0123, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 16:56:18,430 Iter 772, Minibatch Loss= 0.0135, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:56:19,660 Iter 773, Minibatch Loss= 0.0132, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 16:56:20,887 Iter 774, Minibatch Loss= 0.0215, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:56:22,096 Iter 775, Minibatch Loss= 0.0161, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 16:56:23,356 Iter 776, Minibatch Loss= 0.0165, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 16:56:24,616 Iter 777, Minibatch Loss= 0.0095, Training Accuracy= 0.9924, Minibatch error= 0.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:56:25,878 Iter 778, Minibatch Loss= 0.0133, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 16:56:27,121 Iter 779, Minibatch Loss= 0.0095, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 16:56:27,122 Epoch 38, Average loss: 0.0166, learning rate: 0.0010\n",
      "2018-05-27 16:56:36,501 Iter 780, Minibatch Loss= 0.0195, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:56:39,010 Iter 780, Minibatch Loss= 0.0268, Training Accuracy= 0.9605, Minibatch error= 3.9%\n",
      "2018-05-27 16:56:40,266 Iter 781, Minibatch Loss= 0.0184, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 16:56:41,526 Iter 782, Minibatch Loss= 0.0273, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 16:56:42,775 Iter 783, Minibatch Loss= 0.0264, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 16:56:44,033 Iter 784, Minibatch Loss= 0.0202, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 16:56:45,294 Iter 785, Minibatch Loss= 0.0241, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 16:56:46,585 Iter 786, Minibatch Loss= 0.0257, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 16:56:47,852 Iter 787, Minibatch Loss= 0.0261, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 16:56:49,095 Iter 788, Minibatch Loss= 0.0190, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 16:56:50,359 Iter 789, Minibatch Loss= 0.0104, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 16:56:51,606 Iter 790, Minibatch Loss= 0.0246, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:56:52,862 Iter 791, Minibatch Loss= 0.0237, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 16:56:54,170 Iter 792, Minibatch Loss= 0.0204, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:56:55,422 Iter 793, Minibatch Loss= 0.0103, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 16:56:56,695 Iter 794, Minibatch Loss= 0.0172, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 16:56:57,950 Iter 795, Minibatch Loss= 0.0291, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 16:56:59,222 Iter 796, Minibatch Loss= 0.0292, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 16:57:00,489 Iter 797, Minibatch Loss= 0.0255, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:57:01,716 Iter 798, Minibatch Loss= 0.0208, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 16:57:02,950 Iter 799, Minibatch Loss= 0.0189, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 16:57:02,951 Epoch 39, Average loss: 0.0222, learning rate: 0.0010\n",
      "2018-05-27 16:57:12,319 Iter 800, Minibatch Loss= 0.0198, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 16:57:14,684 Iter 800, Minibatch Loss= 0.0139, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 16:57:15,970 Iter 801, Minibatch Loss= 0.0178, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 16:57:17,230 Iter 802, Minibatch Loss= 0.0258, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 16:57:18,520 Iter 803, Minibatch Loss= 0.0158, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 16:57:19,778 Iter 804, Minibatch Loss= 0.0258, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 16:57:21,085 Iter 805, Minibatch Loss= 0.0257, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 16:57:22,372 Iter 806, Minibatch Loss= 0.0181, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 16:57:23,679 Iter 807, Minibatch Loss= 0.0337, Training Accuracy= 0.9448, Minibatch error= 5.5%\n",
      "2018-05-27 16:57:24,947 Iter 808, Minibatch Loss= 0.0239, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 16:57:26,176 Iter 809, Minibatch Loss= 0.0110, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 16:57:27,421 Iter 810, Minibatch Loss= 0.0144, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:57:28,675 Iter 811, Minibatch Loss= 0.0274, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 16:57:29,947 Iter 812, Minibatch Loss= 0.0255, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 16:57:31,254 Iter 813, Minibatch Loss= 0.0076, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 16:57:32,537 Iter 814, Minibatch Loss= 0.0298, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 16:57:33,828 Iter 815, Minibatch Loss= 0.0124, Training Accuracy= 0.9885, Minibatch error= 1.1%\n",
      "2018-05-27 16:57:35,086 Iter 816, Minibatch Loss= 0.0215, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 16:57:36,315 Iter 817, Minibatch Loss= 0.0255, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 16:57:37,566 Iter 818, Minibatch Loss= 0.0258, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 16:57:38,831 Iter 819, Minibatch Loss= 0.0243, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 16:57:38,833 Epoch 40, Average loss: 0.0213, learning rate: 0.0010\n",
      "2018-05-27 16:57:48,226 Iter 820, Minibatch Loss= 0.0195, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 16:57:50,523 Iter 820, Minibatch Loss= 0.0194, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 16:57:51,802 Iter 821, Minibatch Loss= 0.0242, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 16:57:53,071 Iter 822, Minibatch Loss= 0.0112, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 16:57:54,298 Iter 823, Minibatch Loss= 0.0236, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 16:57:55,559 Iter 824, Minibatch Loss= 0.0130, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 16:57:56,830 Iter 825, Minibatch Loss= 0.0121, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 16:57:58,095 Iter 826, Minibatch Loss= 0.0108, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 16:57:59,369 Iter 827, Minibatch Loss= 0.0189, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 16:58:00,606 Iter 828, Minibatch Loss= 0.0193, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:58:01,893 Iter 829, Minibatch Loss= 0.0237, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 16:58:03,167 Iter 830, Minibatch Loss= 0.0213, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 16:58:04,394 Iter 831, Minibatch Loss= 0.0086, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 16:58:05,672 Iter 832, Minibatch Loss= 0.0253, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 16:58:06,913 Iter 833, Minibatch Loss= 0.0235, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 16:58:08,180 Iter 834, Minibatch Loss= 0.0245, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 16:58:09,428 Iter 835, Minibatch Loss= 0.0166, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 16:58:10,636 Iter 836, Minibatch Loss= 0.0079, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 16:58:11,881 Iter 837, Minibatch Loss= 0.0242, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:58:13,125 Iter 838, Minibatch Loss= 0.0130, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 16:58:14,385 Iter 839, Minibatch Loss= 0.0245, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 16:58:14,386 Epoch 41, Average loss: 0.0186, learning rate: 0.0010\n",
      "2018-05-27 16:58:23,836 Iter 840, Minibatch Loss= 0.0201, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 16:58:26,193 Iter 840, Minibatch Loss= 0.0276, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 16:58:27,461 Iter 841, Minibatch Loss= 0.0291, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 16:58:28,728 Iter 842, Minibatch Loss= 0.0276, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 16:58:30,023 Iter 843, Minibatch Loss= 0.0199, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 16:58:31,299 Iter 844, Minibatch Loss= 0.0144, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 16:58:32,595 Iter 845, Minibatch Loss= 0.0236, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 16:58:33,830 Iter 846, Minibatch Loss= 0.0133, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 16:58:35,103 Iter 847, Minibatch Loss= 0.0243, Training Accuracy= 0.9692, Minibatch error= 3.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 16:58:36,343 Iter 848, Minibatch Loss= 0.0170, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 16:58:37,601 Iter 849, Minibatch Loss= 0.0287, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 16:58:38,840 Iter 850, Minibatch Loss= 0.0103, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 16:58:40,105 Iter 851, Minibatch Loss= 0.0315, Training Accuracy= 0.9493, Minibatch error= 5.1%\n",
      "2018-05-27 16:58:41,363 Iter 852, Minibatch Loss= 0.0096, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 16:58:42,624 Iter 853, Minibatch Loss= 0.0256, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 16:58:43,894 Iter 854, Minibatch Loss= 0.0280, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 16:58:45,150 Iter 855, Minibatch Loss= 0.0174, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 16:58:46,398 Iter 856, Minibatch Loss= 0.0244, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 16:58:47,672 Iter 857, Minibatch Loss= 0.0188, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 16:58:48,940 Iter 858, Minibatch Loss= 0.0236, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 16:58:50,201 Iter 859, Minibatch Loss= 0.0241, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 16:58:50,202 Epoch 42, Average loss: 0.0218, learning rate: 0.0010\n",
      "2018-05-27 16:58:59,706 Iter 860, Minibatch Loss= 0.0194, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 16:59:01,993 Iter 860, Minibatch Loss= 0.0323, Training Accuracy= 0.9463, Minibatch error= 5.4%\n",
      "2018-05-27 16:59:03,254 Iter 861, Minibatch Loss= 0.0096, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 16:59:04,498 Iter 862, Minibatch Loss= 0.0147, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 16:59:05,755 Iter 863, Minibatch Loss= 0.0190, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 16:59:07,025 Iter 864, Minibatch Loss= 0.0232, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 16:59:08,282 Iter 865, Minibatch Loss= 0.0260, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:59:09,555 Iter 866, Minibatch Loss= 0.0232, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 16:59:10,811 Iter 867, Minibatch Loss= 0.0265, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 16:59:12,104 Iter 868, Minibatch Loss= 0.0216, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 16:59:13,391 Iter 869, Minibatch Loss= 0.0273, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 16:59:14,688 Iter 870, Minibatch Loss= 0.0289, Training Accuracy= 0.9513, Minibatch error= 4.9%\n",
      "2018-05-27 16:59:15,957 Iter 871, Minibatch Loss= 0.0238, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 16:59:17,219 Iter 872, Minibatch Loss= 0.0205, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 16:59:18,512 Iter 873, Minibatch Loss= 0.0178, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 16:59:19,801 Iter 874, Minibatch Loss= 0.0287, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 16:59:21,050 Iter 875, Minibatch Loss= 0.0199, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 16:59:22,326 Iter 876, Minibatch Loss= 0.0074, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 16:59:23,621 Iter 877, Minibatch Loss= 0.0220, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 16:59:24,924 Iter 878, Minibatch Loss= 0.0334, Training Accuracy= 0.9540, Minibatch error= 4.6%\n",
      "2018-05-27 16:59:26,211 Iter 879, Minibatch Loss= 0.0246, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 16:59:26,212 Epoch 43, Average loss: 0.0223, learning rate: 0.0010\n",
      "2018-05-27 16:59:35,736 Iter 880, Minibatch Loss= 0.0199, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 16:59:38,013 Iter 880, Minibatch Loss= 0.0208, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 16:59:39,273 Iter 881, Minibatch Loss= 0.0268, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 16:59:40,547 Iter 882, Minibatch Loss= 0.0309, Training Accuracy= 0.9541, Minibatch error= 4.6%\n",
      "2018-05-27 16:59:41,838 Iter 883, Minibatch Loss= 0.0235, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 16:59:43,129 Iter 884, Minibatch Loss= 0.0239, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 16:59:44,468 Iter 885, Minibatch Loss= 0.0092, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 16:59:45,757 Iter 886, Minibatch Loss= 0.0245, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 16:59:47,053 Iter 887, Minibatch Loss= 0.0163, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 16:59:48,386 Iter 888, Minibatch Loss= 0.0129, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 16:59:49,641 Iter 889, Minibatch Loss= 0.0280, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 16:59:50,875 Iter 890, Minibatch Loss= 0.0261, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 16:59:52,155 Iter 891, Minibatch Loss= 0.0224, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 16:59:53,463 Iter 892, Minibatch Loss= 0.0184, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 16:59:54,730 Iter 893, Minibatch Loss= 0.0110, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 16:59:56,009 Iter 894, Minibatch Loss= 0.0074, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 16:59:57,274 Iter 895, Minibatch Loss= 0.0296, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 16:59:58,541 Iter 896, Minibatch Loss= 0.0231, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 16:59:59,826 Iter 897, Minibatch Loss= 0.0258, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 17:00:01,086 Iter 898, Minibatch Loss= 0.0191, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:00:02,343 Iter 899, Minibatch Loss= 0.0076, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 17:00:02,345 Epoch 44, Average loss: 0.0204, learning rate: 0.0010\n",
      "2018-05-27 17:00:11,885 Iter 900, Minibatch Loss= 0.0194, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 17:00:14,201 Iter 900, Minibatch Loss= 0.0053, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 17:00:15,447 Iter 901, Minibatch Loss= 0.0455, Training Accuracy= 0.9365, Minibatch error= 6.4%\n",
      "2018-05-27 17:00:16,667 Iter 902, Minibatch Loss= 0.0118, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:00:17,934 Iter 903, Minibatch Loss= 0.0251, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 17:00:19,220 Iter 904, Minibatch Loss= 0.0222, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:00:20,494 Iter 905, Minibatch Loss= 0.0243, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:00:21,737 Iter 906, Minibatch Loss= 0.0304, Training Accuracy= 0.9480, Minibatch error= 5.2%\n",
      "2018-05-27 17:00:23,007 Iter 907, Minibatch Loss= 0.0257, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 17:00:24,258 Iter 908, Minibatch Loss= 0.0249, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 17:00:25,528 Iter 909, Minibatch Loss= 0.0234, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:00:26,767 Iter 910, Minibatch Loss= 0.0196, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 17:00:28,029 Iter 911, Minibatch Loss= 0.0153, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:00:29,284 Iter 912, Minibatch Loss= 0.0291, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 17:00:30,566 Iter 913, Minibatch Loss= 0.0248, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:00:31,807 Iter 914, Minibatch Loss= 0.0061, Training Accuracy= 0.9975, Minibatch error= 0.3%\n",
      "2018-05-27 17:00:33,081 Iter 915, Minibatch Loss= 0.0144, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:00:34,339 Iter 916, Minibatch Loss= 0.0236, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 17:00:35,615 Iter 917, Minibatch Loss= 0.0240, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 17:00:36,848 Iter 918, Minibatch Loss= 0.0113, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 17:00:38,105 Iter 919, Minibatch Loss= 0.0249, Training Accuracy= 0.9614, Minibatch error= 3.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:00:38,106 Epoch 45, Average loss: 0.0215, learning rate: 0.0010\n",
      "2018-05-27 17:00:47,540 Iter 920, Minibatch Loss= 0.0305, Training Accuracy= 0.9421, Minibatch error= 5.8%\n",
      "2018-05-27 17:00:49,886 Iter 920, Minibatch Loss= 0.0263, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 17:00:51,180 Iter 921, Minibatch Loss= 0.0209, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 17:00:52,442 Iter 922, Minibatch Loss= 0.0297, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 17:00:53,681 Iter 923, Minibatch Loss= 0.0170, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 17:00:54,944 Iter 924, Minibatch Loss= 0.0176, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 17:00:56,197 Iter 925, Minibatch Loss= 0.0088, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 17:00:57,490 Iter 926, Minibatch Loss= 0.0120, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 17:00:58,747 Iter 927, Minibatch Loss= 0.0287, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:00:59,993 Iter 928, Minibatch Loss= 0.0083, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 17:01:01,255 Iter 929, Minibatch Loss= 0.0291, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:01:02,547 Iter 930, Minibatch Loss= 0.0055, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 17:01:03,818 Iter 931, Minibatch Loss= 0.0212, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:01:05,101 Iter 932, Minibatch Loss= 0.0239, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:01:06,386 Iter 933, Minibatch Loss= 0.0108, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 17:01:07,653 Iter 934, Minibatch Loss= 0.0157, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 17:01:08,928 Iter 935, Minibatch Loss= 0.0277, Training Accuracy= 0.9566, Minibatch error= 4.3%\n",
      "2018-05-27 17:01:10,198 Iter 936, Minibatch Loss= 0.0107, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:01:11,476 Iter 937, Minibatch Loss= 0.0251, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 17:01:12,740 Iter 938, Minibatch Loss= 0.0130, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 17:01:14,005 Iter 939, Minibatch Loss= 0.0203, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:01:14,006 Epoch 46, Average loss: 0.0191, learning rate: 0.0010\n",
      "2018-05-27 17:01:23,494 Iter 940, Minibatch Loss= 0.0192, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:01:25,931 Iter 940, Minibatch Loss= 0.0158, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:01:27,202 Iter 941, Minibatch Loss= 0.0096, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 17:01:28,540 Iter 942, Minibatch Loss= 0.0210, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:01:29,834 Iter 943, Minibatch Loss= 0.0062, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 17:01:31,106 Iter 944, Minibatch Loss= 0.0104, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 17:01:32,359 Iter 945, Minibatch Loss= 0.0157, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:01:33,682 Iter 946, Minibatch Loss= 0.0283, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 17:01:34,947 Iter 947, Minibatch Loss= 0.0233, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:01:36,222 Iter 948, Minibatch Loss= 0.0100, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 17:01:37,501 Iter 949, Minibatch Loss= 0.0247, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 17:01:38,811 Iter 950, Minibatch Loss= 0.0265, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:01:40,062 Iter 951, Minibatch Loss= 0.0149, Training Accuracy= 0.9795, Minibatch error= 2.1%\n",
      "2018-05-27 17:01:41,314 Iter 952, Minibatch Loss= 0.0181, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:01:42,588 Iter 953, Minibatch Loss= 0.0175, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 17:01:43,910 Iter 954, Minibatch Loss= 0.0288, Training Accuracy= 0.9485, Minibatch error= 5.2%\n",
      "2018-05-27 17:01:45,222 Iter 955, Minibatch Loss= 0.0291, Training Accuracy= 0.9555, Minibatch error= 4.5%\n",
      "2018-05-27 17:01:46,504 Iter 956, Minibatch Loss= 0.0081, Training Accuracy= 0.9945, Minibatch error= 0.5%\n",
      "2018-05-27 17:01:47,786 Iter 957, Minibatch Loss= 0.0129, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 17:01:49,100 Iter 958, Minibatch Loss= 0.0287, Training Accuracy= 0.9655, Minibatch error= 3.4%\n",
      "2018-05-27 17:01:50,398 Iter 959, Minibatch Loss= 0.0265, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 17:01:50,399 Epoch 47, Average loss: 0.0192, learning rate: 0.0010\n",
      "2018-05-27 17:01:59,900 Iter 960, Minibatch Loss= 0.0204, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 17:02:02,503 Iter 960, Minibatch Loss= 0.0259, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:02:03,800 Iter 961, Minibatch Loss= 0.0231, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:02:05,102 Iter 962, Minibatch Loss= 0.0250, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:02:06,390 Iter 963, Minibatch Loss= 0.0153, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 17:02:07,701 Iter 964, Minibatch Loss= 0.0235, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 17:02:09,016 Iter 965, Minibatch Loss= 0.0268, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 17:02:10,304 Iter 966, Minibatch Loss= 0.0128, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:02:11,638 Iter 967, Minibatch Loss= 0.0244, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:02:12,919 Iter 968, Minibatch Loss= 0.0261, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 17:02:14,220 Iter 969, Minibatch Loss= 0.0199, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:02:15,516 Iter 970, Minibatch Loss= 0.0261, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 17:02:16,818 Iter 971, Minibatch Loss= 0.0199, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 17:02:18,090 Iter 972, Minibatch Loss= 0.0240, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:02:19,405 Iter 973, Minibatch Loss= 0.0138, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 17:02:20,666 Iter 974, Minibatch Loss= 0.0142, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 17:02:21,946 Iter 975, Minibatch Loss= 0.0138, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 17:02:23,237 Iter 976, Minibatch Loss= 0.0235, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 17:02:24,542 Iter 977, Minibatch Loss= 0.0141, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 17:02:25,812 Iter 978, Minibatch Loss= 0.0079, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 17:02:27,073 Iter 979, Minibatch Loss= 0.0061, Training Accuracy= 0.9975, Minibatch error= 0.2%\n",
      "2018-05-27 17:02:27,074 Epoch 48, Average loss: 0.0194, learning rate: 0.0010\n",
      "2018-05-27 17:02:36,586 Iter 980, Minibatch Loss= 0.0197, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:02:39,073 Iter 980, Minibatch Loss= 0.0076, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:02:40,358 Iter 981, Minibatch Loss= 0.0104, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 17:02:41,690 Iter 982, Minibatch Loss= 0.0254, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:02:42,992 Iter 983, Minibatch Loss= 0.0249, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:02:44,284 Iter 984, Minibatch Loss= 0.0111, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 17:02:45,578 Iter 985, Minibatch Loss= 0.0132, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:02:46,851 Iter 986, Minibatch Loss= 0.0085, Training Accuracy= 0.9943, Minibatch error= 0.6%\n",
      "2018-05-27 17:02:48,157 Iter 987, Minibatch Loss= 0.0146, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:02:49,473 Iter 988, Minibatch Loss= 0.0173, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 17:02:50,757 Iter 989, Minibatch Loss= 0.0088, Training Accuracy= 0.9932, Minibatch error= 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:02:52,074 Iter 990, Minibatch Loss= 0.0248, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 17:02:53,382 Iter 991, Minibatch Loss= 0.0101, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 17:02:54,684 Iter 992, Minibatch Loss= 0.0160, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:02:55,976 Iter 993, Minibatch Loss= 0.0225, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:02:57,277 Iter 994, Minibatch Loss= 0.0123, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 17:02:58,576 Iter 995, Minibatch Loss= 0.0151, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:02:59,897 Iter 996, Minibatch Loss= 0.0101, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 17:03:01,186 Iter 997, Minibatch Loss= 0.0239, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:03:02,491 Iter 998, Minibatch Loss= 0.0226, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:03:03,792 Iter 999, Minibatch Loss= 0.0229, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:03:03,793 Epoch 49, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 17:03:13,331 Iter 1000, Minibatch Loss= 0.0189, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 17:03:15,820 Iter 1000, Minibatch Loss= 0.0288, Training Accuracy= 0.9552, Minibatch error= 4.5%\n",
      "2018-05-27 17:03:17,129 Iter 1001, Minibatch Loss= 0.0208, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:03:18,405 Iter 1002, Minibatch Loss= 0.0205, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:03:19,697 Iter 1003, Minibatch Loss= 0.0112, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 17:03:21,032 Iter 1004, Minibatch Loss= 0.0271, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 17:03:22,330 Iter 1005, Minibatch Loss= 0.0268, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 17:03:23,666 Iter 1006, Minibatch Loss= 0.0234, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:03:24,934 Iter 1007, Minibatch Loss= 0.0083, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 17:03:26,281 Iter 1008, Minibatch Loss= 0.0230, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 17:03:27,574 Iter 1009, Minibatch Loss= 0.0232, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:03:28,909 Iter 1010, Minibatch Loss= 0.0113, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 17:03:30,243 Iter 1011, Minibatch Loss= 0.0171, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:03:31,549 Iter 1012, Minibatch Loss= 0.0157, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 17:03:32,831 Iter 1013, Minibatch Loss= 0.0116, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 17:03:34,130 Iter 1014, Minibatch Loss= 0.0127, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:03:35,470 Iter 1015, Minibatch Loss= 0.0229, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:03:36,753 Iter 1016, Minibatch Loss= 0.0225, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 17:03:38,049 Iter 1017, Minibatch Loss= 0.0154, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:03:39,330 Iter 1018, Minibatch Loss= 0.0083, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:03:40,617 Iter 1019, Minibatch Loss= 0.0183, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:03:40,619 Epoch 50, Average loss: 0.0182, learning rate: 0.0010\n",
      "2018-05-27 17:03:50,156 Iter 1020, Minibatch Loss= 0.0184, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:03:52,636 Iter 1020, Minibatch Loss= 0.0119, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 17:03:53,957 Iter 1021, Minibatch Loss= 0.0224, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:03:55,254 Iter 1022, Minibatch Loss= 0.0228, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:03:56,562 Iter 1023, Minibatch Loss= 0.0254, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:03:57,875 Iter 1024, Minibatch Loss= 0.0116, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:03:59,209 Iter 1025, Minibatch Loss= 0.0260, Training Accuracy= 0.9420, Minibatch error= 5.8%\n",
      "2018-05-27 17:04:00,523 Iter 1026, Minibatch Loss= 0.0300, Training Accuracy= 0.9476, Minibatch error= 5.2%\n",
      "2018-05-27 17:04:01,814 Iter 1027, Minibatch Loss= 0.0234, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:04:03,121 Iter 1028, Minibatch Loss= 0.0252, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 17:04:04,458 Iter 1029, Minibatch Loss= 0.0188, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 17:04:05,777 Iter 1030, Minibatch Loss= 0.0267, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:04:07,096 Iter 1031, Minibatch Loss= 0.0249, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 17:04:08,404 Iter 1032, Minibatch Loss= 0.0224, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 17:04:09,714 Iter 1033, Minibatch Loss= 0.0100, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:04:11,002 Iter 1034, Minibatch Loss= 0.0185, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:04:12,332 Iter 1035, Minibatch Loss= 0.0125, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:04:13,663 Iter 1036, Minibatch Loss= 0.0114, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:04:14,994 Iter 1037, Minibatch Loss= 0.0120, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 17:04:16,305 Iter 1038, Minibatch Loss= 0.0104, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:04:17,639 Iter 1039, Minibatch Loss= 0.0143, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:04:17,640 Epoch 51, Average loss: 0.0194, learning rate: 0.0010\n",
      "2018-05-27 17:04:27,182 Iter 1040, Minibatch Loss= 0.0185, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:04:29,646 Iter 1040, Minibatch Loss= 0.0160, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:04:30,959 Iter 1041, Minibatch Loss= 0.0101, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:04:32,293 Iter 1042, Minibatch Loss= 0.0252, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:04:33,614 Iter 1043, Minibatch Loss= 0.0241, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:04:34,922 Iter 1044, Minibatch Loss= 0.0138, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:04:36,239 Iter 1045, Minibatch Loss= 0.0241, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:04:37,557 Iter 1046, Minibatch Loss= 0.0098, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 17:04:38,844 Iter 1047, Minibatch Loss= 0.0088, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:04:40,181 Iter 1048, Minibatch Loss= 0.0136, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 17:04:41,515 Iter 1049, Minibatch Loss= 0.0197, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:04:42,843 Iter 1050, Minibatch Loss= 0.0247, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 17:04:44,137 Iter 1051, Minibatch Loss= 0.0226, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:04:45,461 Iter 1052, Minibatch Loss= 0.0053, Training Accuracy= 0.9987, Minibatch error= 0.1%\n",
      "2018-05-27 17:04:46,785 Iter 1053, Minibatch Loss= 0.0245, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:04:48,107 Iter 1054, Minibatch Loss= 0.0247, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 17:04:49,397 Iter 1055, Minibatch Loss= 0.0157, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 17:04:50,683 Iter 1056, Minibatch Loss= 0.0072, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 17:04:52,016 Iter 1057, Minibatch Loss= 0.0153, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 17:04:53,320 Iter 1058, Minibatch Loss= 0.0230, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:04:54,655 Iter 1059, Minibatch Loss= 0.0214, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:04:54,656 Epoch 52, Average loss: 0.0177, learning rate: 0.0010\n",
      "2018-05-27 17:05:04,232 Iter 1060, Minibatch Loss= 0.0188, Training Accuracy= 0.9749, Minibatch error= 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:05:06,748 Iter 1060, Minibatch Loss= 0.0226, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:05:08,042 Iter 1061, Minibatch Loss= 0.0202, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:05:09,348 Iter 1062, Minibatch Loss= 0.0260, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 17:05:10,670 Iter 1063, Minibatch Loss= 0.0295, Training Accuracy= 0.9533, Minibatch error= 4.7%\n",
      "2018-05-27 17:05:11,972 Iter 1064, Minibatch Loss= 0.0247, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:05:13,273 Iter 1065, Minibatch Loss= 0.0203, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:05:14,577 Iter 1066, Minibatch Loss= 0.0152, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 17:05:15,918 Iter 1067, Minibatch Loss= 0.0237, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:05:17,216 Iter 1068, Minibatch Loss= 0.0200, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:05:18,522 Iter 1069, Minibatch Loss= 0.0195, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 17:05:19,848 Iter 1070, Minibatch Loss= 0.0201, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:05:21,157 Iter 1071, Minibatch Loss= 0.0229, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:05:22,478 Iter 1072, Minibatch Loss= 0.0198, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 17:05:23,805 Iter 1073, Minibatch Loss= 0.0231, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 17:05:25,135 Iter 1074, Minibatch Loss= 0.0242, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 17:05:26,470 Iter 1075, Minibatch Loss= 0.0168, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 17:05:27,786 Iter 1076, Minibatch Loss= 0.0105, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 17:05:29,106 Iter 1077, Minibatch Loss= 0.0223, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 17:05:30,420 Iter 1078, Minibatch Loss= 0.0129, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 17:05:31,740 Iter 1079, Minibatch Loss= 0.0229, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:05:31,741 Epoch 53, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 17:05:41,350 Iter 1080, Minibatch Loss= 0.0192, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 17:05:43,861 Iter 1080, Minibatch Loss= 0.0139, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:05:45,180 Iter 1081, Minibatch Loss= 0.0160, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:05:46,494 Iter 1082, Minibatch Loss= 0.0202, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:05:47,825 Iter 1083, Minibatch Loss= 0.0229, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:05:49,151 Iter 1084, Minibatch Loss= 0.0137, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:05:50,448 Iter 1085, Minibatch Loss= 0.0218, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:05:51,776 Iter 1086, Minibatch Loss= 0.0109, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:05:53,113 Iter 1087, Minibatch Loss= 0.0230, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 17:05:54,428 Iter 1088, Minibatch Loss= 0.0055, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 17:05:55,747 Iter 1089, Minibatch Loss= 0.0253, Training Accuracy= 0.9587, Minibatch error= 4.1%\n",
      "2018-05-27 17:05:57,050 Iter 1090, Minibatch Loss= 0.0118, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 17:05:58,403 Iter 1091, Minibatch Loss= 0.0280, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 17:05:59,712 Iter 1092, Minibatch Loss= 0.0136, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:06:01,039 Iter 1093, Minibatch Loss= 0.0243, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:06:02,347 Iter 1094, Minibatch Loss= 0.0249, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:06:03,673 Iter 1095, Minibatch Loss= 0.0198, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 17:06:04,982 Iter 1096, Minibatch Loss= 0.0257, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 17:06:06,327 Iter 1097, Minibatch Loss= 0.0229, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:06:07,618 Iter 1098, Minibatch Loss= 0.0059, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 17:06:08,919 Iter 1099, Minibatch Loss= 0.0144, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 17:06:08,920 Epoch 54, Average loss: 0.0186, learning rate: 0.0010\n",
      "2018-05-27 17:06:18,493 Iter 1100, Minibatch Loss= 0.0199, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:06:21,091 Iter 1100, Minibatch Loss= 0.0071, Training Accuracy= 0.9945, Minibatch error= 0.6%\n",
      "2018-05-27 17:06:22,407 Iter 1101, Minibatch Loss= 0.0231, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 17:06:23,745 Iter 1102, Minibatch Loss= 0.0221, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:06:25,070 Iter 1103, Minibatch Loss= 0.0125, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 17:06:26,416 Iter 1104, Minibatch Loss= 0.0180, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 17:06:27,736 Iter 1105, Minibatch Loss= 0.0082, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 17:06:29,073 Iter 1106, Minibatch Loss= 0.0104, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 17:06:30,403 Iter 1107, Minibatch Loss= 0.0194, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:06:31,756 Iter 1108, Minibatch Loss= 0.0139, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 17:06:33,088 Iter 1109, Minibatch Loss= 0.0232, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:06:34,428 Iter 1110, Minibatch Loss= 0.0192, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 17:06:35,760 Iter 1111, Minibatch Loss= 0.0134, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 17:06:37,119 Iter 1112, Minibatch Loss= 0.0150, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 17:06:38,410 Iter 1113, Minibatch Loss= 0.0146, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:06:39,720 Iter 1114, Minibatch Loss= 0.0152, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 17:06:41,051 Iter 1115, Minibatch Loss= 0.0106, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 17:06:42,389 Iter 1116, Minibatch Loss= 0.0103, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 17:06:43,733 Iter 1117, Minibatch Loss= 0.0232, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:06:45,075 Iter 1118, Minibatch Loss= 0.0229, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:06:46,397 Iter 1119, Minibatch Loss= 0.0219, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 17:06:46,398 Epoch 55, Average loss: 0.0164, learning rate: 0.0010\n",
      "2018-05-27 17:06:55,913 Iter 1120, Minibatch Loss= 0.0183, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:06:58,499 Iter 1120, Minibatch Loss= 0.0230, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 17:06:59,806 Iter 1121, Minibatch Loss= 0.0244, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 17:07:01,115 Iter 1122, Minibatch Loss= 0.0083, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 17:07:02,462 Iter 1123, Minibatch Loss= 0.0136, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:07:03,769 Iter 1124, Minibatch Loss= 0.0254, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:07:05,077 Iter 1125, Minibatch Loss= 0.0226, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:07:06,408 Iter 1126, Minibatch Loss= 0.0234, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:07:07,763 Iter 1127, Minibatch Loss= 0.0230, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 17:07:09,098 Iter 1128, Minibatch Loss= 0.0264, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 17:07:10,444 Iter 1129, Minibatch Loss= 0.0257, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 17:07:11,760 Iter 1130, Minibatch Loss= 0.0243, Training Accuracy= 0.9632, Minibatch error= 3.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:07:13,079 Iter 1131, Minibatch Loss= 0.0152, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:07:14,399 Iter 1132, Minibatch Loss= 0.0090, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 17:07:15,735 Iter 1133, Minibatch Loss= 0.0240, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:07:17,075 Iter 1134, Minibatch Loss= 0.0253, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 17:07:18,397 Iter 1135, Minibatch Loss= 0.0110, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 17:07:19,727 Iter 1136, Minibatch Loss= 0.0237, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:07:21,030 Iter 1137, Minibatch Loss= 0.0156, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:07:22,386 Iter 1138, Minibatch Loss= 0.0256, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:07:23,705 Iter 1139, Minibatch Loss= 0.0230, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:07:23,706 Epoch 56, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 17:07:33,203 Iter 1140, Minibatch Loss= 0.0180, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:07:36,016 Iter 1140, Minibatch Loss= 0.0215, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:07:37,347 Iter 1141, Minibatch Loss= 0.0099, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:07:38,710 Iter 1142, Minibatch Loss= 0.0247, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:07:40,036 Iter 1143, Minibatch Loss= 0.0165, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 17:07:41,383 Iter 1144, Minibatch Loss= 0.0200, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 17:07:42,724 Iter 1145, Minibatch Loss= 0.0196, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:07:44,047 Iter 1146, Minibatch Loss= 0.0244, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:07:45,377 Iter 1147, Minibatch Loss= 0.0253, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:07:46,710 Iter 1148, Minibatch Loss= 0.0248, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 17:07:48,049 Iter 1149, Minibatch Loss= 0.0109, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 17:07:49,351 Iter 1150, Minibatch Loss= 0.0182, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:07:50,676 Iter 1151, Minibatch Loss= 0.0103, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 17:07:52,003 Iter 1152, Minibatch Loss= 0.0257, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 17:07:53,332 Iter 1153, Minibatch Loss= 0.0208, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 17:07:54,643 Iter 1154, Minibatch Loss= 0.0204, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:07:55,972 Iter 1155, Minibatch Loss= 0.0163, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 17:07:57,281 Iter 1156, Minibatch Loss= 0.0053, Training Accuracy= 0.9984, Minibatch error= 0.2%\n",
      "2018-05-27 17:07:58,617 Iter 1157, Minibatch Loss= 0.0241, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:07:59,943 Iter 1158, Minibatch Loss= 0.0161, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 17:08:01,254 Iter 1159, Minibatch Loss= 0.0190, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:08:01,255 Epoch 57, Average loss: 0.0187, learning rate: 0.0010\n",
      "2018-05-27 17:08:10,820 Iter 1160, Minibatch Loss= 0.0183, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:08:13,464 Iter 1160, Minibatch Loss= 0.0103, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:08:14,805 Iter 1161, Minibatch Loss= 0.0163, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 17:08:16,154 Iter 1162, Minibatch Loss= 0.0098, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:08:17,492 Iter 1163, Minibatch Loss= 0.0089, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 17:08:18,821 Iter 1164, Minibatch Loss= 0.0195, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:08:20,154 Iter 1165, Minibatch Loss= 0.0065, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 17:08:21,487 Iter 1166, Minibatch Loss= 0.0162, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:08:22,855 Iter 1167, Minibatch Loss= 0.0242, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 17:08:24,154 Iter 1168, Minibatch Loss= 0.0130, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:08:25,465 Iter 1169, Minibatch Loss= 0.0122, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:08:26,779 Iter 1170, Minibatch Loss= 0.0200, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:08:28,099 Iter 1171, Minibatch Loss= 0.0217, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:08:29,426 Iter 1172, Minibatch Loss= 0.0201, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:08:30,772 Iter 1173, Minibatch Loss= 0.0138, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:08:32,122 Iter 1174, Minibatch Loss= 0.0132, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 17:08:33,450 Iter 1175, Minibatch Loss= 0.0228, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:08:34,793 Iter 1176, Minibatch Loss= 0.0081, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 17:08:36,157 Iter 1177, Minibatch Loss= 0.0268, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 17:08:37,493 Iter 1178, Minibatch Loss= 0.0227, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:08:38,827 Iter 1179, Minibatch Loss= 0.0147, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 17:08:38,828 Epoch 58, Average loss: 0.0162, learning rate: 0.0010\n",
      "2018-05-27 17:08:48,384 Iter 1180, Minibatch Loss= 0.0186, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 17:08:51,011 Iter 1180, Minibatch Loss= 0.0083, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 17:08:52,370 Iter 1181, Minibatch Loss= 0.0115, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:08:53,709 Iter 1182, Minibatch Loss= 0.0201, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:08:55,051 Iter 1183, Minibatch Loss= 0.0243, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:08:56,397 Iter 1184, Minibatch Loss= 0.0233, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:08:57,712 Iter 1185, Minibatch Loss= 0.0190, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:08:59,074 Iter 1186, Minibatch Loss= 0.0253, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 17:09:00,425 Iter 1187, Minibatch Loss= 0.0234, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 17:09:01,775 Iter 1188, Minibatch Loss= 0.0253, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 17:09:03,073 Iter 1189, Minibatch Loss= 0.0108, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:09:04,387 Iter 1190, Minibatch Loss= 0.0162, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 17:09:05,729 Iter 1191, Minibatch Loss= 0.0257, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:09:07,059 Iter 1192, Minibatch Loss= 0.0210, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 17:09:08,381 Iter 1193, Minibatch Loss= 0.0108, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:09:09,751 Iter 1194, Minibatch Loss= 0.0183, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 17:09:11,085 Iter 1195, Minibatch Loss= 0.0154, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 17:09:12,430 Iter 1196, Minibatch Loss= 0.0244, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:09:13,767 Iter 1197, Minibatch Loss= 0.0081, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 17:09:15,117 Iter 1198, Minibatch Loss= 0.0206, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:09:16,471 Iter 1199, Minibatch Loss= 0.0091, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 17:09:16,472 Epoch 59, Average loss: 0.0184, learning rate: 0.0010\n",
      "2018-05-27 17:09:26,036 Iter 1200, Minibatch Loss= 0.0177, Training Accuracy= 0.9767, Minibatch error= 2.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:09:28,724 Iter 1200, Minibatch Loss= 0.0212, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 17:09:30,085 Iter 1201, Minibatch Loss= 0.0225, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:09:31,452 Iter 1202, Minibatch Loss= 0.0112, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:09:32,786 Iter 1203, Minibatch Loss= 0.0088, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 17:09:34,135 Iter 1204, Minibatch Loss= 0.0091, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 17:09:35,503 Iter 1205, Minibatch Loss= 0.0217, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 17:09:36,843 Iter 1206, Minibatch Loss= 0.0066, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 17:09:38,184 Iter 1207, Minibatch Loss= 0.0210, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:09:39,575 Iter 1208, Minibatch Loss= 0.0223, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:09:40,917 Iter 1209, Minibatch Loss= 0.0228, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:09:42,290 Iter 1210, Minibatch Loss= 0.0116, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:09:43,642 Iter 1211, Minibatch Loss= 0.0143, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:09:44,986 Iter 1212, Minibatch Loss= 0.0290, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:09:46,323 Iter 1213, Minibatch Loss= 0.0255, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:09:47,675 Iter 1214, Minibatch Loss= 0.0209, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:09:49,043 Iter 1215, Minibatch Loss= 0.0240, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 17:09:50,392 Iter 1216, Minibatch Loss= 0.0145, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:09:51,716 Iter 1217, Minibatch Loss= 0.0240, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 17:09:53,075 Iter 1218, Minibatch Loss= 0.0259, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 17:09:54,432 Iter 1219, Minibatch Loss= 0.0145, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:09:54,433 Epoch 60, Average loss: 0.0187, learning rate: 0.0010\n",
      "2018-05-27 17:10:03,983 Iter 1220, Minibatch Loss= 0.0179, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:10:06,696 Iter 1220, Minibatch Loss= 0.0217, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:10:08,058 Iter 1221, Minibatch Loss= 0.0221, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:10:09,385 Iter 1222, Minibatch Loss= 0.0180, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:10:10,746 Iter 1223, Minibatch Loss= 0.0235, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:10:12,094 Iter 1224, Minibatch Loss= 0.0242, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 17:10:13,414 Iter 1225, Minibatch Loss= 0.0260, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 17:10:14,752 Iter 1226, Minibatch Loss= 0.0229, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 17:10:16,065 Iter 1227, Minibatch Loss= 0.0143, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 17:10:17,397 Iter 1228, Minibatch Loss= 0.0235, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 17:10:18,741 Iter 1229, Minibatch Loss= 0.0068, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 17:10:20,093 Iter 1230, Minibatch Loss= 0.0200, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 17:10:21,430 Iter 1231, Minibatch Loss= 0.0224, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:10:22,813 Iter 1232, Minibatch Loss= 0.0241, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:10:24,175 Iter 1233, Minibatch Loss= 0.0232, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:10:25,553 Iter 1234, Minibatch Loss= 0.0213, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 17:10:26,893 Iter 1235, Minibatch Loss= 0.0214, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:10:28,232 Iter 1236, Minibatch Loss= 0.0133, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:10:29,599 Iter 1237, Minibatch Loss= 0.0219, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 17:10:30,946 Iter 1238, Minibatch Loss= 0.0274, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 17:10:32,290 Iter 1239, Minibatch Loss= 0.0123, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 17:10:32,291 Epoch 61, Average loss: 0.0204, learning rate: 0.0010\n",
      "2018-05-27 17:10:41,885 Iter 1240, Minibatch Loss= 0.0184, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:10:44,609 Iter 1240, Minibatch Loss= 0.0121, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:10:45,954 Iter 1241, Minibatch Loss= 0.0227, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:10:47,314 Iter 1242, Minibatch Loss= 0.0217, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:10:48,643 Iter 1243, Minibatch Loss= 0.0144, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 17:10:49,992 Iter 1244, Minibatch Loss= 0.0118, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 17:10:51,339 Iter 1245, Minibatch Loss= 0.0138, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:10:52,689 Iter 1246, Minibatch Loss= 0.0179, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 17:10:54,009 Iter 1247, Minibatch Loss= 0.0062, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 17:10:55,367 Iter 1248, Minibatch Loss= 0.0139, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:10:56,684 Iter 1249, Minibatch Loss= 0.0206, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:10:58,040 Iter 1250, Minibatch Loss= 0.0202, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:10:59,358 Iter 1251, Minibatch Loss= 0.0153, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 17:11:00,722 Iter 1252, Minibatch Loss= 0.0056, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 17:11:02,038 Iter 1253, Minibatch Loss= 0.0223, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:11:03,388 Iter 1254, Minibatch Loss= 0.0106, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:11:04,719 Iter 1255, Minibatch Loss= 0.0254, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 17:11:06,099 Iter 1256, Minibatch Loss= 0.0281, Training Accuracy= 0.9486, Minibatch error= 5.1%\n",
      "2018-05-27 17:11:07,412 Iter 1257, Minibatch Loss= 0.0151, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:11:08,784 Iter 1258, Minibatch Loss= 0.0162, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:11:10,120 Iter 1259, Minibatch Loss= 0.0235, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 17:11:10,121 Epoch 62, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 17:11:19,696 Iter 1260, Minibatch Loss= 0.0186, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:11:22,515 Iter 1260, Minibatch Loss= 0.0219, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:11:23,865 Iter 1261, Minibatch Loss= 0.0093, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 17:11:25,218 Iter 1262, Minibatch Loss= 0.0215, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:11:26,576 Iter 1263, Minibatch Loss= 0.0171, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 17:11:27,925 Iter 1264, Minibatch Loss= 0.0236, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:11:29,286 Iter 1265, Minibatch Loss= 0.0161, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 17:11:30,639 Iter 1266, Minibatch Loss= 0.0229, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:11:31,998 Iter 1267, Minibatch Loss= 0.0151, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 17:11:33,346 Iter 1268, Minibatch Loss= 0.0175, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:11:34,705 Iter 1269, Minibatch Loss= 0.0237, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 17:11:36,080 Iter 1270, Minibatch Loss= 0.0227, Training Accuracy= 0.9679, Minibatch error= 3.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:11:37,425 Iter 1271, Minibatch Loss= 0.0234, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:11:38,781 Iter 1272, Minibatch Loss= 0.0122, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:11:40,141 Iter 1273, Minibatch Loss= 0.0249, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:11:41,512 Iter 1274, Minibatch Loss= 0.0232, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:11:42,910 Iter 1275, Minibatch Loss= 0.0234, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:11:44,278 Iter 1276, Minibatch Loss= 0.0178, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 17:11:45,645 Iter 1277, Minibatch Loss= 0.0217, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:11:47,035 Iter 1278, Minibatch Loss= 0.0218, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 17:11:48,385 Iter 1279, Minibatch Loss= 0.0282, Training Accuracy= 0.9526, Minibatch error= 4.7%\n",
      "2018-05-27 17:11:48,385 Epoch 63, Average loss: 0.0206, learning rate: 0.0010\n",
      "2018-05-27 17:11:57,930 Iter 1280, Minibatch Loss= 0.0183, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:12:00,667 Iter 1280, Minibatch Loss= 0.0094, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 17:12:02,034 Iter 1281, Minibatch Loss= 0.0090, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 17:12:03,376 Iter 1282, Minibatch Loss= 0.0125, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:12:04,782 Iter 1283, Minibatch Loss= 0.0185, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:12:06,132 Iter 1284, Minibatch Loss= 0.0083, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 17:12:07,506 Iter 1285, Minibatch Loss= 0.0048, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 17:12:08,870 Iter 1286, Minibatch Loss= 0.0221, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:12:10,244 Iter 1287, Minibatch Loss= 0.0218, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:12:11,646 Iter 1288, Minibatch Loss= 0.0132, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:12:12,998 Iter 1289, Minibatch Loss= 0.0265, Training Accuracy= 0.9497, Minibatch error= 5.0%\n",
      "2018-05-27 17:12:14,342 Iter 1290, Minibatch Loss= 0.0256, Training Accuracy= 0.9516, Minibatch error= 4.8%\n",
      "2018-05-27 17:12:15,690 Iter 1291, Minibatch Loss= 0.0229, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 17:12:17,061 Iter 1292, Minibatch Loss= 0.0246, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 17:12:18,413 Iter 1293, Minibatch Loss= 0.0146, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 17:12:19,767 Iter 1294, Minibatch Loss= 0.0147, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 17:12:21,121 Iter 1295, Minibatch Loss= 0.0247, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:12:22,506 Iter 1296, Minibatch Loss= 0.0047, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 17:12:23,861 Iter 1297, Minibatch Loss= 0.0238, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:12:25,249 Iter 1298, Minibatch Loss= 0.0153, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:12:26,625 Iter 1299, Minibatch Loss= 0.0197, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:12:26,626 Epoch 64, Average loss: 0.0176, learning rate: 0.0010\n",
      "2018-05-27 17:12:36,235 Iter 1300, Minibatch Loss= 0.0178, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 17:12:39,020 Iter 1300, Minibatch Loss= 0.0268, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:12:40,430 Iter 1301, Minibatch Loss= 0.0090, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 17:12:41,794 Iter 1302, Minibatch Loss= 0.0448, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:12:43,170 Iter 1303, Minibatch Loss= 0.0265, Training Accuracy= 0.9553, Minibatch error= 4.5%\n",
      "2018-05-27 17:12:44,519 Iter 1304, Minibatch Loss= 0.0125, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:12:45,889 Iter 1305, Minibatch Loss= 0.0237, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:12:47,272 Iter 1306, Minibatch Loss= 0.0168, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:12:48,636 Iter 1307, Minibatch Loss= 0.0216, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 17:12:50,026 Iter 1308, Minibatch Loss= 0.0336, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:12:51,396 Iter 1309, Minibatch Loss= 0.0257, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 17:12:52,751 Iter 1310, Minibatch Loss= 0.0113, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:12:54,109 Iter 1311, Minibatch Loss= 0.0076, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 17:12:55,474 Iter 1312, Minibatch Loss= 0.0113, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 17:12:56,863 Iter 1313, Minibatch Loss= 0.0126, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:12:58,237 Iter 1314, Minibatch Loss= 0.0229, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:12:59,616 Iter 1315, Minibatch Loss= 0.0197, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:13:01,006 Iter 1316, Minibatch Loss= 0.0135, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 17:13:02,391 Iter 1317, Minibatch Loss= 0.0218, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:13:03,739 Iter 1318, Minibatch Loss= 0.0078, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 17:13:05,092 Iter 1319, Minibatch Loss= 0.0131, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:13:05,093 Epoch 65, Average loss: 0.0178, learning rate: 0.0010\n",
      "2018-05-27 17:13:14,651 Iter 1320, Minibatch Loss= 0.0178, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 17:13:17,696 Iter 1320, Minibatch Loss= 0.0222, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:13:19,082 Iter 1321, Minibatch Loss= 0.0111, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:13:20,468 Iter 1322, Minibatch Loss= 0.0198, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:13:21,852 Iter 1323, Minibatch Loss= 0.0243, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 17:13:23,209 Iter 1324, Minibatch Loss= 0.0214, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:13:24,584 Iter 1325, Minibatch Loss= 0.0233, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 17:13:25,953 Iter 1326, Minibatch Loss= 0.0135, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 17:13:27,348 Iter 1327, Minibatch Loss= 0.0155, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:13:28,734 Iter 1328, Minibatch Loss= 0.0091, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:13:30,121 Iter 1329, Minibatch Loss= 0.0195, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:13:31,497 Iter 1330, Minibatch Loss= 0.0235, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:13:32,868 Iter 1331, Minibatch Loss= 0.0288, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 17:13:34,242 Iter 1332, Minibatch Loss= 0.0210, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 17:13:35,652 Iter 1333, Minibatch Loss= 0.0186, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 17:13:37,008 Iter 1334, Minibatch Loss= 0.0160, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:13:38,375 Iter 1335, Minibatch Loss= 0.0264, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:13:39,742 Iter 1336, Minibatch Loss= 0.0216, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:13:41,088 Iter 1337, Minibatch Loss= 0.0163, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:13:42,477 Iter 1338, Minibatch Loss= 0.0223, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 17:13:43,861 Iter 1339, Minibatch Loss= 0.0146, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:13:43,862 Epoch 66, Average loss: 0.0197, learning rate: 0.0010\n",
      "2018-05-27 17:13:53,421 Iter 1340, Minibatch Loss= 0.0179, Training Accuracy= 0.9792, Minibatch error= 2.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:13:56,263 Iter 1340, Minibatch Loss= 0.0264, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:13:57,654 Iter 1341, Minibatch Loss= 0.0240, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 17:13:59,065 Iter 1342, Minibatch Loss= 0.0224, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:14:00,438 Iter 1343, Minibatch Loss= 0.0111, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:14:01,794 Iter 1344, Minibatch Loss= 0.0213, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:14:03,164 Iter 1345, Minibatch Loss= 0.0183, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 17:14:04,534 Iter 1346, Minibatch Loss= 0.0126, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 17:14:05,882 Iter 1347, Minibatch Loss= 0.0207, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 17:14:07,236 Iter 1348, Minibatch Loss= 0.0131, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:14:08,622 Iter 1349, Minibatch Loss= 0.0084, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 17:14:09,985 Iter 1350, Minibatch Loss= 0.0153, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:14:11,369 Iter 1351, Minibatch Loss= 0.0230, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:14:12,737 Iter 1352, Minibatch Loss= 0.0068, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 17:14:14,138 Iter 1353, Minibatch Loss= 0.0127, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:14:15,499 Iter 1354, Minibatch Loss= 0.0112, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:14:16,878 Iter 1355, Minibatch Loss= 0.0114, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 17:14:18,234 Iter 1356, Minibatch Loss= 0.0116, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 17:14:19,609 Iter 1357, Minibatch Loss= 0.0152, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 17:14:20,953 Iter 1358, Minibatch Loss= 0.0163, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:14:22,306 Iter 1359, Minibatch Loss= 0.0265, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 17:14:22,307 Epoch 67, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 17:14:31,795 Iter 1360, Minibatch Loss= 0.0173, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:14:34,601 Iter 1360, Minibatch Loss= 0.0247, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 17:14:36,002 Iter 1361, Minibatch Loss= 0.0119, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 17:14:37,360 Iter 1362, Minibatch Loss= 0.0082, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 17:14:38,735 Iter 1363, Minibatch Loss= 0.0220, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:14:40,124 Iter 1364, Minibatch Loss= 0.0230, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 17:14:41,488 Iter 1365, Minibatch Loss= 0.0130, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 17:14:42,844 Iter 1366, Minibatch Loss= 0.0119, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 17:14:44,190 Iter 1367, Minibatch Loss= 0.0123, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 17:14:45,566 Iter 1368, Minibatch Loss= 0.0206, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:14:46,951 Iter 1369, Minibatch Loss= 0.0211, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:14:48,356 Iter 1370, Minibatch Loss= 0.0080, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 17:14:49,730 Iter 1371, Minibatch Loss= 0.0210, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:14:51,149 Iter 1372, Minibatch Loss= 0.0086, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 17:14:52,517 Iter 1373, Minibatch Loss= 0.0234, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:14:53,888 Iter 1374, Minibatch Loss= 0.0124, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:14:55,248 Iter 1375, Minibatch Loss= 0.0235, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:14:56,620 Iter 1376, Minibatch Loss= 0.0144, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:14:57,986 Iter 1377, Minibatch Loss= 0.0238, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:14:59,367 Iter 1378, Minibatch Loss= 0.0198, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:15:00,765 Iter 1379, Minibatch Loss= 0.0224, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:15:00,766 Epoch 68, Average loss: 0.0175, learning rate: 0.0010\n",
      "2018-05-27 17:15:10,358 Iter 1380, Minibatch Loss= 0.0179, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:15:13,255 Iter 1380, Minibatch Loss= 0.0243, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 17:15:14,689 Iter 1381, Minibatch Loss= 0.0227, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 17:15:16,078 Iter 1382, Minibatch Loss= 0.0193, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:15:17,503 Iter 1383, Minibatch Loss= 0.0089, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 17:15:18,872 Iter 1384, Minibatch Loss= 0.0206, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 17:15:20,263 Iter 1385, Minibatch Loss= 0.0209, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:15:21,651 Iter 1386, Minibatch Loss= 0.0093, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 17:15:23,037 Iter 1387, Minibatch Loss= 0.0166, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:15:24,395 Iter 1388, Minibatch Loss= 0.0191, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:15:25,764 Iter 1389, Minibatch Loss= 0.0222, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:15:27,194 Iter 1390, Minibatch Loss= 0.0246, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 17:15:28,564 Iter 1391, Minibatch Loss= 0.0240, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:15:29,990 Iter 1392, Minibatch Loss= 0.0147, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:15:31,362 Iter 1393, Minibatch Loss= 0.0124, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:15:32,723 Iter 1394, Minibatch Loss= 0.0089, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 17:15:34,120 Iter 1395, Minibatch Loss= 0.0121, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 17:15:35,506 Iter 1396, Minibatch Loss= 0.0152, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 17:15:36,892 Iter 1397, Minibatch Loss= 0.0229, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:15:38,280 Iter 1398, Minibatch Loss= 0.0207, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 17:15:39,662 Iter 1399, Minibatch Loss= 0.0212, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:15:39,663 Epoch 69, Average loss: 0.0184, learning rate: 0.0010\n",
      "2018-05-27 17:15:49,257 Iter 1400, Minibatch Loss= 0.0182, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 17:15:52,167 Iter 1400, Minibatch Loss= 0.0193, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:15:53,532 Iter 1401, Minibatch Loss= 0.0110, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:15:54,912 Iter 1402, Minibatch Loss= 0.0238, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 17:15:56,319 Iter 1403, Minibatch Loss= 0.0120, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:15:57,729 Iter 1404, Minibatch Loss= 0.0078, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 17:15:59,134 Iter 1405, Minibatch Loss= 0.0229, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:16:00,525 Iter 1406, Minibatch Loss= 0.0156, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:16:01,923 Iter 1407, Minibatch Loss= 0.0248, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:16:03,295 Iter 1408, Minibatch Loss= 0.0225, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:16:04,678 Iter 1409, Minibatch Loss= 0.0135, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:16:06,071 Iter 1410, Minibatch Loss= 0.0064, Training Accuracy= 0.9958, Minibatch error= 0.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:16:07,482 Iter 1411, Minibatch Loss= 0.0201, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:16:08,860 Iter 1412, Minibatch Loss= 0.0230, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 17:16:10,259 Iter 1413, Minibatch Loss= 0.0261, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:16:11,726 Iter 1414, Minibatch Loss= 0.0061, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 17:16:13,108 Iter 1415, Minibatch Loss= 0.0099, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:16:14,520 Iter 1416, Minibatch Loss= 0.0203, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:16:15,914 Iter 1417, Minibatch Loss= 0.0218, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:16:17,318 Iter 1418, Minibatch Loss= 0.0183, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:16:18,693 Iter 1419, Minibatch Loss= 0.0093, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 17:16:18,694 Epoch 70, Average loss: 0.0168, learning rate: 0.0010\n",
      "2018-05-27 17:16:28,272 Iter 1420, Minibatch Loss= 0.0169, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:16:31,215 Iter 1420, Minibatch Loss= 0.0296, Training Accuracy= 0.9549, Minibatch error= 4.5%\n",
      "2018-05-27 17:16:32,608 Iter 1421, Minibatch Loss= 0.0086, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 17:16:33,971 Iter 1422, Minibatch Loss= 0.0220, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:16:35,358 Iter 1423, Minibatch Loss= 0.0095, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:16:36,770 Iter 1424, Minibatch Loss= 0.0137, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 17:16:38,172 Iter 1425, Minibatch Loss= 0.0211, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 17:16:39,555 Iter 1426, Minibatch Loss= 0.0139, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:16:40,952 Iter 1427, Minibatch Loss= 0.0110, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 17:16:42,350 Iter 1428, Minibatch Loss= 0.0213, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 17:16:43,734 Iter 1429, Minibatch Loss= 0.0121, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:16:45,147 Iter 1430, Minibatch Loss= 0.0227, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:16:46,518 Iter 1431, Minibatch Loss= 0.0153, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:16:47,916 Iter 1432, Minibatch Loss= 0.0141, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 17:16:49,295 Iter 1433, Minibatch Loss= 0.0235, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 17:16:50,694 Iter 1434, Minibatch Loss= 0.0063, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 17:16:52,100 Iter 1435, Minibatch Loss= 0.0216, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:16:53,500 Iter 1436, Minibatch Loss= 0.0206, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:16:54,890 Iter 1437, Minibatch Loss= 0.0208, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:16:56,273 Iter 1438, Minibatch Loss= 0.0127, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:16:57,655 Iter 1439, Minibatch Loss= 0.0075, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 17:16:57,656 Epoch 71, Average loss: 0.0165, learning rate: 0.0010\n",
      "2018-05-27 17:17:07,198 Iter 1440, Minibatch Loss= 0.0176, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:17:10,171 Iter 1440, Minibatch Loss= 0.0181, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:17:11,561 Iter 1441, Minibatch Loss= 0.0247, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 17:17:12,963 Iter 1442, Minibatch Loss= 0.0230, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 17:17:14,348 Iter 1443, Minibatch Loss= 0.0198, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:17:15,771 Iter 1444, Minibatch Loss= 0.0165, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:17:17,189 Iter 1445, Minibatch Loss= 0.0205, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 17:17:18,596 Iter 1446, Minibatch Loss= 0.0076, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 17:17:20,006 Iter 1447, Minibatch Loss= 0.0135, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 17:17:21,392 Iter 1448, Minibatch Loss= 0.0226, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:17:22,816 Iter 1449, Minibatch Loss= 0.0137, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:17:24,215 Iter 1450, Minibatch Loss= 0.0230, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:17:25,655 Iter 1451, Minibatch Loss= 0.0130, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:17:27,057 Iter 1452, Minibatch Loss= 0.0103, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 17:17:28,482 Iter 1453, Minibatch Loss= 0.0217, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:17:29,890 Iter 1454, Minibatch Loss= 0.0191, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:17:31,282 Iter 1455, Minibatch Loss= 0.0145, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 17:17:32,705 Iter 1456, Minibatch Loss= 0.0154, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:17:34,128 Iter 1457, Minibatch Loss= 0.0138, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 17:17:35,539 Iter 1458, Minibatch Loss= 0.0235, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:17:36,934 Iter 1459, Minibatch Loss= 0.0217, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:17:36,935 Epoch 72, Average loss: 0.0179, learning rate: 0.0010\n",
      "2018-05-27 17:17:46,482 Iter 1460, Minibatch Loss= 0.0166, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:17:49,396 Iter 1460, Minibatch Loss= 0.0261, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 17:17:50,772 Iter 1461, Minibatch Loss= 0.0075, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 17:17:52,142 Iter 1462, Minibatch Loss= 0.0065, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 17:17:53,528 Iter 1463, Minibatch Loss= 0.0134, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 17:17:54,926 Iter 1464, Minibatch Loss= 0.0172, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 17:17:56,364 Iter 1465, Minibatch Loss= 0.0046, Training Accuracy= 0.9986, Minibatch error= 0.1%\n",
      "2018-05-27 17:17:57,770 Iter 1466, Minibatch Loss= 0.0174, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:17:59,193 Iter 1467, Minibatch Loss= 0.0082, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 17:18:00,597 Iter 1468, Minibatch Loss= 0.0062, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 17:18:01,991 Iter 1469, Minibatch Loss= 0.0124, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:18:03,377 Iter 1470, Minibatch Loss= 0.0114, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:18:04,782 Iter 1471, Minibatch Loss= 0.0103, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 17:18:06,165 Iter 1472, Minibatch Loss= 0.0080, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 17:18:07,566 Iter 1473, Minibatch Loss= 0.0061, Training Accuracy= 0.9965, Minibatch error= 0.4%\n",
      "2018-05-27 17:18:08,953 Iter 1474, Minibatch Loss= 0.0111, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 17:18:10,326 Iter 1475, Minibatch Loss= 0.0116, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 17:18:11,707 Iter 1476, Minibatch Loss= 0.0057, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 17:18:13,157 Iter 1477, Minibatch Loss= 0.0208, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:18:14,550 Iter 1478, Minibatch Loss= 0.0201, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:18:15,979 Iter 1479, Minibatch Loss= 0.0247, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 17:18:15,980 Epoch 73, Average loss: 0.0129, learning rate: 0.0010\n",
      "2018-05-27 17:18:25,531 Iter 1480, Minibatch Loss= 0.0181, Training Accuracy= 0.9716, Minibatch error= 2.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:18:28,513 Iter 1480, Minibatch Loss= 0.0129, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:18:29,949 Iter 1481, Minibatch Loss= 0.0243, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 17:18:31,369 Iter 1482, Minibatch Loss= 0.0101, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:18:32,791 Iter 1483, Minibatch Loss= 0.0238, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:18:34,200 Iter 1484, Minibatch Loss= 0.0109, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:18:35,597 Iter 1485, Minibatch Loss= 0.0122, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:18:37,001 Iter 1486, Minibatch Loss= 0.0146, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:18:38,432 Iter 1487, Minibatch Loss= 0.0105, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 17:18:39,850 Iter 1488, Minibatch Loss= 0.0222, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:18:41,240 Iter 1489, Minibatch Loss= 0.0126, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 17:18:42,649 Iter 1490, Minibatch Loss= 0.0233, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:18:44,086 Iter 1491, Minibatch Loss= 0.0161, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 17:18:45,539 Iter 1492, Minibatch Loss= 0.0197, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:18:46,933 Iter 1493, Minibatch Loss= 0.0119, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:18:48,347 Iter 1494, Minibatch Loss= 0.0143, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 17:18:49,724 Iter 1495, Minibatch Loss= 0.0138, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 17:18:51,133 Iter 1496, Minibatch Loss= 0.0185, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 17:18:52,536 Iter 1497, Minibatch Loss= 0.0189, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:18:53,947 Iter 1498, Minibatch Loss= 0.0246, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:18:55,361 Iter 1499, Minibatch Loss= 0.0188, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:18:55,362 Epoch 74, Average loss: 0.0170, learning rate: 0.0010\n",
      "2018-05-27 17:19:04,909 Iter 1500, Minibatch Loss= 0.0169, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:19:07,969 Iter 1500, Minibatch Loss= 0.0207, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:19:09,401 Iter 1501, Minibatch Loss= 0.0059, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 17:19:10,828 Iter 1502, Minibatch Loss= 0.0222, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 17:19:12,243 Iter 1503, Minibatch Loss= 0.0178, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:19:13,667 Iter 1504, Minibatch Loss= 0.0083, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:19:15,091 Iter 1505, Minibatch Loss= 0.0152, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:19:16,505 Iter 1506, Minibatch Loss= 0.0078, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 17:19:17,908 Iter 1507, Minibatch Loss= 0.0081, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 17:19:19,354 Iter 1508, Minibatch Loss= 0.0205, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 17:19:20,767 Iter 1509, Minibatch Loss= 0.0162, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 17:19:22,188 Iter 1510, Minibatch Loss= 0.0226, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:19:23,627 Iter 1511, Minibatch Loss= 0.0232, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:19:25,026 Iter 1512, Minibatch Loss= 0.0197, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:19:26,425 Iter 1513, Minibatch Loss= 0.0109, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:19:27,817 Iter 1514, Minibatch Loss= 0.0170, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:19:29,252 Iter 1515, Minibatch Loss= 0.0176, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:19:30,658 Iter 1516, Minibatch Loss= 0.0207, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:19:32,112 Iter 1517, Minibatch Loss= 0.0211, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 17:19:33,506 Iter 1518, Minibatch Loss= 0.0233, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:19:34,909 Iter 1519, Minibatch Loss= 0.0262, Training Accuracy= 0.9574, Minibatch error= 4.3%\n",
      "2018-05-27 17:19:34,910 Epoch 75, Average loss: 0.0176, learning rate: 0.0010\n",
      "2018-05-27 17:19:44,489 Iter 1520, Minibatch Loss= 0.0168, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 17:19:47,717 Iter 1520, Minibatch Loss= 0.0187, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:19:49,167 Iter 1521, Minibatch Loss= 0.0094, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:19:50,605 Iter 1522, Minibatch Loss= 0.0220, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:19:52,026 Iter 1523, Minibatch Loss= 0.0170, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:19:53,429 Iter 1524, Minibatch Loss= 0.0070, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 17:19:54,861 Iter 1525, Minibatch Loss= 0.0222, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:19:56,289 Iter 1526, Minibatch Loss= 0.0207, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:19:57,732 Iter 1527, Minibatch Loss= 0.0267, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 17:19:59,194 Iter 1528, Minibatch Loss= 0.0239, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:20:00,597 Iter 1529, Minibatch Loss= 0.0163, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 17:20:02,006 Iter 1530, Minibatch Loss= 0.0223, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:20:03,396 Iter 1531, Minibatch Loss= 0.0208, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:20:04,831 Iter 1532, Minibatch Loss= 0.0243, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 17:20:06,225 Iter 1533, Minibatch Loss= 0.0162, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 17:20:07,684 Iter 1534, Minibatch Loss= 0.0131, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:20:09,117 Iter 1535, Minibatch Loss= 0.0084, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 17:20:10,524 Iter 1536, Minibatch Loss= 0.0208, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:20:11,955 Iter 1537, Minibatch Loss= 0.0200, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:20:13,404 Iter 1538, Minibatch Loss= 0.0228, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:20:14,828 Iter 1539, Minibatch Loss= 0.0148, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 17:20:14,829 Epoch 76, Average loss: 0.0186, learning rate: 0.0010\n",
      "2018-05-27 17:20:24,426 Iter 1540, Minibatch Loss= 0.0169, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:20:27,455 Iter 1540, Minibatch Loss= 0.0227, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 17:20:28,904 Iter 1541, Minibatch Loss= 0.0078, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 17:20:30,333 Iter 1542, Minibatch Loss= 0.0202, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 17:20:31,774 Iter 1543, Minibatch Loss= 0.0178, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:20:33,226 Iter 1544, Minibatch Loss= 0.0229, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:20:34,627 Iter 1545, Minibatch Loss= 0.0180, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 17:20:36,051 Iter 1546, Minibatch Loss= 0.0197, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:20:37,469 Iter 1547, Minibatch Loss= 0.0157, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 17:20:38,904 Iter 1548, Minibatch Loss= 0.0214, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:20:40,323 Iter 1549, Minibatch Loss= 0.0090, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:20:41,738 Iter 1550, Minibatch Loss= 0.0184, Training Accuracy= 0.9747, Minibatch error= 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:20:43,143 Iter 1551, Minibatch Loss= 0.0244, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 17:20:44,551 Iter 1552, Minibatch Loss= 0.0063, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:20:45,951 Iter 1553, Minibatch Loss= 0.0111, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 17:20:47,385 Iter 1554, Minibatch Loss= 0.0087, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:20:48,812 Iter 1555, Minibatch Loss= 0.0199, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:20:50,204 Iter 1556, Minibatch Loss= 0.0185, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:20:51,619 Iter 1557, Minibatch Loss= 0.0087, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 17:20:52,992 Iter 1558, Minibatch Loss= 0.0097, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:20:54,390 Iter 1559, Minibatch Loss= 0.0091, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 17:20:54,391 Epoch 77, Average loss: 0.0156, learning rate: 0.0010\n",
      "2018-05-27 17:21:03,930 Iter 1560, Minibatch Loss= 0.0167, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:21:07,048 Iter 1560, Minibatch Loss= 0.0122, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:21:08,475 Iter 1561, Minibatch Loss= 0.0170, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 17:21:09,896 Iter 1562, Minibatch Loss= 0.0194, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 17:21:11,298 Iter 1563, Minibatch Loss= 0.0263, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 17:21:12,731 Iter 1564, Minibatch Loss= 0.0169, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 17:21:14,139 Iter 1565, Minibatch Loss= 0.0217, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 17:21:15,546 Iter 1566, Minibatch Loss= 0.0120, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:21:16,998 Iter 1567, Minibatch Loss= 0.0196, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 17:21:18,408 Iter 1568, Minibatch Loss= 0.0197, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 17:21:19,874 Iter 1569, Minibatch Loss= 0.0174, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 17:21:21,275 Iter 1570, Minibatch Loss= 0.0102, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 17:21:22,734 Iter 1571, Minibatch Loss= 0.0089, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 17:21:24,170 Iter 1572, Minibatch Loss= 0.0222, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 17:21:25,588 Iter 1573, Minibatch Loss= 0.0218, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:21:27,035 Iter 1574, Minibatch Loss= 0.0140, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 17:21:28,450 Iter 1575, Minibatch Loss= 0.0099, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 17:21:29,863 Iter 1576, Minibatch Loss= 0.0137, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 17:21:31,283 Iter 1577, Minibatch Loss= 0.0185, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 17:21:32,735 Iter 1578, Minibatch Loss= 0.0154, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:21:34,178 Iter 1579, Minibatch Loss= 0.0184, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 17:21:34,179 Epoch 78, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 17:21:43,744 Iter 1580, Minibatch Loss= 0.0168, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 17:21:46,796 Iter 1580, Minibatch Loss= 0.0051, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 17:21:48,217 Iter 1581, Minibatch Loss= 0.0155, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:21:49,650 Iter 1582, Minibatch Loss= 0.0191, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 17:21:51,116 Iter 1583, Minibatch Loss= 0.0129, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:21:52,539 Iter 1584, Minibatch Loss= 0.0137, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:21:53,954 Iter 1585, Minibatch Loss= 0.0180, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:21:55,368 Iter 1586, Minibatch Loss= 0.0196, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:21:56,789 Iter 1587, Minibatch Loss= 0.0121, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 17:21:58,231 Iter 1588, Minibatch Loss= 0.0127, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 17:21:59,650 Iter 1589, Minibatch Loss= 0.0142, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 17:22:01,109 Iter 1590, Minibatch Loss= 0.0121, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 17:22:02,549 Iter 1591, Minibatch Loss= 0.0187, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:22:03,988 Iter 1592, Minibatch Loss= 0.0196, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 17:22:05,442 Iter 1593, Minibatch Loss= 0.0199, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:22:06,856 Iter 1594, Minibatch Loss= 0.0126, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:22:08,277 Iter 1595, Minibatch Loss= 0.0123, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:22:09,699 Iter 1596, Minibatch Loss= 0.0107, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:22:11,159 Iter 1597, Minibatch Loss= 0.0218, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:22:12,614 Iter 1598, Minibatch Loss= 0.0155, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:22:14,046 Iter 1599, Minibatch Loss= 0.0142, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:22:14,047 Epoch 79, Average loss: 0.0153, learning rate: 0.0010\n",
      "2018-05-27 17:22:23,625 Iter 1600, Minibatch Loss= 0.0174, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:22:26,827 Iter 1600, Minibatch Loss= 0.0161, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:22:28,295 Iter 1601, Minibatch Loss= 0.0098, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 17:22:29,741 Iter 1602, Minibatch Loss= 0.0194, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 17:22:31,177 Iter 1603, Minibatch Loss= 0.0205, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:22:32,621 Iter 1604, Minibatch Loss= 0.0199, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:22:34,088 Iter 1605, Minibatch Loss= 0.0229, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 17:22:35,523 Iter 1606, Minibatch Loss= 0.0242, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 17:22:36,990 Iter 1607, Minibatch Loss= 0.0130, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 17:22:38,404 Iter 1608, Minibatch Loss= 0.0142, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:22:39,834 Iter 1609, Minibatch Loss= 0.0128, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:22:41,282 Iter 1610, Minibatch Loss= 0.0220, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:22:42,714 Iter 1611, Minibatch Loss= 0.0218, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:22:44,165 Iter 1612, Minibatch Loss= 0.0112, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:22:45,653 Iter 1613, Minibatch Loss= 0.0112, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:22:47,110 Iter 1614, Minibatch Loss= 0.0242, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:22:48,589 Iter 1615, Minibatch Loss= 0.0207, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:22:50,033 Iter 1616, Minibatch Loss= 0.0134, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:22:51,498 Iter 1617, Minibatch Loss= 0.0214, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:22:52,941 Iter 1618, Minibatch Loss= 0.0238, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 17:22:54,397 Iter 1619, Minibatch Loss= 0.0232, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 17:22:54,398 Epoch 80, Average loss: 0.0187, learning rate: 0.0010\n",
      "2018-05-27 17:23:03,928 Iter 1620, Minibatch Loss= 0.0176, Training Accuracy= 0.9740, Minibatch error= 2.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:23:07,071 Iter 1620, Minibatch Loss= 0.0174, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:23:08,494 Iter 1621, Minibatch Loss= 0.0207, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 17:23:09,958 Iter 1622, Minibatch Loss= 0.0219, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 17:23:11,368 Iter 1623, Minibatch Loss= 0.0191, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 17:23:12,811 Iter 1624, Minibatch Loss= 0.0210, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:23:14,271 Iter 1625, Minibatch Loss= 0.0212, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:23:15,692 Iter 1626, Minibatch Loss= 0.0202, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:23:17,172 Iter 1627, Minibatch Loss= 0.0234, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 17:23:18,604 Iter 1628, Minibatch Loss= 0.0191, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:23:20,047 Iter 1629, Minibatch Loss= 0.0272, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:23:21,501 Iter 1630, Minibatch Loss= 0.0219, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:23:22,959 Iter 1631, Minibatch Loss= 0.0112, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 17:23:24,406 Iter 1632, Minibatch Loss= 0.0114, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:23:25,834 Iter 1633, Minibatch Loss= 0.0124, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:23:27,300 Iter 1634, Minibatch Loss= 0.0231, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:23:28,719 Iter 1635, Minibatch Loss= 0.0176, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:23:30,139 Iter 1636, Minibatch Loss= 0.0129, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 17:23:31,582 Iter 1637, Minibatch Loss= 0.0128, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:23:32,997 Iter 1638, Minibatch Loss= 0.0141, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 17:23:34,449 Iter 1639, Minibatch Loss= 0.0139, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 17:23:34,450 Epoch 81, Average loss: 0.0183, learning rate: 0.0010\n",
      "2018-05-27 17:23:44,059 Iter 1640, Minibatch Loss= 0.0170, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:23:47,246 Iter 1640, Minibatch Loss= 0.0252, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 17:23:48,689 Iter 1641, Minibatch Loss= 0.0191, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:23:50,147 Iter 1642, Minibatch Loss= 0.0090, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:23:51,625 Iter 1643, Minibatch Loss= 0.0280, Training Accuracy= 0.9428, Minibatch error= 5.7%\n",
      "2018-05-27 17:23:53,092 Iter 1644, Minibatch Loss= 0.0230, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 17:23:54,568 Iter 1645, Minibatch Loss= 0.0056, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 17:23:56,040 Iter 1646, Minibatch Loss= 0.0233, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:23:57,516 Iter 1647, Minibatch Loss= 0.0123, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:23:58,965 Iter 1648, Minibatch Loss= 0.0232, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:24:00,425 Iter 1649, Minibatch Loss= 0.0173, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 17:24:01,876 Iter 1650, Minibatch Loss= 0.0106, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 17:24:03,301 Iter 1651, Minibatch Loss= 0.0220, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 17:24:04,787 Iter 1652, Minibatch Loss= 0.0233, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:24:06,253 Iter 1653, Minibatch Loss= 0.0142, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:24:07,726 Iter 1654, Minibatch Loss= 0.0104, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:24:09,198 Iter 1655, Minibatch Loss= 0.0115, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:24:10,654 Iter 1656, Minibatch Loss= 0.0131, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:24:12,118 Iter 1657, Minibatch Loss= 0.0122, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:24:13,555 Iter 1658, Minibatch Loss= 0.0150, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 17:24:15,004 Iter 1659, Minibatch Loss= 0.0199, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:24:15,005 Epoch 82, Average loss: 0.0175, learning rate: 0.0010\n",
      "2018-05-27 17:24:24,556 Iter 1660, Minibatch Loss= 0.0161, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:24:27,752 Iter 1660, Minibatch Loss= 0.0144, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:24:29,224 Iter 1661, Minibatch Loss= 0.0091, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 17:24:30,684 Iter 1662, Minibatch Loss= 0.0211, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:24:32,159 Iter 1663, Minibatch Loss= 0.0210, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:24:33,597 Iter 1664, Minibatch Loss= 0.0182, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 17:24:35,054 Iter 1665, Minibatch Loss= 0.0212, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:24:36,517 Iter 1666, Minibatch Loss= 0.0234, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 17:24:37,986 Iter 1667, Minibatch Loss= 0.0195, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:24:39,464 Iter 1668, Minibatch Loss= 0.0208, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:24:40,889 Iter 1669, Minibatch Loss= 0.0130, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:24:42,351 Iter 1670, Minibatch Loss= 0.0210, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:24:43,783 Iter 1671, Minibatch Loss= 0.0122, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 17:24:45,265 Iter 1672, Minibatch Loss= 0.0211, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 17:24:46,710 Iter 1673, Minibatch Loss= 0.0198, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:24:48,135 Iter 1674, Minibatch Loss= 0.0126, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:24:49,579 Iter 1675, Minibatch Loss= 0.0161, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:24:51,036 Iter 1676, Minibatch Loss= 0.0221, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:24:52,467 Iter 1677, Minibatch Loss= 0.0131, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:24:53,928 Iter 1678, Minibatch Loss= 0.0115, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:24:55,358 Iter 1679, Minibatch Loss= 0.0204, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:24:55,359 Epoch 83, Average loss: 0.0178, learning rate: 0.0010\n",
      "2018-05-27 17:25:04,882 Iter 1680, Minibatch Loss= 0.0162, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 17:25:08,017 Iter 1680, Minibatch Loss= 0.0089, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:25:09,454 Iter 1681, Minibatch Loss= 0.0102, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 17:25:10,913 Iter 1682, Minibatch Loss= 0.0214, Training Accuracy= 0.9655, Minibatch error= 3.4%\n",
      "2018-05-27 17:25:12,367 Iter 1683, Minibatch Loss= 0.0108, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:25:13,799 Iter 1684, Minibatch Loss= 0.0207, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:25:15,279 Iter 1685, Minibatch Loss= 0.0202, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:25:16,753 Iter 1686, Minibatch Loss= 0.0209, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:25:18,192 Iter 1687, Minibatch Loss= 0.0109, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 17:25:19,625 Iter 1688, Minibatch Loss= 0.0231, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:25:21,086 Iter 1689, Minibatch Loss= 0.0070, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 17:25:22,525 Iter 1690, Minibatch Loss= 0.0089, Training Accuracy= 0.9915, Minibatch error= 0.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:25:23,951 Iter 1691, Minibatch Loss= 0.0187, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:25:25,402 Iter 1692, Minibatch Loss= 0.0157, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 17:25:26,827 Iter 1693, Minibatch Loss= 0.0168, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:25:28,242 Iter 1694, Minibatch Loss= 0.0154, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:25:29,700 Iter 1695, Minibatch Loss= 0.0216, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:25:31,191 Iter 1696, Minibatch Loss= 0.0203, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:25:32,644 Iter 1697, Minibatch Loss= 0.0157, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:25:34,079 Iter 1698, Minibatch Loss= 0.0161, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 17:25:35,540 Iter 1699, Minibatch Loss= 0.0148, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 17:25:35,541 Epoch 84, Average loss: 0.0161, learning rate: 0.0010\n",
      "2018-05-27 17:25:45,040 Iter 1700, Minibatch Loss= 0.0173, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 17:25:48,202 Iter 1700, Minibatch Loss= 0.0205, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 17:25:49,659 Iter 1701, Minibatch Loss= 0.0213, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:25:51,108 Iter 1702, Minibatch Loss= 0.0235, Training Accuracy= 0.9635, Minibatch error= 3.6%\n",
      "2018-05-27 17:25:52,580 Iter 1703, Minibatch Loss= 0.0108, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 17:25:54,032 Iter 1704, Minibatch Loss= 0.0193, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 17:25:55,517 Iter 1705, Minibatch Loss= 0.0221, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 17:25:56,973 Iter 1706, Minibatch Loss= 0.0090, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 17:25:58,421 Iter 1707, Minibatch Loss= 0.0235, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:25:59,867 Iter 1708, Minibatch Loss= 0.0134, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:26:01,311 Iter 1709, Minibatch Loss= 0.0191, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 17:26:02,805 Iter 1710, Minibatch Loss= 0.0204, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:26:04,249 Iter 1711, Minibatch Loss= 0.0156, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:26:05,754 Iter 1712, Minibatch Loss= 0.0232, Training Accuracy= 0.9573, Minibatch error= 4.3%\n",
      "2018-05-27 17:26:07,178 Iter 1713, Minibatch Loss= 0.0176, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:26:08,582 Iter 1714, Minibatch Loss= 0.0243, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 17:26:10,050 Iter 1715, Minibatch Loss= 0.0181, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 17:26:11,530 Iter 1716, Minibatch Loss= 0.0131, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 17:26:13,006 Iter 1717, Minibatch Loss= 0.0098, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 17:26:14,452 Iter 1718, Minibatch Loss= 0.0217, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:26:15,946 Iter 1719, Minibatch Loss= 0.0210, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 17:26:15,947 Epoch 85, Average loss: 0.0186, learning rate: 0.0010\n",
      "2018-05-27 17:26:25,524 Iter 1720, Minibatch Loss= 0.0172, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 17:26:29,042 Iter 1720, Minibatch Loss= 0.0096, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:26:30,487 Iter 1721, Minibatch Loss= 0.0108, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 17:26:31,960 Iter 1722, Minibatch Loss= 0.0209, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:26:33,452 Iter 1723, Minibatch Loss= 0.0209, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:26:34,901 Iter 1724, Minibatch Loss= 0.0106, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:26:36,409 Iter 1725, Minibatch Loss= 0.0243, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 17:26:37,877 Iter 1726, Minibatch Loss= 0.0164, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:26:39,330 Iter 1727, Minibatch Loss= 0.0165, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 17:26:40,809 Iter 1728, Minibatch Loss= 0.0151, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 17:26:42,271 Iter 1729, Minibatch Loss= 0.0148, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 17:26:43,768 Iter 1730, Minibatch Loss= 0.0168, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:26:45,225 Iter 1731, Minibatch Loss= 0.0111, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:26:46,709 Iter 1732, Minibatch Loss= 0.0153, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 17:26:48,159 Iter 1733, Minibatch Loss= 0.0133, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:26:49,614 Iter 1734, Minibatch Loss= 0.0269, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:26:51,104 Iter 1735, Minibatch Loss= 0.0136, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:26:52,541 Iter 1736, Minibatch Loss= 0.0063, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 17:26:53,980 Iter 1737, Minibatch Loss= 0.0062, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:26:55,414 Iter 1738, Minibatch Loss= 0.0064, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 17:26:56,873 Iter 1739, Minibatch Loss= 0.0069, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 17:26:56,874 Epoch 86, Average loss: 0.0147, learning rate: 0.0010\n",
      "2018-05-27 17:27:06,399 Iter 1740, Minibatch Loss= 0.0162, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 17:27:09,620 Iter 1740, Minibatch Loss= 0.0099, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 17:27:11,114 Iter 1741, Minibatch Loss= 0.0061, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 17:27:12,584 Iter 1742, Minibatch Loss= 0.0208, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 17:27:14,052 Iter 1743, Minibatch Loss= 0.0048, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 17:27:15,517 Iter 1744, Minibatch Loss= 0.0117, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:27:16,945 Iter 1745, Minibatch Loss= 0.0171, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 17:27:18,427 Iter 1746, Minibatch Loss= 0.0132, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 17:27:19,863 Iter 1747, Minibatch Loss= 0.0107, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 17:27:21,325 Iter 1748, Minibatch Loss= 0.0062, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:27:22,798 Iter 1749, Minibatch Loss= 0.0228, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:27:24,320 Iter 1750, Minibatch Loss= 0.0181, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:27:25,822 Iter 1751, Minibatch Loss= 0.0203, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 17:27:27,289 Iter 1752, Minibatch Loss= 0.0198, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:27:28,747 Iter 1753, Minibatch Loss= 0.0130, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 17:27:30,208 Iter 1754, Minibatch Loss= 0.0212, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:27:31,661 Iter 1755, Minibatch Loss= 0.0162, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 17:27:33,135 Iter 1756, Minibatch Loss= 0.0118, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:27:34,624 Iter 1757, Minibatch Loss= 0.0229, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:27:36,078 Iter 1758, Minibatch Loss= 0.0088, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 17:27:37,550 Iter 1759, Minibatch Loss= 0.0179, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 17:27:37,551 Epoch 87, Average loss: 0.0148, learning rate: 0.0010\n",
      "2018-05-27 17:27:47,073 Iter 1760, Minibatch Loss= 0.0165, Training Accuracy= 0.9783, Minibatch error= 2.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:27:50,325 Iter 1760, Minibatch Loss= 0.0369, Training Accuracy= 0.9423, Minibatch error= 5.8%\n",
      "2018-05-27 17:27:51,802 Iter 1761, Minibatch Loss= 0.0208, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 17:27:53,270 Iter 1762, Minibatch Loss= 0.0167, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 17:27:54,726 Iter 1763, Minibatch Loss= 0.0201, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 17:27:56,176 Iter 1764, Minibatch Loss= 0.0231, Training Accuracy= 0.9695, Minibatch error= 3.1%\n",
      "2018-05-27 17:27:57,664 Iter 1765, Minibatch Loss= 0.0191, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:27:59,150 Iter 1766, Minibatch Loss= 0.0126, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:28:00,583 Iter 1767, Minibatch Loss= 0.0189, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:28:02,028 Iter 1768, Minibatch Loss= 0.0161, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:28:03,503 Iter 1769, Minibatch Loss= 0.0129, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:28:04,978 Iter 1770, Minibatch Loss= 0.0158, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 17:28:06,478 Iter 1771, Minibatch Loss= 0.0064, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 17:28:07,962 Iter 1772, Minibatch Loss= 0.0206, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:28:09,450 Iter 1773, Minibatch Loss= 0.0203, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:28:10,944 Iter 1774, Minibatch Loss= 0.0208, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:28:12,449 Iter 1775, Minibatch Loss= 0.0190, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 17:28:13,950 Iter 1776, Minibatch Loss= 0.0202, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:28:15,409 Iter 1777, Minibatch Loss= 0.0173, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:28:16,884 Iter 1778, Minibatch Loss= 0.0218, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:28:18,309 Iter 1779, Minibatch Loss= 0.0090, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:28:18,310 Epoch 88, Average loss: 0.0189, learning rate: 0.0010\n",
      "2018-05-27 17:28:27,873 Iter 1780, Minibatch Loss= 0.0160, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 17:28:31,144 Iter 1780, Minibatch Loss= 0.0066, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 17:28:32,655 Iter 1781, Minibatch Loss= 0.0212, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:28:34,113 Iter 1782, Minibatch Loss= 0.0194, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:28:35,612 Iter 1783, Minibatch Loss= 0.0140, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 17:28:37,072 Iter 1784, Minibatch Loss= 0.0125, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 17:28:38,558 Iter 1785, Minibatch Loss= 0.0196, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:28:40,052 Iter 1786, Minibatch Loss= 0.0097, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:28:41,535 Iter 1787, Minibatch Loss= 0.0194, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:28:43,030 Iter 1788, Minibatch Loss= 0.0126, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 17:28:44,513 Iter 1789, Minibatch Loss= 0.0200, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 17:28:46,015 Iter 1790, Minibatch Loss= 0.0102, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:28:47,517 Iter 1791, Minibatch Loss= 0.0189, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:28:48,990 Iter 1792, Minibatch Loss= 0.0086, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 17:28:50,536 Iter 1793, Minibatch Loss= 0.0185, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:28:52,084 Iter 1794, Minibatch Loss= 0.0068, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 17:28:53,583 Iter 1795, Minibatch Loss= 0.0220, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 17:28:55,067 Iter 1796, Minibatch Loss= 0.0186, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:28:56,563 Iter 1797, Minibatch Loss= 0.0243, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 17:28:58,087 Iter 1798, Minibatch Loss= 0.0219, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:28:59,552 Iter 1799, Minibatch Loss= 0.0094, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:28:59,553 Epoch 89, Average loss: 0.0157, learning rate: 0.0010\n",
      "2018-05-27 17:29:09,173 Iter 1800, Minibatch Loss= 0.0160, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:29:12,562 Iter 1800, Minibatch Loss= 0.0151, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 17:29:14,090 Iter 1801, Minibatch Loss= 0.0197, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:29:15,599 Iter 1802, Minibatch Loss= 0.0195, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:29:17,059 Iter 1803, Minibatch Loss= 0.0113, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:29:18,537 Iter 1804, Minibatch Loss= 0.0116, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 17:29:20,007 Iter 1805, Minibatch Loss= 0.0087, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 17:29:21,523 Iter 1806, Minibatch Loss= 0.0108, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:29:23,070 Iter 1807, Minibatch Loss= 0.0240, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 17:29:24,560 Iter 1808, Minibatch Loss= 0.0132, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 17:29:26,068 Iter 1809, Minibatch Loss= 0.0076, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 17:29:27,528 Iter 1810, Minibatch Loss= 0.0079, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 17:29:28,990 Iter 1811, Minibatch Loss= 0.0228, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 17:29:30,458 Iter 1812, Minibatch Loss= 0.0187, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 17:29:31,954 Iter 1813, Minibatch Loss= 0.0217, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:29:33,467 Iter 1814, Minibatch Loss= 0.0197, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:29:34,954 Iter 1815, Minibatch Loss= 0.0196, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:29:36,459 Iter 1816, Minibatch Loss= 0.0125, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 17:29:37,935 Iter 1817, Minibatch Loss= 0.0215, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 17:29:39,379 Iter 1818, Minibatch Loss= 0.0178, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 17:29:40,868 Iter 1819, Minibatch Loss= 0.0066, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 17:29:40,869 Epoch 90, Average loss: 0.0157, learning rate: 0.0010\n",
      "2018-05-27 17:29:50,368 Iter 1820, Minibatch Loss= 0.0166, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:29:53,840 Iter 1820, Minibatch Loss= 0.0089, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:29:55,281 Iter 1821, Minibatch Loss= 0.0165, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 17:29:56,786 Iter 1822, Minibatch Loss= 0.0091, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:29:58,261 Iter 1823, Minibatch Loss= 0.0101, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 17:29:59,732 Iter 1824, Minibatch Loss= 0.0227, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:30:01,209 Iter 1825, Minibatch Loss= 0.0080, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 17:30:02,689 Iter 1826, Minibatch Loss= 0.0230, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:30:04,198 Iter 1827, Minibatch Loss= 0.0210, Training Accuracy= 0.9665, Minibatch error= 3.4%\n",
      "2018-05-27 17:30:05,665 Iter 1828, Minibatch Loss= 0.0111, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 17:30:07,137 Iter 1829, Minibatch Loss= 0.0072, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 17:30:08,644 Iter 1830, Minibatch Loss= 0.0113, Training Accuracy= 0.9869, Minibatch error= 1.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:30:10,098 Iter 1831, Minibatch Loss= 0.0073, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 17:30:11,583 Iter 1832, Minibatch Loss= 0.0208, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:30:13,064 Iter 1833, Minibatch Loss= 0.0239, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 17:30:14,551 Iter 1834, Minibatch Loss= 0.0106, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:30:16,057 Iter 1835, Minibatch Loss= 0.0191, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 17:30:17,539 Iter 1836, Minibatch Loss= 0.0221, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 17:30:19,012 Iter 1837, Minibatch Loss= 0.0091, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 17:30:20,531 Iter 1838, Minibatch Loss= 0.0210, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:30:22,082 Iter 1839, Minibatch Loss= 0.0049, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 17:30:22,083 Epoch 91, Average loss: 0.0145, learning rate: 0.0010\n",
      "2018-05-27 17:30:31,639 Iter 1840, Minibatch Loss= 0.0159, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:30:35,051 Iter 1840, Minibatch Loss= 0.0104, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 17:30:36,576 Iter 1841, Minibatch Loss= 0.0139, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:30:38,063 Iter 1842, Minibatch Loss= 0.0127, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:30:39,560 Iter 1843, Minibatch Loss= 0.0172, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 17:30:41,025 Iter 1844, Minibatch Loss= 0.0106, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 17:30:42,536 Iter 1845, Minibatch Loss= 0.0209, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:30:44,056 Iter 1846, Minibatch Loss= 0.0171, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:30:45,513 Iter 1847, Minibatch Loss= 0.0206, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:30:47,011 Iter 1848, Minibatch Loss= 0.0205, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:30:48,496 Iter 1849, Minibatch Loss= 0.0199, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:30:49,945 Iter 1850, Minibatch Loss= 0.0241, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:30:51,466 Iter 1851, Minibatch Loss= 0.0099, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 17:30:52,940 Iter 1852, Minibatch Loss= 0.0121, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 17:30:54,469 Iter 1853, Minibatch Loss= 0.0196, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 17:30:55,913 Iter 1854, Minibatch Loss= 0.0112, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:30:57,392 Iter 1855, Minibatch Loss= 0.0080, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 17:30:58,893 Iter 1856, Minibatch Loss= 0.0232, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 17:31:00,369 Iter 1857, Minibatch Loss= 0.0083, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 17:31:01,891 Iter 1858, Minibatch Loss= 0.0206, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:31:03,390 Iter 1859, Minibatch Loss= 0.0198, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:31:03,391 Epoch 92, Average loss: 0.0161, learning rate: 0.0010\n",
      "2018-05-27 17:31:12,908 Iter 1860, Minibatch Loss= 0.0156, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:31:16,322 Iter 1860, Minibatch Loss= 0.0205, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:31:17,835 Iter 1861, Minibatch Loss= 0.0218, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:31:19,340 Iter 1862, Minibatch Loss= 0.0211, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:31:20,847 Iter 1863, Minibatch Loss= 0.0109, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:31:22,326 Iter 1864, Minibatch Loss= 0.0071, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:31:23,817 Iter 1865, Minibatch Loss= 0.0129, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:31:25,318 Iter 1866, Minibatch Loss= 0.0204, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:31:26,832 Iter 1867, Minibatch Loss= 0.0215, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:31:28,323 Iter 1868, Minibatch Loss= 0.0096, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 17:31:29,811 Iter 1869, Minibatch Loss= 0.0141, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:31:31,308 Iter 1870, Minibatch Loss= 0.0199, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:31:32,823 Iter 1871, Minibatch Loss= 0.0209, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:31:34,335 Iter 1872, Minibatch Loss= 0.0138, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 17:31:35,836 Iter 1873, Minibatch Loss= 0.0092, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 17:31:37,386 Iter 1874, Minibatch Loss= 0.0188, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:31:38,885 Iter 1875, Minibatch Loss= 0.0124, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:31:40,379 Iter 1876, Minibatch Loss= 0.0212, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:31:41,873 Iter 1877, Minibatch Loss= 0.0110, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:31:43,343 Iter 1878, Minibatch Loss= 0.0164, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:31:44,830 Iter 1879, Minibatch Loss= 0.0188, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:31:44,831 Epoch 93, Average loss: 0.0162, learning rate: 0.0010\n",
      "2018-05-27 17:31:54,469 Iter 1880, Minibatch Loss= 0.0158, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 17:31:57,896 Iter 1880, Minibatch Loss= 0.0188, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 17:31:59,409 Iter 1881, Minibatch Loss= 0.0111, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:32:00,895 Iter 1882, Minibatch Loss= 0.0172, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:32:02,415 Iter 1883, Minibatch Loss= 0.0177, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:32:03,937 Iter 1884, Minibatch Loss= 0.0219, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:32:05,477 Iter 1885, Minibatch Loss= 0.0122, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 17:32:06,966 Iter 1886, Minibatch Loss= 0.0160, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:32:08,473 Iter 1887, Minibatch Loss= 0.0102, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 17:32:09,979 Iter 1888, Minibatch Loss= 0.0271, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:32:11,478 Iter 1889, Minibatch Loss= 0.0082, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:32:12,969 Iter 1890, Minibatch Loss= 0.0189, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:32:14,469 Iter 1891, Minibatch Loss= 0.0058, Training Accuracy= 0.9958, Minibatch error= 0.4%\n",
      "2018-05-27 17:32:15,958 Iter 1892, Minibatch Loss= 0.0157, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 17:32:17,475 Iter 1893, Minibatch Loss= 0.0229, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 17:32:18,977 Iter 1894, Minibatch Loss= 0.0072, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 17:32:20,496 Iter 1895, Minibatch Loss= 0.0218, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:32:22,027 Iter 1896, Minibatch Loss= 0.0106, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 17:32:23,531 Iter 1897, Minibatch Loss= 0.0205, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:32:25,002 Iter 1898, Minibatch Loss= 0.0115, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 17:32:26,528 Iter 1899, Minibatch Loss= 0.0152, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:32:26,529 Epoch 94, Average loss: 0.0165, learning rate: 0.0010\n",
      "2018-05-27 17:32:36,141 Iter 1900, Minibatch Loss= 0.0166, Training Accuracy= 0.9796, Minibatch error= 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:32:39,628 Iter 1900, Minibatch Loss= 0.0204, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:32:41,110 Iter 1901, Minibatch Loss= 0.0172, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:32:42,668 Iter 1902, Minibatch Loss= 0.0143, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 17:32:44,156 Iter 1903, Minibatch Loss= 0.0090, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 17:32:45,655 Iter 1904, Minibatch Loss= 0.0110, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 17:32:47,182 Iter 1905, Minibatch Loss= 0.0153, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 17:32:48,717 Iter 1906, Minibatch Loss= 0.0233, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:32:50,251 Iter 1907, Minibatch Loss= 0.0129, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 17:32:51,792 Iter 1908, Minibatch Loss= 0.0200, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:32:53,290 Iter 1909, Minibatch Loss= 0.0215, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:32:54,823 Iter 1910, Minibatch Loss= 0.0206, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:32:56,333 Iter 1911, Minibatch Loss= 0.0099, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 17:32:57,860 Iter 1912, Minibatch Loss= 0.0071, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 17:32:59,365 Iter 1913, Minibatch Loss= 0.0104, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:33:00,849 Iter 1914, Minibatch Loss= 0.0143, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 17:33:02,346 Iter 1915, Minibatch Loss= 0.0171, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:33:03,848 Iter 1916, Minibatch Loss= 0.0162, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 17:33:05,376 Iter 1917, Minibatch Loss= 0.0228, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:33:06,897 Iter 1918, Minibatch Loss= 0.0157, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:33:08,356 Iter 1919, Minibatch Loss= 0.0104, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 17:33:08,357 Epoch 95, Average loss: 0.0157, learning rate: 0.0010\n",
      "2018-05-27 17:33:17,927 Iter 1920, Minibatch Loss= 0.0156, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:33:21,578 Iter 1920, Minibatch Loss= 0.0060, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 17:33:23,137 Iter 1921, Minibatch Loss= 0.0112, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 17:33:24,674 Iter 1922, Minibatch Loss= 0.0136, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:33:26,181 Iter 1923, Minibatch Loss= 0.0040, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 17:33:27,710 Iter 1924, Minibatch Loss= 0.0129, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:33:29,203 Iter 1925, Minibatch Loss= 0.0114, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 17:33:30,683 Iter 1926, Minibatch Loss= 0.0199, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:33:32,173 Iter 1927, Minibatch Loss= 0.0095, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 17:33:33,697 Iter 1928, Minibatch Loss= 0.0234, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:33:35,231 Iter 1929, Minibatch Loss= 0.0073, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 17:33:36,737 Iter 1930, Minibatch Loss= 0.0117, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:33:38,285 Iter 1931, Minibatch Loss= 0.0206, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:33:39,812 Iter 1932, Minibatch Loss= 0.0137, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 17:33:41,329 Iter 1933, Minibatch Loss= 0.0084, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 17:33:42,854 Iter 1934, Minibatch Loss= 0.0131, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 17:33:44,362 Iter 1935, Minibatch Loss= 0.0136, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 17:33:45,865 Iter 1936, Minibatch Loss= 0.0130, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 17:33:47,352 Iter 1937, Minibatch Loss= 0.0101, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:33:48,876 Iter 1938, Minibatch Loss= 0.0231, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:33:50,421 Iter 1939, Minibatch Loss= 0.0057, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 17:33:50,422 Epoch 96, Average loss: 0.0130, learning rate: 0.0010\n",
      "2018-05-27 17:34:00,009 Iter 1940, Minibatch Loss= 0.0155, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:34:03,474 Iter 1940, Minibatch Loss= 0.0212, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:34:04,993 Iter 1941, Minibatch Loss= 0.0216, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 17:34:06,550 Iter 1942, Minibatch Loss= 0.0217, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:34:08,078 Iter 1943, Minibatch Loss= 0.0129, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 17:34:09,586 Iter 1944, Minibatch Loss= 0.0229, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 17:34:11,105 Iter 1945, Minibatch Loss= 0.0178, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:34:12,627 Iter 1946, Minibatch Loss= 0.0058, Training Accuracy= 0.9958, Minibatch error= 0.4%\n",
      "2018-05-27 17:34:14,171 Iter 1947, Minibatch Loss= 0.0139, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:34:15,730 Iter 1948, Minibatch Loss= 0.0085, Training Accuracy= 0.9925, Minibatch error= 0.7%\n",
      "2018-05-27 17:34:17,257 Iter 1949, Minibatch Loss= 0.0203, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 17:34:18,780 Iter 1950, Minibatch Loss= 0.0057, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 17:34:20,310 Iter 1951, Minibatch Loss= 0.0156, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:34:21,848 Iter 1952, Minibatch Loss= 0.0100, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 17:34:23,375 Iter 1953, Minibatch Loss= 0.0144, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:34:24,905 Iter 1954, Minibatch Loss= 0.0196, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:34:26,402 Iter 1955, Minibatch Loss= 0.0110, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:34:27,928 Iter 1956, Minibatch Loss= 0.0212, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:34:29,440 Iter 1957, Minibatch Loss= 0.0072, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:34:30,990 Iter 1958, Minibatch Loss= 0.0213, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:34:32,485 Iter 1959, Minibatch Loss= 0.0101, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 17:34:32,486 Epoch 97, Average loss: 0.0152, learning rate: 0.0010\n",
      "2018-05-27 17:34:42,078 Iter 1960, Minibatch Loss= 0.0160, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:34:45,586 Iter 1960, Minibatch Loss= 0.0101, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 17:34:47,102 Iter 1961, Minibatch Loss= 0.0125, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:34:48,651 Iter 1962, Minibatch Loss= 0.0195, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:34:50,164 Iter 1963, Minibatch Loss= 0.0210, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:34:51,732 Iter 1964, Minibatch Loss= 0.0156, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 17:34:53,289 Iter 1965, Minibatch Loss= 0.0140, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 17:34:54,849 Iter 1966, Minibatch Loss= 0.0093, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 17:34:56,357 Iter 1967, Minibatch Loss= 0.0236, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 17:34:57,866 Iter 1968, Minibatch Loss= 0.0206, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:34:59,371 Iter 1969, Minibatch Loss= 0.0183, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:35:00,861 Iter 1970, Minibatch Loss= 0.0188, Training Accuracy= 0.9691, Minibatch error= 3.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:35:02,362 Iter 1971, Minibatch Loss= 0.0149, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 17:35:03,884 Iter 1972, Minibatch Loss= 0.0260, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:35:05,419 Iter 1973, Minibatch Loss= 0.0118, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 17:35:06,932 Iter 1974, Minibatch Loss= 0.0185, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 17:35:08,461 Iter 1975, Minibatch Loss= 0.0098, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:35:09,955 Iter 1976, Minibatch Loss= 0.0181, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 17:35:11,503 Iter 1977, Minibatch Loss= 0.0209, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:35:13,020 Iter 1978, Minibatch Loss= 0.0112, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:35:14,585 Iter 1979, Minibatch Loss= 0.0184, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:35:14,586 Epoch 98, Average loss: 0.0171, learning rate: 0.0010\n",
      "2018-05-27 17:35:24,161 Iter 1980, Minibatch Loss= 0.0154, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:35:27,700 Iter 1980, Minibatch Loss= 0.0040, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 17:35:29,260 Iter 1981, Minibatch Loss= 0.0102, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:35:30,804 Iter 1982, Minibatch Loss= 0.0188, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 17:35:32,329 Iter 1983, Minibatch Loss= 0.0125, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:35:33,862 Iter 1984, Minibatch Loss= 0.0068, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 17:35:35,463 Iter 1985, Minibatch Loss= 0.0115, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:35:37,041 Iter 1986, Minibatch Loss= 0.0194, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 17:35:38,595 Iter 1987, Minibatch Loss= 0.0224, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:35:40,163 Iter 1988, Minibatch Loss= 0.0109, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:35:41,692 Iter 1989, Minibatch Loss= 0.0217, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:35:43,224 Iter 1990, Minibatch Loss= 0.0224, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:35:44,758 Iter 1991, Minibatch Loss= 0.0131, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 17:35:46,314 Iter 1992, Minibatch Loss= 0.0216, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:35:47,829 Iter 1993, Minibatch Loss= 0.0050, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 17:35:49,354 Iter 1994, Minibatch Loss= 0.0198, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 17:35:50,879 Iter 1995, Minibatch Loss= 0.0215, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 17:35:52,463 Iter 1996, Minibatch Loss= 0.0155, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 17:35:54,077 Iter 1997, Minibatch Loss= 0.0179, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:35:55,602 Iter 1998, Minibatch Loss= 0.0172, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:35:57,134 Iter 1999, Minibatch Loss= 0.0056, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:35:57,135 Epoch 99, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 17:36:06,723 Iter 2000, Minibatch Loss= 0.0155, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 17:36:08,764 Optimization Finished!\n",
      "2018-05-27 17:36:09,025 Layers 3, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:36:11,141 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 17:36:09.018586-#epoch100-#iter20-lambda1e-05'\n",
      "2018-05-27 17:36:11,143 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 17:36:09.018586-#epoch100-#iter20-lambda1e-05'\n",
      "2018-05-27 17:36:31,006 Verification error= 17.9%, loss= 0.2147\n",
      "2018-05-27 17:36:31,329 Start optimization\n",
      "2018-05-27 17:36:41,079 Iter 0, Minibatch Loss= 0.2147, Training Accuracy= 0.8212, Minibatch error= 17.9%\n",
      "2018-05-27 17:36:42,684 Iter 0, Minibatch Loss= 0.1461, Training Accuracy= 0.9502, Minibatch error= 5.0%\n",
      "2018-05-27 17:36:43,787 Iter 1, Minibatch Loss= 0.1441, Training Accuracy= 0.9426, Minibatch error= 5.7%\n",
      "2018-05-27 17:36:44,883 Iter 2, Minibatch Loss= 0.3497, Training Accuracy= 0.5767, Minibatch error= 42.3%\n",
      "2018-05-27 17:36:45,986 Iter 3, Minibatch Loss= 0.3624, Training Accuracy= 0.5294, Minibatch error= 47.1%\n",
      "2018-05-27 17:36:47,100 Iter 4, Minibatch Loss= 0.2967, Training Accuracy= 0.6380, Minibatch error= 36.2%\n",
      "2018-05-27 17:36:48,211 Iter 5, Minibatch Loss= 0.2533, Training Accuracy= 0.7217, Minibatch error= 27.8%\n",
      "2018-05-27 17:36:49,301 Iter 6, Minibatch Loss= 0.1735, Training Accuracy= 0.9067, Minibatch error= 9.3%\n",
      "2018-05-27 17:36:50,424 Iter 7, Minibatch Loss= 0.3173, Training Accuracy= 0.5716, Minibatch error= 42.8%\n",
      "2018-05-27 17:36:51,535 Iter 8, Minibatch Loss= 0.2574, Training Accuracy= 0.7157, Minibatch error= 28.4%\n",
      "2018-05-27 17:36:52,669 Iter 9, Minibatch Loss= 0.1439, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:36:53,789 Iter 10, Minibatch Loss= 0.1548, Training Accuracy= 0.9516, Minibatch error= 4.8%\n",
      "2018-05-27 17:36:54,903 Iter 11, Minibatch Loss= 0.2838, Training Accuracy= 0.6432, Minibatch error= 35.7%\n",
      "2018-05-27 17:36:56,016 Iter 12, Minibatch Loss= 0.2967, Training Accuracy= 0.6118, Minibatch error= 38.8%\n",
      "2018-05-27 17:36:57,122 Iter 13, Minibatch Loss= 0.1763, Training Accuracy= 0.8836, Minibatch error= 11.6%\n",
      "2018-05-27 17:36:58,230 Iter 14, Minibatch Loss= 0.3080, Training Accuracy= 0.5802, Minibatch error= 42.0%\n",
      "2018-05-27 17:36:59,314 Iter 15, Minibatch Loss= 0.3275, Training Accuracy= 0.5541, Minibatch error= 44.6%\n",
      "2018-05-27 17:37:00,414 Iter 16, Minibatch Loss= 0.1864, Training Accuracy= 0.8601, Minibatch error= 14.0%\n",
      "2018-05-27 17:37:01,492 Iter 17, Minibatch Loss= 0.1202, Training Accuracy= 0.9943, Minibatch error= 0.6%\n",
      "2018-05-27 17:37:02,579 Iter 18, Minibatch Loss= 0.1485, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:37:03,664 Iter 19, Minibatch Loss= 0.1246, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:37:03,665 Epoch 0, Average loss: 0.2333, learning rate: 0.0010\n",
      "2018-05-27 17:37:13,207 Iter 20, Minibatch Loss= 0.1938, Training Accuracy= 0.9305, Minibatch error= 6.9%\n",
      "2018-05-27 17:37:14,744 Iter 20, Minibatch Loss= 0.1202, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:37:15,853 Iter 21, Minibatch Loss= 0.1313, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:37:16,966 Iter 22, Minibatch Loss= 0.2646, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:37:18,067 Iter 23, Minibatch Loss= 0.2528, Training Accuracy= 0.8741, Minibatch error= 12.6%\n",
      "2018-05-27 17:37:19,160 Iter 24, Minibatch Loss= 0.2674, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:37:20,257 Iter 25, Minibatch Loss= 0.2807, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 17:37:21,396 Iter 26, Minibatch Loss= 0.1112, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:37:22,513 Iter 27, Minibatch Loss= 0.0926, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 17:37:23,675 Iter 28, Minibatch Loss= 0.2729, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:37:24,783 Iter 29, Minibatch Loss= 0.2345, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 17:37:25,893 Iter 30, Minibatch Loss= 0.2737, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 17:37:27,018 Iter 31, Minibatch Loss= 0.2225, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:37:28,127 Iter 32, Minibatch Loss= 0.2094, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:37:29,247 Iter 33, Minibatch Loss= 0.1599, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:37:30,345 Iter 34, Minibatch Loss= 0.0444, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 17:37:31,463 Iter 35, Minibatch Loss= 0.1353, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:37:32,560 Iter 36, Minibatch Loss= 0.0396, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 17:37:33,654 Iter 37, Minibatch Loss= 0.0353, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 17:37:34,760 Iter 38, Minibatch Loss= 0.0427, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:37:35,886 Iter 39, Minibatch Loss= 0.1081, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:37:35,887 Epoch 1, Average loss: 0.1791, learning rate: 0.0010\n",
      "2018-05-27 17:37:45,440 Iter 40, Minibatch Loss= 0.0657, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 17:37:47,579 Iter 40, Minibatch Loss= 0.1102, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:37:48,700 Iter 41, Minibatch Loss= 0.0925, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:37:49,834 Iter 42, Minibatch Loss= 0.0240, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 17:37:50,963 Iter 43, Minibatch Loss= 0.0455, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 17:37:52,085 Iter 44, Minibatch Loss= 0.0386, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:37:53,256 Iter 45, Minibatch Loss= 0.0434, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 17:37:54,392 Iter 46, Minibatch Loss= 0.0256, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 17:37:55,521 Iter 47, Minibatch Loss= 0.0452, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:37:56,650 Iter 48, Minibatch Loss= 0.0454, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:37:57,768 Iter 49, Minibatch Loss= 0.0315, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:37:58,876 Iter 50, Minibatch Loss= 0.0265, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 17:37:59,984 Iter 51, Minibatch Loss= 0.0324, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 17:38:01,106 Iter 52, Minibatch Loss= 0.0502, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 17:38:02,249 Iter 53, Minibatch Loss= 0.0438, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:38:03,384 Iter 54, Minibatch Loss= 0.0369, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:38:04,511 Iter 55, Minibatch Loss= 0.0425, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:38:05,655 Iter 56, Minibatch Loss= 0.0171, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 17:38:06,783 Iter 57, Minibatch Loss= 0.0519, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 17:38:07,927 Iter 58, Minibatch Loss= 0.0290, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:38:09,056 Iter 59, Minibatch Loss= 0.0465, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:38:09,057 Epoch 2, Average loss: 0.0516, learning rate: 0.0010\n",
      "2018-05-27 17:38:18,603 Iter 60, Minibatch Loss= 0.0334, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:38:20,194 Iter 60, Minibatch Loss= 0.0493, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 17:38:21,348 Iter 61, Minibatch Loss= 0.0569, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:38:22,507 Iter 62, Minibatch Loss= 0.0496, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:38:23,675 Iter 63, Minibatch Loss= 0.0504, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:38:24,803 Iter 64, Minibatch Loss= 0.0107, Training Accuracy= 0.9985, Minibatch error= 0.2%\n",
      "2018-05-27 17:38:25,952 Iter 65, Minibatch Loss= 0.0319, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:38:27,083 Iter 66, Minibatch Loss= 0.0256, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 17:38:28,232 Iter 67, Minibatch Loss= 0.0379, Training Accuracy= 0.9679, Minibatch error= 3.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:38:29,372 Iter 68, Minibatch Loss= 0.0424, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:38:30,498 Iter 69, Minibatch Loss= 0.0373, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:38:31,620 Iter 70, Minibatch Loss= 0.0446, Training Accuracy= 0.9655, Minibatch error= 3.4%\n",
      "2018-05-27 17:38:32,750 Iter 71, Minibatch Loss= 0.0224, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:38:33,876 Iter 72, Minibatch Loss= 0.0403, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 17:38:34,996 Iter 73, Minibatch Loss= 0.0111, Training Accuracy= 0.9955, Minibatch error= 0.4%\n",
      "2018-05-27 17:38:36,132 Iter 74, Minibatch Loss= 0.0430, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:38:37,272 Iter 75, Minibatch Loss= 0.0482, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:38:38,394 Iter 76, Minibatch Loss= 0.0185, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 17:38:39,524 Iter 77, Minibatch Loss= 0.0200, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:38:40,636 Iter 78, Minibatch Loss= 0.0349, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:38:41,772 Iter 79, Minibatch Loss= 0.0427, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:38:41,774 Epoch 3, Average loss: 0.0368, learning rate: 0.0010\n",
      "2018-05-27 17:38:51,335 Iter 80, Minibatch Loss= 0.0282, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:38:52,995 Iter 80, Minibatch Loss= 0.0193, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 17:38:54,137 Iter 81, Minibatch Loss= 0.0360, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 17:38:55,277 Iter 82, Minibatch Loss= 0.0416, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 17:38:56,416 Iter 83, Minibatch Loss= 0.0394, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:38:57,536 Iter 84, Minibatch Loss= 0.0144, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:38:58,654 Iter 85, Minibatch Loss= 0.0291, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 17:38:59,779 Iter 86, Minibatch Loss= 0.0376, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:39:00,925 Iter 87, Minibatch Loss= 0.0197, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:39:02,064 Iter 88, Minibatch Loss= 0.0219, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 17:39:03,194 Iter 89, Minibatch Loss= 0.0518, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 17:39:04,310 Iter 90, Minibatch Loss= 0.0177, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 17:39:05,441 Iter 91, Minibatch Loss= 0.0202, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 17:39:06,592 Iter 92, Minibatch Loss= 0.0513, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:39:07,698 Iter 93, Minibatch Loss= 0.0186, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 17:39:08,828 Iter 94, Minibatch Loss= 0.0183, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 17:39:09,949 Iter 95, Minibatch Loss= 0.0386, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:39:11,096 Iter 96, Minibatch Loss= 0.0318, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:39:12,241 Iter 97, Minibatch Loss= 0.0339, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 17:39:13,372 Iter 98, Minibatch Loss= 0.0293, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 17:39:14,504 Iter 99, Minibatch Loss= 0.0095, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 17:39:14,505 Epoch 4, Average loss: 0.0311, learning rate: 0.0010\n",
      "2018-05-27 17:39:24,092 Iter 100, Minibatch Loss= 0.0238, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 17:39:25,746 Iter 100, Minibatch Loss= 0.0266, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:39:26,918 Iter 101, Minibatch Loss= 0.0454, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:39:28,039 Iter 102, Minibatch Loss= 0.0154, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:39:29,173 Iter 103, Minibatch Loss= 0.0117, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 17:39:30,329 Iter 104, Minibatch Loss= 0.0376, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:39:31,465 Iter 105, Minibatch Loss= 0.0215, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:39:32,620 Iter 106, Minibatch Loss= 0.0417, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:39:33,763 Iter 107, Minibatch Loss= 0.0168, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:39:34,898 Iter 108, Minibatch Loss= 0.0239, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 17:39:36,022 Iter 109, Minibatch Loss= 0.0499, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 17:39:37,163 Iter 110, Minibatch Loss= 0.0233, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:39:38,304 Iter 111, Minibatch Loss= 0.0066, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 17:39:39,462 Iter 112, Minibatch Loss= 0.0326, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 17:39:40,609 Iter 113, Minibatch Loss= 0.0145, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:39:41,737 Iter 114, Minibatch Loss= 0.0328, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 17:39:42,876 Iter 115, Minibatch Loss= 0.0335, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:39:43,977 Iter 116, Minibatch Loss= 0.0251, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:39:45,119 Iter 117, Minibatch Loss= 0.0463, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 17:39:46,296 Iter 118, Minibatch Loss= 0.0166, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 17:39:47,443 Iter 119, Minibatch Loss= 0.0173, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:39:47,444 Epoch 5, Average loss: 0.0270, learning rate: 0.0010\n",
      "2018-05-27 17:39:57,032 Iter 120, Minibatch Loss= 0.0307, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 17:39:58,672 Iter 120, Minibatch Loss= 0.0428, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 17:39:59,796 Iter 121, Minibatch Loss= 0.0369, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:00,907 Iter 122, Minibatch Loss= 0.0139, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 17:40:02,041 Iter 123, Minibatch Loss= 0.0369, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:03,187 Iter 124, Minibatch Loss= 0.0128, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 17:40:04,320 Iter 125, Minibatch Loss= 0.0195, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 17:40:05,458 Iter 126, Minibatch Loss= 0.0349, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 17:40:06,589 Iter 127, Minibatch Loss= 0.0402, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:07,726 Iter 128, Minibatch Loss= 0.0362, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 17:40:08,876 Iter 129, Minibatch Loss= 0.0170, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 17:40:10,018 Iter 130, Minibatch Loss= 0.0410, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:40:11,151 Iter 131, Minibatch Loss= 0.0396, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:12,268 Iter 132, Minibatch Loss= 0.0192, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 17:40:13,415 Iter 133, Minibatch Loss= 0.0394, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:14,557 Iter 134, Minibatch Loss= 0.0493, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:40:15,689 Iter 135, Minibatch Loss= 0.0068, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 17:40:16,826 Iter 136, Minibatch Loss= 0.0280, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:40:17,935 Iter 137, Minibatch Loss= 0.0112, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 17:40:19,083 Iter 138, Minibatch Loss= 0.0289, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:40:20,209 Iter 139, Minibatch Loss= 0.0298, Training Accuracy= 0.9703, Minibatch error= 3.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:40:20,210 Epoch 6, Average loss: 0.0290, learning rate: 0.0010\n",
      "2018-05-27 17:40:29,775 Iter 140, Minibatch Loss= 0.0196, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:40:31,460 Iter 140, Minibatch Loss= 0.0331, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 17:40:32,588 Iter 141, Minibatch Loss= 0.0400, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:40:33,731 Iter 142, Minibatch Loss= 0.0116, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 17:40:34,865 Iter 143, Minibatch Loss= 0.0085, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 17:40:36,019 Iter 144, Minibatch Loss= 0.0332, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 17:40:37,139 Iter 145, Minibatch Loss= 0.0428, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:40:38,287 Iter 146, Minibatch Loss= 0.0413, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:40:39,445 Iter 147, Minibatch Loss= 0.0263, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:40:40,570 Iter 148, Minibatch Loss= 0.0263, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 17:40:41,710 Iter 149, Minibatch Loss= 0.0313, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 17:40:42,837 Iter 150, Minibatch Loss= 0.0124, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 17:40:43,983 Iter 151, Minibatch Loss= 0.0250, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 17:40:45,122 Iter 152, Minibatch Loss= 0.0258, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:40:46,262 Iter 153, Minibatch Loss= 0.0135, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:40:47,427 Iter 154, Minibatch Loss= 0.0512, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:40:48,579 Iter 155, Minibatch Loss= 0.0382, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:40:49,744 Iter 156, Minibatch Loss= 0.0134, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 17:40:50,908 Iter 157, Minibatch Loss= 0.0274, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:40:52,039 Iter 158, Minibatch Loss= 0.0176, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 17:40:53,215 Iter 159, Minibatch Loss= 0.0167, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 17:40:53,216 Epoch 7, Average loss: 0.0275, learning rate: 0.0010\n",
      "2018-05-27 17:41:02,782 Iter 160, Minibatch Loss= 0.0189, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:41:04,588 Iter 160, Minibatch Loss= 0.0101, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:41:05,736 Iter 161, Minibatch Loss= 0.0320, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:41:06,886 Iter 162, Minibatch Loss= 0.0182, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:41:08,042 Iter 163, Minibatch Loss= 0.0305, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:41:09,178 Iter 164, Minibatch Loss= 0.0269, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:41:10,322 Iter 165, Minibatch Loss= 0.0302, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 17:41:11,450 Iter 166, Minibatch Loss= 0.0100, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 17:41:12,564 Iter 167, Minibatch Loss= 0.0131, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:41:13,704 Iter 168, Minibatch Loss= 0.0171, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 17:41:14,823 Iter 169, Minibatch Loss= 0.0168, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 17:41:15,981 Iter 170, Minibatch Loss= 0.0081, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:41:17,130 Iter 171, Minibatch Loss= 0.0182, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:41:18,273 Iter 172, Minibatch Loss= 0.0126, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 17:41:19,423 Iter 173, Minibatch Loss= 0.0325, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:41:20,550 Iter 174, Minibatch Loss= 0.0261, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:41:21,692 Iter 175, Minibatch Loss= 0.0091, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 17:41:22,867 Iter 176, Minibatch Loss= 0.0157, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:41:24,033 Iter 177, Minibatch Loss= 0.0116, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:41:25,186 Iter 178, Minibatch Loss= 0.0108, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 17:41:26,331 Iter 179, Minibatch Loss= 0.0149, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:41:26,332 Epoch 8, Average loss: 0.0190, learning rate: 0.0010\n",
      "2018-05-27 17:41:35,908 Iter 180, Minibatch Loss= 0.0198, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 17:41:37,608 Iter 180, Minibatch Loss= 0.0046, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 17:41:38,779 Iter 181, Minibatch Loss= 0.0235, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:41:39,895 Iter 182, Minibatch Loss= 0.0086, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 17:41:41,023 Iter 183, Minibatch Loss= 0.0274, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 17:41:42,160 Iter 184, Minibatch Loss= 0.0181, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 17:41:43,305 Iter 185, Minibatch Loss= 0.0321, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:41:44,430 Iter 186, Minibatch Loss= 0.0156, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 17:41:45,562 Iter 187, Minibatch Loss= 0.0073, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 17:41:46,717 Iter 188, Minibatch Loss= 0.0238, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:41:47,853 Iter 189, Minibatch Loss= 0.0272, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:41:48,996 Iter 190, Minibatch Loss= 0.0188, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 17:41:50,137 Iter 191, Minibatch Loss= 0.0239, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:41:51,295 Iter 192, Minibatch Loss= 0.0195, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 17:41:52,475 Iter 193, Minibatch Loss= 0.0233, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:41:53,679 Iter 194, Minibatch Loss= 0.0369, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:41:54,816 Iter 195, Minibatch Loss= 0.0304, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:41:56,003 Iter 196, Minibatch Loss= 0.0314, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 17:41:57,138 Iter 197, Minibatch Loss= 0.0240, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 17:41:58,289 Iter 198, Minibatch Loss= 0.0253, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:41:59,455 Iter 199, Minibatch Loss= 0.0248, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:41:59,456 Epoch 9, Average loss: 0.0208, learning rate: 0.0010\n",
      "2018-05-27 17:42:08,953 Iter 200, Minibatch Loss= 0.0178, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:42:10,658 Iter 200, Minibatch Loss= 0.0260, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 17:42:11,821 Iter 201, Minibatch Loss= 0.0150, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:42:12,961 Iter 202, Minibatch Loss= 0.0258, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:42:14,105 Iter 203, Minibatch Loss= 0.0192, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:42:15,241 Iter 204, Minibatch Loss= 0.0177, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 17:42:16,386 Iter 205, Minibatch Loss= 0.0321, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 17:42:17,554 Iter 206, Minibatch Loss= 0.0346, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:42:18,690 Iter 207, Minibatch Loss= 0.0498, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 17:42:19,849 Iter 208, Minibatch Loss= 0.0416, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:42:21,000 Iter 209, Minibatch Loss= 0.0341, Training Accuracy= 0.9764, Minibatch error= 2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:42:22,145 Iter 210, Minibatch Loss= 0.0291, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:42:23,358 Iter 211, Minibatch Loss= 0.0166, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 17:42:24,496 Iter 212, Minibatch Loss= 0.0129, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 17:42:25,636 Iter 213, Minibatch Loss= 0.0193, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:42:26,778 Iter 214, Minibatch Loss= 0.0085, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 17:42:27,927 Iter 215, Minibatch Loss= 0.0334, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:42:29,076 Iter 216, Minibatch Loss= 0.0249, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:42:30,233 Iter 217, Minibatch Loss= 0.0273, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:42:31,390 Iter 218, Minibatch Loss= 0.0235, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:42:32,524 Iter 219, Minibatch Loss= 0.0262, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:42:32,525 Epoch 10, Average loss: 0.0217, learning rate: 0.0010\n",
      "2018-05-27 17:42:42,016 Iter 220, Minibatch Loss= 0.0210, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 17:42:43,746 Iter 220, Minibatch Loss= 0.0286, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:42:44,922 Iter 221, Minibatch Loss= 0.0274, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:42:46,075 Iter 222, Minibatch Loss= 0.0076, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 17:42:47,227 Iter 223, Minibatch Loss= 0.0252, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:42:48,386 Iter 224, Minibatch Loss= 0.0097, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 17:42:49,556 Iter 225, Minibatch Loss= 0.0258, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 17:42:50,712 Iter 226, Minibatch Loss= 0.0085, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 17:42:51,881 Iter 227, Minibatch Loss= 0.0276, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:42:53,086 Iter 228, Minibatch Loss= 0.0275, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:42:54,253 Iter 229, Minibatch Loss= 0.0219, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 17:42:55,397 Iter 230, Minibatch Loss= 0.0113, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:42:56,539 Iter 231, Minibatch Loss= 0.0063, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 17:42:57,694 Iter 232, Minibatch Loss= 0.0240, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 17:42:58,837 Iter 233, Minibatch Loss= 0.0265, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:42:59,999 Iter 234, Minibatch Loss= 0.0251, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:43:01,142 Iter 235, Minibatch Loss= 0.0059, Training Accuracy= 0.9955, Minibatch error= 0.4%\n",
      "2018-05-27 17:43:02,302 Iter 236, Minibatch Loss= 0.0232, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 17:43:03,430 Iter 237, Minibatch Loss= 0.0122, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 17:43:04,580 Iter 238, Minibatch Loss= 0.0093, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 17:43:05,723 Iter 239, Minibatch Loss= 0.0130, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:43:05,724 Epoch 11, Average loss: 0.0174, learning rate: 0.0010\n",
      "2018-05-27 17:43:15,276 Iter 240, Minibatch Loss= 0.0190, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 17:43:17,048 Iter 240, Minibatch Loss= 0.0259, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:43:18,194 Iter 241, Minibatch Loss= 0.0156, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:43:19,345 Iter 242, Minibatch Loss= 0.0168, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:43:20,505 Iter 243, Minibatch Loss= 0.0161, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:43:21,665 Iter 244, Minibatch Loss= 0.0253, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 17:43:22,847 Iter 245, Minibatch Loss= 0.0117, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 17:43:23,975 Iter 246, Minibatch Loss= 0.0317, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 17:43:25,154 Iter 247, Minibatch Loss= 0.0278, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:43:26,329 Iter 248, Minibatch Loss= 0.0075, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 17:43:27,498 Iter 249, Minibatch Loss= 0.0258, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:43:28,659 Iter 250, Minibatch Loss= 0.0277, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 17:43:29,794 Iter 251, Minibatch Loss= 0.0105, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:43:30,920 Iter 252, Minibatch Loss= 0.0299, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 17:43:32,065 Iter 253, Minibatch Loss= 0.0308, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:43:33,211 Iter 254, Minibatch Loss= 0.0078, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 17:43:34,377 Iter 255, Minibatch Loss= 0.0066, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 17:43:35,523 Iter 256, Minibatch Loss= 0.0205, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 17:43:36,671 Iter 257, Minibatch Loss= 0.0143, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:43:37,810 Iter 258, Minibatch Loss= 0.0180, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:43:38,951 Iter 259, Minibatch Loss= 0.0314, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:43:38,952 Epoch 12, Average loss: 0.0196, learning rate: 0.0010\n",
      "2018-05-27 17:43:48,419 Iter 260, Minibatch Loss= 0.0195, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:43:50,123 Iter 260, Minibatch Loss= 0.0170, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 17:43:51,282 Iter 261, Minibatch Loss= 0.0430, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 17:43:52,473 Iter 262, Minibatch Loss= 0.0556, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:43:53,686 Iter 263, Minibatch Loss= 0.0522, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:43:54,825 Iter 264, Minibatch Loss= 0.0393, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:43:55,991 Iter 265, Minibatch Loss= 0.0431, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 17:43:57,145 Iter 266, Minibatch Loss= 0.0276, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 17:43:58,301 Iter 267, Minibatch Loss= 0.0173, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 17:43:59,454 Iter 268, Minibatch Loss= 0.0166, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 17:44:00,615 Iter 269, Minibatch Loss= 0.0153, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:44:01,759 Iter 270, Minibatch Loss= 0.0159, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:44:02,911 Iter 271, Minibatch Loss= 0.0358, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:44:04,042 Iter 272, Minibatch Loss= 0.0209, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 17:44:05,191 Iter 273, Minibatch Loss= 0.0307, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:44:06,331 Iter 274, Minibatch Loss= 0.0167, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 17:44:07,502 Iter 275, Minibatch Loss= 0.0295, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:44:08,642 Iter 276, Minibatch Loss= 0.0284, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:44:09,807 Iter 277, Minibatch Loss= 0.0182, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 17:44:10,968 Iter 278, Minibatch Loss= 0.0285, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 17:44:12,118 Iter 279, Minibatch Loss= 0.0160, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 17:44:12,119 Epoch 13, Average loss: 0.0192, learning rate: 0.0010\n",
      "2018-05-27 17:44:21,624 Iter 280, Minibatch Loss= 0.0189, Training Accuracy= 0.9748, Minibatch error= 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:44:23,493 Iter 280, Minibatch Loss= 0.0156, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:44:24,636 Iter 281, Minibatch Loss= 0.0131, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 17:44:25,793 Iter 282, Minibatch Loss= 0.0101, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 17:44:26,958 Iter 283, Minibatch Loss= 0.0142, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:44:28,126 Iter 284, Minibatch Loss= 0.0226, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 17:44:29,295 Iter 285, Minibatch Loss= 0.0267, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 17:44:30,452 Iter 286, Minibatch Loss= 0.0100, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:44:31,625 Iter 287, Minibatch Loss= 0.0213, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 17:44:32,779 Iter 288, Minibatch Loss= 0.0271, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:44:33,948 Iter 289, Minibatch Loss= 0.0106, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 17:44:35,104 Iter 290, Minibatch Loss= 0.0110, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 17:44:36,270 Iter 291, Minibatch Loss= 0.0289, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:44:37,442 Iter 292, Minibatch Loss= 0.0251, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 17:44:38,600 Iter 293, Minibatch Loss= 0.0148, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 17:44:39,765 Iter 294, Minibatch Loss= 0.0110, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 17:44:40,927 Iter 295, Minibatch Loss= 0.0173, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 17:44:42,070 Iter 296, Minibatch Loss= 0.0120, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 17:44:43,215 Iter 297, Minibatch Loss= 0.0189, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:44:44,390 Iter 298, Minibatch Loss= 0.0325, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 17:44:45,549 Iter 299, Minibatch Loss= 0.0103, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:44:45,550 Epoch 14, Average loss: 0.0173, learning rate: 0.0010\n",
      "2018-05-27 17:44:55,074 Iter 300, Minibatch Loss= 0.0191, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 17:44:56,972 Iter 300, Minibatch Loss= 0.0149, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:44:58,145 Iter 301, Minibatch Loss= 0.0109, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 17:44:59,383 Iter 302, Minibatch Loss= 0.0210, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:45:00,539 Iter 303, Minibatch Loss= 0.0481, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:45:01,698 Iter 304, Minibatch Loss= 0.0499, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:45:02,861 Iter 305, Minibatch Loss= 0.0644, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:45:04,023 Iter 306, Minibatch Loss= 0.0626, Training Accuracy= 0.9958, Minibatch error= 0.4%\n",
      "2018-05-27 17:45:05,180 Iter 307, Minibatch Loss= 0.0298, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:45:06,359 Iter 308, Minibatch Loss= 0.0273, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:45:07,501 Iter 309, Minibatch Loss= 0.0159, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:45:08,688 Iter 310, Minibatch Loss= 0.0084, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 17:45:09,839 Iter 311, Minibatch Loss= 0.0075, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 17:45:10,994 Iter 312, Minibatch Loss= 0.0040, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 17:45:12,148 Iter 313, Minibatch Loss= 0.0293, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:45:13,308 Iter 314, Minibatch Loss= 0.0080, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 17:45:14,484 Iter 315, Minibatch Loss= 0.0283, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:45:15,655 Iter 316, Minibatch Loss= 0.0251, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:45:16,818 Iter 317, Minibatch Loss= 0.0158, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 17:45:17,980 Iter 318, Minibatch Loss= 0.0269, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:45:19,146 Iter 319, Minibatch Loss= 0.0213, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 17:45:19,147 Epoch 15, Average loss: 0.0172, learning rate: 0.0010\n",
      "2018-05-27 17:45:28,647 Iter 320, Minibatch Loss= 0.0202, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 17:45:30,467 Iter 320, Minibatch Loss= 0.0224, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:45:31,656 Iter 321, Minibatch Loss= 0.0129, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 17:45:32,810 Iter 322, Minibatch Loss= 0.0310, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:45:33,980 Iter 323, Minibatch Loss= 0.0241, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 17:45:35,155 Iter 324, Minibatch Loss= 0.0076, Training Accuracy= 0.9987, Minibatch error= 0.1%\n",
      "2018-05-27 17:45:36,344 Iter 325, Minibatch Loss= 0.0276, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:45:37,523 Iter 326, Minibatch Loss= 0.0190, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 17:45:38,697 Iter 327, Minibatch Loss= 0.0295, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:45:39,861 Iter 328, Minibatch Loss= 0.0183, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:45:41,027 Iter 329, Minibatch Loss= 0.0317, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:45:42,190 Iter 330, Minibatch Loss= 0.0233, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 17:45:43,356 Iter 331, Minibatch Loss= 0.0291, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:45:44,535 Iter 332, Minibatch Loss= 0.0197, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:45:45,706 Iter 333, Minibatch Loss= 0.0257, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 17:45:46,872 Iter 334, Minibatch Loss= 0.0158, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 17:45:48,025 Iter 335, Minibatch Loss= 0.0217, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:45:49,185 Iter 336, Minibatch Loss= 0.0200, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 17:45:50,380 Iter 337, Minibatch Loss= 0.0302, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 17:45:51,561 Iter 338, Minibatch Loss= 0.0217, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 17:45:52,739 Iter 339, Minibatch Loss= 0.0220, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 17:45:52,740 Epoch 16, Average loss: 0.0184, learning rate: 0.0010\n",
      "2018-05-27 17:46:02,295 Iter 340, Minibatch Loss= 0.0194, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 17:46:04,109 Iter 340, Minibatch Loss= 0.0249, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 17:46:05,295 Iter 341, Minibatch Loss= 0.0155, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:46:06,479 Iter 342, Minibatch Loss= 0.0192, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 17:46:07,659 Iter 343, Minibatch Loss= 0.0294, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 17:46:08,815 Iter 344, Minibatch Loss= 0.0280, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 17:46:09,966 Iter 345, Minibatch Loss= 0.0181, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 17:46:11,115 Iter 346, Minibatch Loss= 0.0185, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 17:46:12,287 Iter 347, Minibatch Loss= 0.0123, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 17:46:13,439 Iter 348, Minibatch Loss= 0.0048, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 17:46:14,614 Iter 349, Minibatch Loss= 0.0201, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 17:46:15,781 Iter 350, Minibatch Loss= 0.0082, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 17:46:16,962 Iter 351, Minibatch Loss= 0.0184, Training Accuracy= 0.9781, Minibatch error= 2.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:46:18,135 Iter 352, Minibatch Loss= 0.0078, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 17:46:19,299 Iter 353, Minibatch Loss= 0.0199, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 17:46:20,488 Iter 354, Minibatch Loss= 0.0253, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:46:21,646 Iter 355, Minibatch Loss= 0.0182, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:46:22,796 Iter 356, Minibatch Loss= 0.0711, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:46:23,993 Iter 357, Minibatch Loss= 0.0537, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 17:46:25,154 Iter 358, Minibatch Loss= 0.0434, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 17:46:26,298 Iter 359, Minibatch Loss= 0.0186, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:46:26,299 Epoch 17, Average loss: 0.0170, learning rate: 0.0010\n",
      "2018-05-27 17:46:35,776 Iter 360, Minibatch Loss= 0.0232, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:46:37,672 Iter 360, Minibatch Loss= 0.0270, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:46:38,869 Iter 361, Minibatch Loss= 0.0281, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:46:40,040 Iter 362, Minibatch Loss= 0.0132, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 17:46:41,200 Iter 363, Minibatch Loss= 0.0121, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:46:42,365 Iter 364, Minibatch Loss= 0.0073, Training Accuracy= 0.9943, Minibatch error= 0.6%\n",
      "2018-05-27 17:46:43,565 Iter 365, Minibatch Loss= 0.0146, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:46:44,723 Iter 366, Minibatch Loss= 0.0212, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 17:46:45,881 Iter 367, Minibatch Loss= 0.0119, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 17:46:47,067 Iter 368, Minibatch Loss= 0.0246, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 17:46:48,309 Iter 369, Minibatch Loss= 0.0163, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:46:49,473 Iter 370, Minibatch Loss= 0.0271, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:46:50,644 Iter 371, Minibatch Loss= 0.0216, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 17:46:51,818 Iter 372, Minibatch Loss= 0.0172, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 17:46:53,008 Iter 373, Minibatch Loss= 0.0365, Training Accuracy= 0.9531, Minibatch error= 4.7%\n",
      "2018-05-27 17:46:54,170 Iter 374, Minibatch Loss= 0.0168, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:46:55,318 Iter 375, Minibatch Loss= 0.0080, Training Accuracy= 0.9955, Minibatch error= 0.4%\n",
      "2018-05-27 17:46:56,492 Iter 376, Minibatch Loss= 0.0055, Training Accuracy= 0.9981, Minibatch error= 0.2%\n",
      "2018-05-27 17:46:57,678 Iter 377, Minibatch Loss= 0.0218, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 17:46:58,884 Iter 378, Minibatch Loss= 0.0272, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:47:00,050 Iter 379, Minibatch Loss= 0.0182, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:47:00,051 Epoch 18, Average loss: 0.0159, learning rate: 0.0010\n",
      "2018-05-27 17:47:09,591 Iter 380, Minibatch Loss= 0.0187, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 17:47:11,470 Iter 380, Minibatch Loss= 0.0154, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:47:12,646 Iter 381, Minibatch Loss= 0.0259, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:47:13,835 Iter 382, Minibatch Loss= 0.0294, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:47:15,013 Iter 383, Minibatch Loss= 0.0274, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 17:47:16,193 Iter 384, Minibatch Loss= 0.0210, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:47:17,481 Iter 385, Minibatch Loss= 0.0335, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 17:47:18,683 Iter 386, Minibatch Loss= 0.0309, Training Accuracy= 0.9615, Minibatch error= 3.9%\n",
      "2018-05-27 17:47:19,872 Iter 387, Minibatch Loss= 0.0209, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:47:21,050 Iter 388, Minibatch Loss= 0.0302, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 17:47:22,238 Iter 389, Minibatch Loss= 0.0251, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:47:23,425 Iter 390, Minibatch Loss= 0.0122, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 17:47:24,597 Iter 391, Minibatch Loss= 0.0138, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:47:25,787 Iter 392, Minibatch Loss= 0.0117, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 17:47:26,956 Iter 393, Minibatch Loss= 0.0035, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 17:47:28,134 Iter 394, Minibatch Loss= 0.0265, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:47:29,308 Iter 395, Minibatch Loss= 0.0269, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:47:30,529 Iter 396, Minibatch Loss= 0.0295, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:47:31,728 Iter 397, Minibatch Loss= 0.0572, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 17:47:32,912 Iter 398, Minibatch Loss= 0.0664, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 17:47:34,079 Iter 399, Minibatch Loss= 0.0206, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 17:47:34,080 Epoch 19, Average loss: 0.0201, learning rate: 0.0010\n",
      "2018-05-27 17:47:43,626 Iter 400, Minibatch Loss= 0.0304, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:47:45,532 Iter 400, Minibatch Loss= 0.0304, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:47:46,719 Iter 401, Minibatch Loss= 0.0206, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 17:47:47,890 Iter 402, Minibatch Loss= 0.0281, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:47:49,056 Iter 403, Minibatch Loss= 0.0277, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 17:47:50,216 Iter 404, Minibatch Loss= 0.0252, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 17:47:51,398 Iter 405, Minibatch Loss= 0.0129, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:47:52,570 Iter 406, Minibatch Loss= 0.0139, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 17:47:53,728 Iter 407, Minibatch Loss= 0.0070, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 17:47:54,904 Iter 408, Minibatch Loss= 0.0103, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 17:47:56,068 Iter 409, Minibatch Loss= 0.0235, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 17:47:57,259 Iter 410, Minibatch Loss= 0.0197, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 17:47:58,436 Iter 411, Minibatch Loss= 0.0202, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 17:47:59,625 Iter 412, Minibatch Loss= 0.0274, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 17:48:00,797 Iter 413, Minibatch Loss= 0.0175, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 17:48:01,987 Iter 414, Minibatch Loss= 0.0371, Training Accuracy= 0.9635, Minibatch error= 3.6%\n",
      "2018-05-27 17:48:03,164 Iter 415, Minibatch Loss= 0.0282, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 17:48:04,334 Iter 416, Minibatch Loss= 0.0380, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 17:48:05,595 Iter 417, Minibatch Loss= 0.0340, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:48:06,796 Iter 418, Minibatch Loss= 0.0360, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 17:48:08,003 Iter 419, Minibatch Loss= 0.0311, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:48:08,004 Epoch 20, Average loss: 0.0181, learning rate: 0.0010\n",
      "2018-05-27 17:48:17,527 Iter 420, Minibatch Loss= 0.0261, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 17:48:19,500 Iter 420, Minibatch Loss= 0.0276, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:48:20,703 Iter 421, Minibatch Loss= 0.0175, Training Accuracy= 0.9892, Minibatch error= 1.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:48:21,902 Iter 422, Minibatch Loss= 0.0193, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 17:48:23,117 Iter 423, Minibatch Loss= 0.0153, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:48:24,315 Iter 424, Minibatch Loss= 0.0259, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:48:25,514 Iter 425, Minibatch Loss= 0.0256, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:48:26,711 Iter 426, Minibatch Loss= 0.0277, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:48:27,878 Iter 427, Minibatch Loss= 0.0338, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 17:48:29,078 Iter 428, Minibatch Loss= 0.0315, Training Accuracy= 0.9635, Minibatch error= 3.6%\n",
      "2018-05-27 17:48:30,251 Iter 429, Minibatch Loss= 0.0164, Training Accuracy= 0.9918, Minibatch error= 0.8%\n",
      "2018-05-27 17:48:31,422 Iter 430, Minibatch Loss= 0.0143, Training Accuracy= 0.9949, Minibatch error= 0.5%\n",
      "2018-05-27 17:48:32,605 Iter 431, Minibatch Loss= 0.0256, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 17:48:33,776 Iter 432, Minibatch Loss= 0.0349, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 17:48:34,936 Iter 433, Minibatch Loss= 0.0136, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 17:48:36,131 Iter 434, Minibatch Loss= 0.0160, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 17:48:37,330 Iter 435, Minibatch Loss= 0.0065, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 17:48:38,532 Iter 436, Minibatch Loss= 0.0209, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 17:48:39,725 Iter 437, Minibatch Loss= 0.0266, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 17:48:40,911 Iter 438, Minibatch Loss= 0.0260, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:48:42,101 Iter 439, Minibatch Loss= 0.0251, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 17:48:42,103 Epoch 21, Average loss: 0.0175, learning rate: 0.0010\n",
      "2018-05-27 17:48:51,639 Iter 440, Minibatch Loss= 0.0206, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 17:48:53,733 Iter 440, Minibatch Loss= 0.0329, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:48:54,936 Iter 441, Minibatch Loss= 0.0578, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 17:48:56,150 Iter 442, Minibatch Loss= 0.1039, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:48:57,337 Iter 443, Minibatch Loss= 0.1215, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 17:48:58,526 Iter 444, Minibatch Loss= 0.0725, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 17:48:59,720 Iter 445, Minibatch Loss= 0.0504, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:49:00,940 Iter 446, Minibatch Loss= 0.0387, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:49:02,149 Iter 447, Minibatch Loss= 0.0293, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:49:03,327 Iter 448, Minibatch Loss= 0.0104, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 17:49:04,510 Iter 449, Minibatch Loss= 0.0083, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 17:49:05,715 Iter 450, Minibatch Loss= 0.0315, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:49:06,908 Iter 451, Minibatch Loss= 0.0172, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 17:49:08,120 Iter 452, Minibatch Loss= 0.0292, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 17:49:09,306 Iter 453, Minibatch Loss= 0.0371, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:49:10,518 Iter 454, Minibatch Loss= 0.0508, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 17:49:11,696 Iter 455, Minibatch Loss= 0.0727, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 17:49:12,883 Iter 456, Minibatch Loss= 0.0789, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 17:49:14,083 Iter 457, Minibatch Loss= 0.0726, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 17:49:15,266 Iter 458, Minibatch Loss= 0.0607, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:49:16,455 Iter 459, Minibatch Loss= 0.0410, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:49:16,456 Epoch 22, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 17:49:25,962 Iter 460, Minibatch Loss= 0.0431, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 17:49:27,913 Iter 460, Minibatch Loss= 0.0321, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:49:29,081 Iter 461, Minibatch Loss= 0.0217, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 17:49:30,284 Iter 462, Minibatch Loss= 0.0150, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 17:49:31,500 Iter 463, Minibatch Loss= 0.0204, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:49:32,695 Iter 464, Minibatch Loss= 0.0316, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:49:33,880 Iter 465, Minibatch Loss= 0.0276, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:49:35,077 Iter 466, Minibatch Loss= 0.0183, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:49:36,295 Iter 467, Minibatch Loss= 0.0090, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 17:49:37,504 Iter 468, Minibatch Loss= 0.0299, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:49:38,701 Iter 469, Minibatch Loss= 0.0252, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:49:39,895 Iter 470, Minibatch Loss= 0.0284, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 17:49:41,087 Iter 471, Minibatch Loss= 0.0346, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:49:42,301 Iter 472, Minibatch Loss= 0.0325, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:49:43,512 Iter 473, Minibatch Loss= 0.0358, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:49:44,692 Iter 474, Minibatch Loss= 0.0334, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 17:49:45,882 Iter 475, Minibatch Loss= 0.0190, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 17:49:47,056 Iter 476, Minibatch Loss= 0.0271, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:49:48,256 Iter 477, Minibatch Loss= 0.0203, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 17:49:49,425 Iter 478, Minibatch Loss= 0.0246, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:49:50,614 Iter 479, Minibatch Loss= 0.0127, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:49:50,615 Epoch 23, Average loss: 0.0171, learning rate: 0.0010\n",
      "2018-05-27 17:50:00,053 Iter 480, Minibatch Loss= 0.0194, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:50:02,087 Iter 480, Minibatch Loss= 0.0254, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:50:03,265 Iter 481, Minibatch Loss= 0.0189, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 17:50:04,433 Iter 482, Minibatch Loss= 0.0082, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 17:50:05,629 Iter 483, Minibatch Loss= 0.0211, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:50:06,817 Iter 484, Minibatch Loss= 0.0315, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 17:50:08,027 Iter 485, Minibatch Loss= 0.0386, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 17:50:09,236 Iter 486, Minibatch Loss= 0.0459, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 17:50:10,429 Iter 487, Minibatch Loss= 0.0424, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 17:50:11,654 Iter 488, Minibatch Loss= 0.0411, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:50:12,869 Iter 489, Minibatch Loss= 0.0353, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:50:14,085 Iter 490, Minibatch Loss= 0.0207, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 17:50:15,298 Iter 491, Minibatch Loss= 0.0114, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:50:16,528 Iter 492, Minibatch Loss= 0.0238, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:50:17,748 Iter 493, Minibatch Loss= 0.0206, Training Accuracy= 0.9696, Minibatch error= 3.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:50:18,959 Iter 494, Minibatch Loss= 0.0254, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:50:20,185 Iter 495, Minibatch Loss= 0.0238, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:50:21,426 Iter 496, Minibatch Loss= 0.0105, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 17:50:22,605 Iter 497, Minibatch Loss= 0.0089, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 17:50:23,799 Iter 498, Minibatch Loss= 0.0197, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 17:50:24,991 Iter 499, Minibatch Loss= 0.0084, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 17:50:24,992 Epoch 24, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 17:50:34,535 Iter 500, Minibatch Loss= 0.0179, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 17:50:36,548 Iter 500, Minibatch Loss= 0.0032, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 17:50:37,768 Iter 501, Minibatch Loss= 0.0221, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 17:50:38,969 Iter 502, Minibatch Loss= 0.0180, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 17:50:40,192 Iter 503, Minibatch Loss= 0.0238, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:50:41,400 Iter 504, Minibatch Loss= 0.0205, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:50:42,618 Iter 505, Minibatch Loss= 0.0077, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:50:43,815 Iter 506, Minibatch Loss= 0.0118, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 17:50:45,020 Iter 507, Minibatch Loss= 0.0246, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 17:50:46,255 Iter 508, Minibatch Loss= 0.0245, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 17:50:47,502 Iter 509, Minibatch Loss= 0.0297, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 17:50:48,708 Iter 510, Minibatch Loss= 0.0227, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 17:50:49,914 Iter 511, Minibatch Loss= 0.0221, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:50:51,129 Iter 512, Minibatch Loss= 0.0265, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 17:50:52,349 Iter 513, Minibatch Loss= 0.0230, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:50:53,555 Iter 514, Minibatch Loss= 0.0197, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:50:54,759 Iter 515, Minibatch Loss= 0.0226, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:50:55,977 Iter 516, Minibatch Loss= 0.0249, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 17:50:57,179 Iter 517, Minibatch Loss= 0.0102, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 17:50:58,379 Iter 518, Minibatch Loss= 0.0191, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 17:50:59,609 Iter 519, Minibatch Loss= 0.0187, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:50:59,610 Epoch 25, Average loss: 0.0189, learning rate: 0.0010\n",
      "2018-05-27 17:51:09,139 Iter 520, Minibatch Loss= 0.0161, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 17:51:11,158 Iter 520, Minibatch Loss= 0.0198, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 17:51:12,362 Iter 521, Minibatch Loss= 0.0257, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 17:51:13,576 Iter 522, Minibatch Loss= 0.0024, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 17:51:14,782 Iter 523, Minibatch Loss= 0.0067, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:51:16,017 Iter 524, Minibatch Loss= 0.0285, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 17:51:17,226 Iter 525, Minibatch Loss= 0.0222, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 17:51:18,408 Iter 526, Minibatch Loss= 0.0076, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 17:51:19,626 Iter 527, Minibatch Loss= 0.0043, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 17:51:20,828 Iter 528, Minibatch Loss= 0.0243, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:51:22,036 Iter 529, Minibatch Loss= 0.0163, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 17:51:23,225 Iter 530, Minibatch Loss= 0.0241, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 17:51:24,452 Iter 531, Minibatch Loss= 0.0231, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:51:25,658 Iter 532, Minibatch Loss= 0.0046, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 17:51:26,899 Iter 533, Minibatch Loss= 0.0111, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 17:51:28,104 Iter 534, Minibatch Loss= 0.0189, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 17:51:29,301 Iter 535, Minibatch Loss= 0.0033, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 17:51:30,506 Iter 536, Minibatch Loss= 0.0206, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 17:51:31,724 Iter 537, Minibatch Loss= 0.0162, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 17:51:32,933 Iter 538, Minibatch Loss= 0.0044, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 17:51:34,132 Iter 539, Minibatch Loss= 0.0151, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:51:34,134 Epoch 26, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 17:51:43,725 Iter 540, Minibatch Loss= 0.0163, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 17:51:45,759 Iter 540, Minibatch Loss= 0.0232, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:51:46,983 Iter 541, Minibatch Loss= 0.0267, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 17:51:48,199 Iter 542, Minibatch Loss= 0.0175, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:51:49,401 Iter 543, Minibatch Loss= 0.0052, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 17:51:50,598 Iter 544, Minibatch Loss= 0.0027, Training Accuracy= 0.9979, Minibatch error= 0.2%\n",
      "2018-05-27 17:51:51,821 Iter 545, Minibatch Loss= 0.0201, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 17:51:53,020 Iter 546, Minibatch Loss= 0.0200, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 17:51:54,214 Iter 547, Minibatch Loss= 0.0041, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 17:51:55,454 Iter 548, Minibatch Loss= 0.0164, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 17:51:56,667 Iter 549, Minibatch Loss= 0.0224, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 17:51:57,884 Iter 550, Minibatch Loss= 0.0104, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 17:51:59,089 Iter 551, Minibatch Loss= 0.0214, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:52:00,299 Iter 552, Minibatch Loss= 0.0249, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 17:52:01,504 Iter 553, Minibatch Loss= 0.0236, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 17:52:02,716 Iter 554, Minibatch Loss= 0.0298, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:52:03,941 Iter 555, Minibatch Loss= 0.0226, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 17:52:05,144 Iter 556, Minibatch Loss= 0.0152, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:52:06,336 Iter 557, Minibatch Loss= 0.0095, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 17:52:07,576 Iter 558, Minibatch Loss= 0.0126, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 17:52:08,844 Iter 559, Minibatch Loss= 0.0276, Training Accuracy= 0.9565, Minibatch error= 4.3%\n",
      "2018-05-27 17:52:08,845 Epoch 27, Average loss: 0.0177, learning rate: 0.0010\n",
      "2018-05-27 17:52:18,316 Iter 560, Minibatch Loss= 0.0159, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 17:52:20,394 Iter 560, Minibatch Loss= 0.0238, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:52:21,604 Iter 561, Minibatch Loss= 0.0227, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:52:22,833 Iter 562, Minibatch Loss= 0.0129, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 17:52:24,036 Iter 563, Minibatch Loss= 0.0227, Training Accuracy= 0.9698, Minibatch error= 3.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:52:25,268 Iter 564, Minibatch Loss= 0.0166, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:52:26,492 Iter 565, Minibatch Loss= 0.0154, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:52:27,701 Iter 566, Minibatch Loss= 0.0018, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 17:52:28,926 Iter 567, Minibatch Loss= 0.0226, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:52:30,142 Iter 568, Minibatch Loss= 0.0194, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 17:52:31,361 Iter 569, Minibatch Loss= 0.0229, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:52:32,567 Iter 570, Minibatch Loss= 0.0132, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 17:52:33,772 Iter 571, Minibatch Loss= 0.0237, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 17:52:34,994 Iter 572, Minibatch Loss= 0.0126, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 17:52:36,201 Iter 573, Minibatch Loss= 0.0165, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 17:52:37,387 Iter 574, Minibatch Loss= 0.0191, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 17:52:38,620 Iter 575, Minibatch Loss= 0.0148, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 17:52:39,811 Iter 576, Minibatch Loss= 0.0117, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 17:52:41,041 Iter 577, Minibatch Loss= 0.0125, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 17:52:42,302 Iter 578, Minibatch Loss= 0.0054, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 17:52:43,519 Iter 579, Minibatch Loss= 0.0104, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:52:43,520 Epoch 28, Average loss: 0.0159, learning rate: 0.0010\n",
      "2018-05-27 17:52:53,060 Iter 580, Minibatch Loss= 0.0151, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 17:52:55,119 Iter 580, Minibatch Loss= 0.0107, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:52:56,319 Iter 581, Minibatch Loss= 0.0423, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:52:57,528 Iter 582, Minibatch Loss= 0.0493, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 17:52:58,764 Iter 583, Minibatch Loss= 0.0297, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 17:52:59,954 Iter 584, Minibatch Loss= 0.0128, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 17:53:01,171 Iter 585, Minibatch Loss= 0.0113, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:53:02,371 Iter 586, Minibatch Loss= 0.0239, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 17:53:03,582 Iter 587, Minibatch Loss= 0.0215, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:53:04,762 Iter 588, Minibatch Loss= 0.0097, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 17:53:05,961 Iter 589, Minibatch Loss= 0.0215, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:53:07,186 Iter 590, Minibatch Loss= 0.0246, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:53:08,392 Iter 591, Minibatch Loss= 0.0151, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:53:09,618 Iter 592, Minibatch Loss= 0.0230, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 17:53:10,831 Iter 593, Minibatch Loss= 0.0153, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:53:12,054 Iter 594, Minibatch Loss= 0.0248, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 17:53:13,256 Iter 595, Minibatch Loss= 0.0145, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 17:53:14,483 Iter 596, Minibatch Loss= 0.0089, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 17:53:15,759 Iter 597, Minibatch Loss= 0.0227, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:53:16,978 Iter 598, Minibatch Loss= 0.0140, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 17:53:18,180 Iter 599, Minibatch Loss= 0.0123, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 17:53:18,181 Epoch 29, Average loss: 0.0178, learning rate: 0.0010\n",
      "2018-05-27 17:53:27,708 Iter 600, Minibatch Loss= 0.0161, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 17:53:29,942 Iter 600, Minibatch Loss= 0.0273, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:53:31,131 Iter 601, Minibatch Loss= 0.0039, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 17:53:32,358 Iter 602, Minibatch Loss= 0.0201, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:53:33,592 Iter 603, Minibatch Loss= 0.0138, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 17:53:34,819 Iter 604, Minibatch Loss= 0.0278, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:53:36,036 Iter 605, Minibatch Loss= 0.0308, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 17:53:37,243 Iter 606, Minibatch Loss= 0.0290, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:53:38,494 Iter 607, Minibatch Loss= 0.0303, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 17:53:39,746 Iter 608, Minibatch Loss= 0.0177, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 17:53:40,979 Iter 609, Minibatch Loss= 0.0193, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 17:53:42,196 Iter 610, Minibatch Loss= 0.0222, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:53:43,434 Iter 611, Minibatch Loss= 0.0044, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 17:53:44,739 Iter 612, Minibatch Loss= 0.0175, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:53:45,963 Iter 613, Minibatch Loss= 0.0150, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 17:53:47,173 Iter 614, Minibatch Loss= 0.0142, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:53:48,392 Iter 615, Minibatch Loss= 0.0318, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 17:53:49,623 Iter 616, Minibatch Loss= 0.0426, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 17:53:50,829 Iter 617, Minibatch Loss= 0.0675, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 17:53:52,100 Iter 618, Minibatch Loss= 0.0903, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 17:53:53,311 Iter 619, Minibatch Loss= 0.0803, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:53:53,312 Epoch 30, Average loss: 0.0174, learning rate: 0.0010\n",
      "2018-05-27 17:54:02,775 Iter 620, Minibatch Loss= 0.0734, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 17:54:04,933 Iter 620, Minibatch Loss= 0.0358, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:54:06,174 Iter 621, Minibatch Loss= 0.0140, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 17:54:07,443 Iter 622, Minibatch Loss= 0.0228, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:54:08,669 Iter 623, Minibatch Loss= 0.0081, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 17:54:09,931 Iter 624, Minibatch Loss= 0.0045, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 17:54:11,141 Iter 625, Minibatch Loss= 0.0129, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 17:54:12,359 Iter 626, Minibatch Loss= 0.0203, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 17:54:13,575 Iter 627, Minibatch Loss= 0.0080, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 17:54:14,811 Iter 628, Minibatch Loss= 0.0171, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:54:16,026 Iter 629, Minibatch Loss= 0.0226, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 17:54:17,251 Iter 630, Minibatch Loss= 0.0218, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:54:18,474 Iter 631, Minibatch Loss= 0.0292, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 17:54:19,684 Iter 632, Minibatch Loss= 0.0298, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 17:54:20,904 Iter 633, Minibatch Loss= 0.0460, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 17:54:22,142 Iter 634, Minibatch Loss= 0.0531, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 17:54:23,383 Iter 635, Minibatch Loss= 0.0377, Training Accuracy= 0.9814, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:54:24,607 Iter 636, Minibatch Loss= 0.0326, Training Accuracy= 0.9533, Minibatch error= 4.7%\n",
      "2018-05-27 17:54:25,846 Iter 637, Minibatch Loss= 0.0189, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:54:27,076 Iter 638, Minibatch Loss= 0.0172, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 17:54:28,350 Iter 639, Minibatch Loss= 0.0264, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 17:54:28,351 Epoch 31, Average loss: 0.0170, learning rate: 0.0010\n",
      "2018-05-27 17:54:37,807 Iter 640, Minibatch Loss= 0.0155, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 17:54:39,960 Iter 640, Minibatch Loss= 0.0257, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:54:41,165 Iter 641, Minibatch Loss= 0.0152, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 17:54:42,399 Iter 642, Minibatch Loss= 0.0226, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:54:43,620 Iter 643, Minibatch Loss= 0.0287, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 17:54:44,832 Iter 644, Minibatch Loss= 0.0188, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:54:46,040 Iter 645, Minibatch Loss= 0.0299, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 17:54:47,263 Iter 646, Minibatch Loss= 0.0302, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 17:54:48,502 Iter 647, Minibatch Loss= 0.0214, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 17:54:49,738 Iter 648, Minibatch Loss= 0.0260, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 17:54:50,969 Iter 649, Minibatch Loss= 0.0234, Training Accuracy= 0.9655, Minibatch error= 3.4%\n",
      "2018-05-27 17:54:52,210 Iter 650, Minibatch Loss= 0.0237, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:54:53,448 Iter 651, Minibatch Loss= 0.0115, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 17:54:54,656 Iter 652, Minibatch Loss= 0.0133, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:54:55,896 Iter 653, Minibatch Loss= 0.0266, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 17:54:57,127 Iter 654, Minibatch Loss= 0.0266, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 17:54:58,347 Iter 655, Minibatch Loss= 0.0137, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 17:54:59,553 Iter 656, Minibatch Loss= 0.0270, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 17:55:00,803 Iter 657, Minibatch Loss= 0.0344, Training Accuracy= 0.9563, Minibatch error= 4.4%\n",
      "2018-05-27 17:55:02,003 Iter 658, Minibatch Loss= 0.0327, Training Accuracy= 0.9558, Minibatch error= 4.4%\n",
      "2018-05-27 17:55:03,203 Iter 659, Minibatch Loss= 0.0271, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 17:55:03,204 Epoch 32, Average loss: 0.0202, learning rate: 0.0010\n",
      "2018-05-27 17:55:12,689 Iter 660, Minibatch Loss= 0.0240, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 17:55:14,854 Iter 660, Minibatch Loss= 0.0174, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 17:55:16,079 Iter 661, Minibatch Loss= 0.0272, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 17:55:17,301 Iter 662, Minibatch Loss= 0.0194, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:55:18,522 Iter 663, Minibatch Loss= 0.0234, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 17:55:19,731 Iter 664, Minibatch Loss= 0.0205, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 17:55:20,972 Iter 665, Minibatch Loss= 0.0342, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 17:55:22,214 Iter 666, Minibatch Loss= 0.0281, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 17:55:23,441 Iter 667, Minibatch Loss= 0.0199, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:55:24,670 Iter 668, Minibatch Loss= 0.0244, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 17:55:25,923 Iter 669, Minibatch Loss= 0.0154, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 17:55:27,140 Iter 670, Minibatch Loss= 0.0146, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 17:55:28,376 Iter 671, Minibatch Loss= 0.0249, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:55:29,622 Iter 672, Minibatch Loss= 0.0249, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 17:55:30,889 Iter 673, Minibatch Loss= 0.0095, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 17:55:32,115 Iter 674, Minibatch Loss= 0.0114, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:55:33,370 Iter 675, Minibatch Loss= 0.0111, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:55:34,602 Iter 676, Minibatch Loss= 0.0251, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:55:35,831 Iter 677, Minibatch Loss= 0.0454, Training Accuracy= 0.9487, Minibatch error= 5.1%\n",
      "2018-05-27 17:55:37,085 Iter 678, Minibatch Loss= 0.0507, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:55:38,314 Iter 679, Minibatch Loss= 0.0694, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 17:55:38,315 Epoch 33, Average loss: 0.0175, learning rate: 0.0010\n",
      "2018-05-27 17:55:47,827 Iter 680, Minibatch Loss= 0.0600, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 17:55:50,012 Iter 680, Minibatch Loss= 0.0595, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 17:55:51,256 Iter 681, Minibatch Loss= 0.0353, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 17:55:52,531 Iter 682, Minibatch Loss= 0.0337, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 17:55:53,754 Iter 683, Minibatch Loss= 0.0262, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 17:55:54,960 Iter 684, Minibatch Loss= 0.0326, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 17:55:56,189 Iter 685, Minibatch Loss= 0.0192, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 17:55:57,433 Iter 686, Minibatch Loss= 0.0270, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 17:55:58,655 Iter 687, Minibatch Loss= 0.0185, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 17:55:59,902 Iter 688, Minibatch Loss= 0.0257, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:56:01,141 Iter 689, Minibatch Loss= 0.0332, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 17:56:02,391 Iter 690, Minibatch Loss= 0.0439, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 17:56:03,638 Iter 691, Minibatch Loss= 0.0746, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 17:56:04,859 Iter 692, Minibatch Loss= 0.0498, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 17:56:06,082 Iter 693, Minibatch Loss= 0.0394, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 17:56:07,330 Iter 694, Minibatch Loss= 0.0270, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 17:56:08,575 Iter 695, Minibatch Loss= 0.0079, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 17:56:09,791 Iter 696, Minibatch Loss= 0.0238, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:56:11,029 Iter 697, Minibatch Loss= 0.0226, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 17:56:12,266 Iter 698, Minibatch Loss= 0.0208, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 17:56:13,512 Iter 699, Minibatch Loss= 0.0128, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 17:56:13,513 Epoch 34, Average loss: 0.0188, learning rate: 0.0010\n",
      "2018-05-27 17:56:22,984 Iter 700, Minibatch Loss= 0.0162, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:56:25,175 Iter 700, Minibatch Loss= 0.0116, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 17:56:26,426 Iter 701, Minibatch Loss= 0.0161, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 17:56:27,676 Iter 702, Minibatch Loss= 0.0175, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 17:56:28,934 Iter 703, Minibatch Loss= 0.0266, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 17:56:30,155 Iter 704, Minibatch Loss= 0.0189, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 17:56:31,433 Iter 705, Minibatch Loss= 0.0281, Training Accuracy= 0.9793, Minibatch error= 2.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:56:32,645 Iter 706, Minibatch Loss= 0.0310, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 17:56:33,862 Iter 707, Minibatch Loss= 0.0314, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 17:56:35,111 Iter 708, Minibatch Loss= 0.0263, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 17:56:36,372 Iter 709, Minibatch Loss= 0.0133, Training Accuracy= 0.9949, Minibatch error= 0.5%\n",
      "2018-05-27 17:56:37,684 Iter 710, Minibatch Loss= 0.0238, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:56:38,990 Iter 711, Minibatch Loss= 0.0090, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 17:56:40,228 Iter 712, Minibatch Loss= 0.0282, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 17:56:41,496 Iter 713, Minibatch Loss= 0.0287, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 17:56:42,742 Iter 714, Minibatch Loss= 0.0375, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 17:56:43,977 Iter 715, Minibatch Loss= 0.0583, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 17:56:45,220 Iter 716, Minibatch Loss= 0.0615, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 17:56:46,488 Iter 717, Minibatch Loss= 0.0791, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 17:56:47,748 Iter 718, Minibatch Loss= 0.0607, Training Accuracy= 0.9532, Minibatch error= 4.7%\n",
      "2018-05-27 17:56:49,038 Iter 719, Minibatch Loss= 0.0489, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 17:56:49,039 Epoch 35, Average loss: 0.0143, learning rate: 0.0010\n",
      "2018-05-27 17:56:58,509 Iter 720, Minibatch Loss= 0.0494, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 17:57:00,741 Iter 720, Minibatch Loss= 0.0376, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 17:57:01,989 Iter 721, Minibatch Loss= 0.0338, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:57:03,328 Iter 722, Minibatch Loss= 0.0310, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:57:04,588 Iter 723, Minibatch Loss= 0.0240, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:57:05,840 Iter 724, Minibatch Loss= 0.0244, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 17:57:07,096 Iter 725, Minibatch Loss= 0.0233, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 17:57:08,341 Iter 726, Minibatch Loss= 0.0128, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 17:57:09,582 Iter 727, Minibatch Loss= 0.0195, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 17:57:10,842 Iter 728, Minibatch Loss= 0.0363, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 17:57:12,100 Iter 729, Minibatch Loss= 0.0430, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 17:57:13,333 Iter 730, Minibatch Loss= 0.0506, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 17:57:14,599 Iter 731, Minibatch Loss= 0.0760, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:57:15,839 Iter 732, Minibatch Loss= 0.0604, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 17:57:17,111 Iter 733, Minibatch Loss= 0.0743, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 17:57:18,382 Iter 734, Minibatch Loss= 0.0562, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:57:19,628 Iter 735, Minibatch Loss= 0.0382, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 17:57:20,879 Iter 736, Minibatch Loss= 0.0322, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 17:57:22,152 Iter 737, Minibatch Loss= 0.0304, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 17:57:23,396 Iter 738, Minibatch Loss= 0.0214, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 17:57:24,669 Iter 739, Minibatch Loss= 0.0246, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 17:57:24,670 Epoch 36, Average loss: 0.0142, learning rate: 0.0010\n",
      "2018-05-27 17:57:34,279 Iter 740, Minibatch Loss= 0.0229, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 17:57:36,580 Iter 740, Minibatch Loss= 0.0246, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 17:57:37,849 Iter 741, Minibatch Loss= 0.0159, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 17:57:39,165 Iter 742, Minibatch Loss= 0.0284, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 17:57:40,429 Iter 743, Minibatch Loss= 0.0328, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 17:57:41,678 Iter 744, Minibatch Loss= 0.0439, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 17:57:42,962 Iter 745, Minibatch Loss= 0.0568, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:57:44,216 Iter 746, Minibatch Loss= 0.0540, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 17:57:45,486 Iter 747, Minibatch Loss= 0.0525, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 17:57:46,732 Iter 748, Minibatch Loss= 0.0409, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 17:57:47,990 Iter 749, Minibatch Loss= 0.0392, Training Accuracy= 0.9555, Minibatch error= 4.5%\n",
      "2018-05-27 17:57:49,248 Iter 750, Minibatch Loss= 0.0311, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 17:57:50,502 Iter 751, Minibatch Loss= 0.0267, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 17:57:51,752 Iter 752, Minibatch Loss= 0.0266, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 17:57:53,041 Iter 753, Minibatch Loss= 0.0257, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:57:54,305 Iter 754, Minibatch Loss= 0.0210, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 17:57:55,551 Iter 755, Minibatch Loss= 0.0261, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 17:57:56,803 Iter 756, Minibatch Loss= 0.0282, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 17:57:58,048 Iter 757, Minibatch Loss= 0.0291, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 17:57:59,311 Iter 758, Minibatch Loss= 0.0277, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 17:58:00,561 Iter 759, Minibatch Loss= 0.0380, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 17:58:00,562 Epoch 37, Average loss: 0.0165, learning rate: 0.0010\n",
      "2018-05-27 17:58:10,042 Iter 760, Minibatch Loss= 0.0401, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 17:58:12,416 Iter 760, Minibatch Loss= 0.0498, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 17:58:13,661 Iter 761, Minibatch Loss= 0.0501, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 17:58:14,895 Iter 762, Minibatch Loss= 0.0366, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 17:58:16,150 Iter 763, Minibatch Loss= 0.0216, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 17:58:17,393 Iter 764, Minibatch Loss= 0.0198, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 17:58:18,630 Iter 765, Minibatch Loss= 0.0127, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 17:58:19,884 Iter 766, Minibatch Loss= 0.0203, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:58:21,111 Iter 767, Minibatch Loss= 0.0066, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 17:58:22,398 Iter 768, Minibatch Loss= 0.0210, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 17:58:23,650 Iter 769, Minibatch Loss= 0.0071, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 17:58:24,927 Iter 770, Minibatch Loss= 0.0226, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:58:26,169 Iter 771, Minibatch Loss= 0.0128, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 17:58:27,447 Iter 772, Minibatch Loss= 0.0228, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 17:58:28,712 Iter 773, Minibatch Loss= 0.0236, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 17:58:29,944 Iter 774, Minibatch Loss= 0.0112, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 17:58:31,197 Iter 775, Minibatch Loss= 0.0188, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:58:32,489 Iter 776, Minibatch Loss= 0.0049, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 17:58:33,738 Iter 777, Minibatch Loss= 0.0050, Training Accuracy= 0.9927, Minibatch error= 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 17:58:35,004 Iter 778, Minibatch Loss= 0.0040, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 17:58:36,252 Iter 779, Minibatch Loss= 0.0227, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 17:58:36,253 Epoch 38, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 17:58:45,757 Iter 780, Minibatch Loss= 0.0142, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 17:58:48,023 Iter 780, Minibatch Loss= 0.0142, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 17:58:49,315 Iter 781, Minibatch Loss= 0.0179, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 17:58:50,571 Iter 782, Minibatch Loss= 0.0228, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 17:58:51,867 Iter 783, Minibatch Loss= 0.0294, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 17:58:53,136 Iter 784, Minibatch Loss= 0.0700, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 17:58:54,389 Iter 785, Minibatch Loss= 0.0813, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 17:58:55,640 Iter 786, Minibatch Loss= 0.0525, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 17:58:56,924 Iter 787, Minibatch Loss= 0.0344, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 17:58:58,198 Iter 788, Minibatch Loss= 0.0302, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 17:58:59,470 Iter 789, Minibatch Loss= 0.0189, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 17:59:00,709 Iter 790, Minibatch Loss= 0.0226, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 17:59:01,959 Iter 791, Minibatch Loss= 0.0189, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:59:03,222 Iter 792, Minibatch Loss= 0.0166, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 17:59:04,500 Iter 793, Minibatch Loss= 0.0238, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:59:05,786 Iter 794, Minibatch Loss= 0.0237, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 17:59:07,035 Iter 795, Minibatch Loss= 0.0224, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 17:59:08,318 Iter 796, Minibatch Loss= 0.0256, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 17:59:09,582 Iter 797, Minibatch Loss= 0.0277, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 17:59:10,858 Iter 798, Minibatch Loss= 0.0300, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 17:59:12,106 Iter 799, Minibatch Loss= 0.0253, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 17:59:12,107 Epoch 39, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 17:59:21,635 Iter 800, Minibatch Loss= 0.0288, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 17:59:23,933 Iter 800, Minibatch Loss= 0.0180, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 17:59:25,215 Iter 801, Minibatch Loss= 0.0138, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 17:59:26,481 Iter 802, Minibatch Loss= 0.0163, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 17:59:27,746 Iter 803, Minibatch Loss= 0.0284, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 17:59:29,017 Iter 804, Minibatch Loss= 0.0227, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 17:59:30,289 Iter 805, Minibatch Loss= 0.0237, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 17:59:31,541 Iter 806, Minibatch Loss= 0.0183, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 17:59:32,800 Iter 807, Minibatch Loss= 0.0236, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 17:59:34,072 Iter 808, Minibatch Loss= 0.0238, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 17:59:35,320 Iter 809, Minibatch Loss= 0.0164, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 17:59:36,569 Iter 810, Minibatch Loss= 0.0232, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 17:59:37,858 Iter 811, Minibatch Loss= 0.0225, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 17:59:39,133 Iter 812, Minibatch Loss= 0.0223, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 17:59:40,426 Iter 813, Minibatch Loss= 0.0268, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 17:59:41,674 Iter 814, Minibatch Loss= 0.0147, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 17:59:42,985 Iter 815, Minibatch Loss= 0.0220, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 17:59:44,271 Iter 816, Minibatch Loss= 0.0265, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 17:59:45,567 Iter 817, Minibatch Loss= 0.0237, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 17:59:46,828 Iter 818, Minibatch Loss= 0.0300, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 17:59:48,107 Iter 819, Minibatch Loss= 0.0340, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 17:59:48,108 Epoch 40, Average loss: 0.0184, learning rate: 0.0010\n",
      "2018-05-27 17:59:57,637 Iter 820, Minibatch Loss= 0.0296, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 17:59:59,968 Iter 820, Minibatch Loss= 0.0265, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:00:01,261 Iter 821, Minibatch Loss= 0.0349, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:00:02,536 Iter 822, Minibatch Loss= 0.0361, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 18:00:03,822 Iter 823, Minibatch Loss= 0.0404, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:00:05,071 Iter 824, Minibatch Loss= 0.0453, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 18:00:06,341 Iter 825, Minibatch Loss= 0.0358, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 18:00:07,586 Iter 826, Minibatch Loss= 0.0134, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 18:00:08,846 Iter 827, Minibatch Loss= 0.0079, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 18:00:10,116 Iter 828, Minibatch Loss= 0.0114, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 18:00:11,403 Iter 829, Minibatch Loss= 0.0210, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 18:00:12,652 Iter 830, Minibatch Loss= 0.0083, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:00:13,953 Iter 831, Minibatch Loss= 0.0217, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:00:15,215 Iter 832, Minibatch Loss= 0.0220, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:00:16,500 Iter 833, Minibatch Loss= 0.0172, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 18:00:17,761 Iter 834, Minibatch Loss= 0.0200, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 18:00:19,085 Iter 835, Minibatch Loss= 0.0350, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 18:00:20,345 Iter 836, Minibatch Loss= 0.0449, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 18:00:21,626 Iter 837, Minibatch Loss= 0.0288, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:00:22,900 Iter 838, Minibatch Loss= 0.0279, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:00:24,190 Iter 839, Minibatch Loss= 0.0242, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:00:24,191 Epoch 41, Average loss: 0.0179, learning rate: 0.0010\n",
      "2018-05-27 18:00:33,750 Iter 840, Minibatch Loss= 0.0209, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 18:00:36,112 Iter 840, Minibatch Loss= 0.0258, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 18:00:37,405 Iter 841, Minibatch Loss= 0.0257, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 18:00:38,681 Iter 842, Minibatch Loss= 0.0185, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:00:39,944 Iter 843, Minibatch Loss= 0.0258, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:00:41,210 Iter 844, Minibatch Loss= 0.0138, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:00:42,500 Iter 845, Minibatch Loss= 0.0198, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:00:43,761 Iter 846, Minibatch Loss= 0.0150, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:00:45,053 Iter 847, Minibatch Loss= 0.0236, Training Accuracy= 0.9680, Minibatch error= 3.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:00:46,333 Iter 848, Minibatch Loss= 0.0236, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 18:00:47,608 Iter 849, Minibatch Loss= 0.0140, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:00:48,883 Iter 850, Minibatch Loss= 0.0243, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 18:00:50,160 Iter 851, Minibatch Loss= 0.0133, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:00:51,432 Iter 852, Minibatch Loss= 0.0084, Training Accuracy= 0.9910, Minibatch error= 0.9%\n",
      "2018-05-27 18:00:52,701 Iter 853, Minibatch Loss= 0.0040, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 18:00:53,975 Iter 854, Minibatch Loss= 0.0095, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:00:55,207 Iter 855, Minibatch Loss= 0.0089, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:00:56,458 Iter 856, Minibatch Loss= 0.0262, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:00:57,727 Iter 857, Minibatch Loss= 0.0064, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 18:00:58,969 Iter 858, Minibatch Loss= 0.0098, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 18:01:00,210 Iter 859, Minibatch Loss= 0.0227, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:01:00,211 Epoch 42, Average loss: 0.0146, learning rate: 0.0010\n",
      "2018-05-27 18:01:09,729 Iter 860, Minibatch Loss= 0.0157, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 18:01:12,104 Iter 860, Minibatch Loss= 0.0228, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 18:01:13,357 Iter 861, Minibatch Loss= 0.0181, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:01:14,616 Iter 862, Minibatch Loss= 0.0270, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:01:15,888 Iter 863, Minibatch Loss= 0.0212, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:01:17,155 Iter 864, Minibatch Loss= 0.0219, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:01:18,399 Iter 865, Minibatch Loss= 0.0217, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:01:19,667 Iter 866, Minibatch Loss= 0.0123, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:01:20,919 Iter 867, Minibatch Loss= 0.0069, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 18:01:22,186 Iter 868, Minibatch Loss= 0.0103, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:01:23,444 Iter 869, Minibatch Loss= 0.0077, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 18:01:24,708 Iter 870, Minibatch Loss= 0.0264, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 18:01:25,974 Iter 871, Minibatch Loss= 0.0070, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:01:27,233 Iter 872, Minibatch Loss= 0.0210, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:01:28,483 Iter 873, Minibatch Loss= 0.0193, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:01:29,761 Iter 874, Minibatch Loss= 0.0187, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:01:31,002 Iter 875, Minibatch Loss= 0.0248, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 18:01:32,271 Iter 876, Minibatch Loss= 0.0239, Training Accuracy= 0.9573, Minibatch error= 4.3%\n",
      "2018-05-27 18:01:33,534 Iter 877, Minibatch Loss= 0.0039, Training Accuracy= 0.9977, Minibatch error= 0.2%\n",
      "2018-05-27 18:01:34,819 Iter 878, Minibatch Loss= 0.0270, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 18:01:36,087 Iter 879, Minibatch Loss= 0.0313, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:01:36,088 Epoch 43, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 18:01:45,604 Iter 880, Minibatch Loss= 0.0310, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 18:01:47,963 Iter 880, Minibatch Loss= 0.0328, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:01:49,227 Iter 881, Minibatch Loss= 0.0245, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:01:50,512 Iter 882, Minibatch Loss= 0.0245, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 18:01:51,773 Iter 883, Minibatch Loss= 0.0300, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 18:01:53,047 Iter 884, Minibatch Loss= 0.0298, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:01:54,312 Iter 885, Minibatch Loss= 0.0370, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 18:01:55,565 Iter 886, Minibatch Loss= 0.0392, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 18:01:56,824 Iter 887, Minibatch Loss= 0.0405, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 18:01:58,138 Iter 888, Minibatch Loss= 0.0341, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:01:59,414 Iter 889, Minibatch Loss= 0.0213, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:02:00,713 Iter 890, Minibatch Loss= 0.0166, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 18:02:01,969 Iter 891, Minibatch Loss= 0.0108, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 18:02:03,235 Iter 892, Minibatch Loss= 0.0152, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 18:02:04,500 Iter 893, Minibatch Loss= 0.0226, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:02:05,773 Iter 894, Minibatch Loss= 0.0083, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:02:07,038 Iter 895, Minibatch Loss= 0.0185, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 18:02:08,284 Iter 896, Minibatch Loss= 0.0219, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:02:09,548 Iter 897, Minibatch Loss= 0.0133, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:02:10,791 Iter 898, Minibatch Loss= 0.0206, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:02:12,045 Iter 899, Minibatch Loss= 0.0194, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:02:12,046 Epoch 44, Average loss: 0.0171, learning rate: 0.0010\n",
      "2018-05-27 18:02:21,581 Iter 900, Minibatch Loss= 0.0151, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 18:02:24,000 Iter 900, Minibatch Loss= 0.0223, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:02:25,264 Iter 901, Minibatch Loss= 0.0192, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:02:26,538 Iter 902, Minibatch Loss= 0.0138, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 18:02:27,814 Iter 903, Minibatch Loss= 0.0235, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 18:02:29,085 Iter 904, Minibatch Loss= 0.0184, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:02:30,359 Iter 905, Minibatch Loss= 0.0196, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 18:02:31,645 Iter 906, Minibatch Loss= 0.0212, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:02:32,904 Iter 907, Minibatch Loss= 0.0111, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 18:02:34,165 Iter 908, Minibatch Loss= 0.0196, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:02:35,449 Iter 909, Minibatch Loss= 0.0224, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 18:02:36,722 Iter 910, Minibatch Loss= 0.0138, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 18:02:37,991 Iter 911, Minibatch Loss= 0.0220, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 18:02:39,257 Iter 912, Minibatch Loss= 0.0077, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 18:02:40,532 Iter 913, Minibatch Loss= 0.0091, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:02:41,786 Iter 914, Minibatch Loss= 0.0075, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:02:43,037 Iter 915, Minibatch Loss= 0.0199, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:02:44,291 Iter 916, Minibatch Loss= 0.0119, Training Accuracy= 0.9795, Minibatch error= 2.1%\n",
      "2018-05-27 18:02:45,559 Iter 917, Minibatch Loss= 0.0057, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 18:02:46,838 Iter 918, Minibatch Loss= 0.0087, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:02:48,118 Iter 919, Minibatch Loss= 0.0214, Training Accuracy= 0.9683, Minibatch error= 3.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:02:48,119 Epoch 45, Average loss: 0.0165, learning rate: 0.0010\n",
      "2018-05-27 18:02:57,605 Iter 920, Minibatch Loss= 0.0144, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 18:03:00,001 Iter 920, Minibatch Loss= 0.0208, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 18:03:01,270 Iter 921, Minibatch Loss= 0.0258, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 18:03:02,524 Iter 922, Minibatch Loss= 0.0193, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:03:03,804 Iter 923, Minibatch Loss= 0.0056, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 18:03:05,063 Iter 924, Minibatch Loss= 0.0207, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 18:03:06,321 Iter 925, Minibatch Loss= 0.0197, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:03:07,585 Iter 926, Minibatch Loss= 0.0095, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 18:03:08,855 Iter 927, Minibatch Loss= 0.0198, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:03:10,134 Iter 928, Minibatch Loss= 0.0103, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 18:03:11,427 Iter 929, Minibatch Loss= 0.0144, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 18:03:12,692 Iter 930, Minibatch Loss= 0.0106, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 18:03:13,964 Iter 931, Minibatch Loss= 0.0219, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:03:15,228 Iter 932, Minibatch Loss= 0.0149, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 18:03:16,529 Iter 933, Minibatch Loss= 0.0157, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 18:03:17,801 Iter 934, Minibatch Loss= 0.0073, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 18:03:19,109 Iter 935, Minibatch Loss= 0.0062, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 18:03:20,369 Iter 936, Minibatch Loss= 0.0136, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:03:21,638 Iter 937, Minibatch Loss= 0.0203, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:03:22,909 Iter 938, Minibatch Loss= 0.0031, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 18:03:24,227 Iter 939, Minibatch Loss= 0.0189, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:03:24,228 Epoch 46, Average loss: 0.0153, learning rate: 0.0010\n",
      "2018-05-27 18:03:33,705 Iter 940, Minibatch Loss= 0.0136, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:03:36,321 Iter 940, Minibatch Loss= 0.0202, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:03:37,615 Iter 941, Minibatch Loss= 0.0205, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 18:03:38,900 Iter 942, Minibatch Loss= 0.0226, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 18:03:40,162 Iter 943, Minibatch Loss= 0.0206, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 18:03:41,449 Iter 944, Minibatch Loss= 0.0139, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:03:42,720 Iter 945, Minibatch Loss= 0.0195, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:03:43,985 Iter 946, Minibatch Loss= 0.0087, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 18:03:45,238 Iter 947, Minibatch Loss= 0.0130, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 18:03:46,535 Iter 948, Minibatch Loss= 0.0063, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 18:03:47,808 Iter 949, Minibatch Loss= 0.0227, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 18:03:49,100 Iter 950, Minibatch Loss= 0.0153, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 18:03:50,387 Iter 951, Minibatch Loss= 0.0086, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:03:51,706 Iter 952, Minibatch Loss= 0.0023, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 18:03:52,976 Iter 953, Minibatch Loss= 0.0078, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 18:03:54,236 Iter 954, Minibatch Loss= 0.0206, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:03:55,514 Iter 955, Minibatch Loss= 0.0101, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:03:56,781 Iter 956, Minibatch Loss= 0.0204, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:03:58,052 Iter 957, Minibatch Loss= 0.0185, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:03:59,314 Iter 958, Minibatch Loss= 0.0099, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 18:04:00,597 Iter 959, Minibatch Loss= 0.0075, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 18:04:00,598 Epoch 47, Average loss: 0.0148, learning rate: 0.0010\n",
      "2018-05-27 18:04:10,167 Iter 960, Minibatch Loss= 0.0144, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:04:12,695 Iter 960, Minibatch Loss= 0.0244, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:04:13,986 Iter 961, Minibatch Loss= 0.0135, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 18:04:15,273 Iter 962, Minibatch Loss= 0.0092, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:04:16,552 Iter 963, Minibatch Loss= 0.0141, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 18:04:17,834 Iter 964, Minibatch Loss= 0.0182, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:04:19,125 Iter 965, Minibatch Loss= 0.0153, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:04:20,410 Iter 966, Minibatch Loss= 0.0205, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:04:21,696 Iter 967, Minibatch Loss= 0.0116, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 18:04:22,987 Iter 968, Minibatch Loss= 0.0197, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:04:24,278 Iter 969, Minibatch Loss= 0.0127, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:04:25,558 Iter 970, Minibatch Loss= 0.0155, Training Accuracy= 0.9755, Minibatch error= 2.4%\n",
      "2018-05-27 18:04:26,876 Iter 971, Minibatch Loss= 0.0110, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:04:28,165 Iter 972, Minibatch Loss= 0.0121, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:04:29,446 Iter 973, Minibatch Loss= 0.0179, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 18:04:30,739 Iter 974, Minibatch Loss= 0.0207, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:04:32,011 Iter 975, Minibatch Loss= 0.0173, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:04:33,302 Iter 976, Minibatch Loss= 0.0119, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:04:34,574 Iter 977, Minibatch Loss= 0.0106, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:04:35,860 Iter 978, Minibatch Loss= 0.0355, Training Accuracy= 0.8914, Minibatch error= 10.9%\n",
      "2018-05-27 18:04:37,150 Iter 979, Minibatch Loss= 0.0288, Training Accuracy= 0.9176, Minibatch error= 8.2%\n",
      "2018-05-27 18:04:37,151 Epoch 48, Average loss: 0.0165, learning rate: 0.0010\n",
      "2018-05-27 18:04:46,720 Iter 980, Minibatch Loss= 0.0213, Training Accuracy= 0.9396, Minibatch error= 6.0%\n",
      "2018-05-27 18:04:49,209 Iter 980, Minibatch Loss= 0.0140, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:04:50,497 Iter 981, Minibatch Loss= 0.0243, Training Accuracy= 0.9559, Minibatch error= 4.4%\n",
      "2018-05-27 18:04:51,845 Iter 982, Minibatch Loss= 0.0158, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:04:53,120 Iter 983, Minibatch Loss= 0.0128, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 18:04:54,438 Iter 984, Minibatch Loss= 0.0048, Training Accuracy= 0.9943, Minibatch error= 0.6%\n",
      "2018-05-27 18:04:55,704 Iter 985, Minibatch Loss= 0.0080, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 18:04:57,004 Iter 986, Minibatch Loss= 0.0176, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:04:58,288 Iter 987, Minibatch Loss= 0.0159, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 18:04:59,594 Iter 988, Minibatch Loss= 0.0223, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:05:00,879 Iter 989, Minibatch Loss= 0.0051, Training Accuracy= 0.9932, Minibatch error= 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:05:02,178 Iter 990, Minibatch Loss= 0.0219, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:05:03,454 Iter 991, Minibatch Loss= 0.0306, Training Accuracy= 0.9521, Minibatch error= 4.8%\n",
      "2018-05-27 18:05:04,708 Iter 992, Minibatch Loss= 0.0367, Training Accuracy= 0.9505, Minibatch error= 5.0%\n",
      "2018-05-27 18:05:06,005 Iter 993, Minibatch Loss= 0.0421, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 18:05:07,305 Iter 994, Minibatch Loss= 0.0329, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 18:05:08,598 Iter 995, Minibatch Loss= 0.0252, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 18:05:09,862 Iter 996, Minibatch Loss= 0.0256, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:05:11,137 Iter 997, Minibatch Loss= 0.0233, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:05:12,437 Iter 998, Minibatch Loss= 0.0108, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 18:05:13,731 Iter 999, Minibatch Loss= 0.0207, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:05:13,732 Epoch 49, Average loss: 0.0183, learning rate: 0.0010\n",
      "2018-05-27 18:05:23,250 Iter 1000, Minibatch Loss= 0.0148, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 18:05:25,768 Iter 1000, Minibatch Loss= 0.0109, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:05:27,038 Iter 1001, Minibatch Loss= 0.0207, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:05:28,336 Iter 1002, Minibatch Loss= 0.0139, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:05:29,623 Iter 1003, Minibatch Loss= 0.0100, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:05:30,898 Iter 1004, Minibatch Loss= 0.0210, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:05:32,179 Iter 1005, Minibatch Loss= 0.0099, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:05:33,456 Iter 1006, Minibatch Loss= 0.0129, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:05:34,738 Iter 1007, Minibatch Loss= 0.0117, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 18:05:35,993 Iter 1008, Minibatch Loss= 0.0186, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:05:37,267 Iter 1009, Minibatch Loss= 0.0139, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 18:05:38,578 Iter 1010, Minibatch Loss= 0.0102, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:05:39,852 Iter 1011, Minibatch Loss= 0.0070, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 18:05:41,126 Iter 1012, Minibatch Loss= 0.0193, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 18:05:42,402 Iter 1013, Minibatch Loss= 0.0137, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:05:43,681 Iter 1014, Minibatch Loss= 0.0094, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:05:44,956 Iter 1015, Minibatch Loss= 0.0195, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:05:46,242 Iter 1016, Minibatch Loss= 0.0205, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:05:47,535 Iter 1017, Minibatch Loss= 0.0143, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 18:05:48,832 Iter 1018, Minibatch Loss= 0.0111, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:05:50,116 Iter 1019, Minibatch Loss= 0.0194, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:05:50,117 Epoch 50, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 18:05:59,625 Iter 1020, Minibatch Loss= 0.0145, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 18:06:02,148 Iter 1020, Minibatch Loss= 0.0057, Training Accuracy= 0.9918, Minibatch error= 0.8%\n",
      "2018-05-27 18:06:03,433 Iter 1021, Minibatch Loss= 0.0231, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 18:06:04,720 Iter 1022, Minibatch Loss= 0.0101, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 18:06:06,010 Iter 1023, Minibatch Loss= 0.0119, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 18:06:07,294 Iter 1024, Minibatch Loss= 0.0090, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:06:08,580 Iter 1025, Minibatch Loss= 0.0217, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:06:09,856 Iter 1026, Minibatch Loss= 0.0208, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 18:06:11,140 Iter 1027, Minibatch Loss= 0.0042, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:06:12,427 Iter 1028, Minibatch Loss= 0.0228, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:06:13,728 Iter 1029, Minibatch Loss= 0.0219, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:06:15,019 Iter 1030, Minibatch Loss= 0.0059, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:06:16,303 Iter 1031, Minibatch Loss= 0.0192, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:06:17,578 Iter 1032, Minibatch Loss= 0.0044, Training Accuracy= 0.9938, Minibatch error= 0.6%\n",
      "2018-05-27 18:06:18,881 Iter 1033, Minibatch Loss= 0.0188, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:06:20,160 Iter 1034, Minibatch Loss= 0.0113, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:06:21,458 Iter 1035, Minibatch Loss= 0.0193, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:06:22,746 Iter 1036, Minibatch Loss= 0.0237, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 18:06:24,042 Iter 1037, Minibatch Loss= 0.0408, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 18:06:25,322 Iter 1038, Minibatch Loss= 0.0364, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:06:26,616 Iter 1039, Minibatch Loss= 0.0295, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 18:06:26,617 Epoch 51, Average loss: 0.0171, learning rate: 0.0010\n",
      "2018-05-27 18:06:36,147 Iter 1040, Minibatch Loss= 0.0264, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 18:06:38,749 Iter 1040, Minibatch Loss= 0.0225, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:06:40,045 Iter 1041, Minibatch Loss= 0.0244, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:06:41,364 Iter 1042, Minibatch Loss= 0.0137, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 18:06:42,660 Iter 1043, Minibatch Loss= 0.0070, Training Accuracy= 0.9978, Minibatch error= 0.2%\n",
      "2018-05-27 18:06:43,972 Iter 1044, Minibatch Loss= 0.0237, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:06:45,263 Iter 1045, Minibatch Loss= 0.0228, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 18:06:46,563 Iter 1046, Minibatch Loss= 0.0244, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:06:47,853 Iter 1047, Minibatch Loss= 0.0103, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 18:06:49,156 Iter 1048, Minibatch Loss= 0.0204, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:06:50,461 Iter 1049, Minibatch Loss= 0.0176, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 18:06:51,757 Iter 1050, Minibatch Loss= 0.0237, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:06:53,054 Iter 1051, Minibatch Loss= 0.0214, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:06:54,359 Iter 1052, Minibatch Loss= 0.0189, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 18:06:55,653 Iter 1053, Minibatch Loss= 0.0106, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:06:56,944 Iter 1054, Minibatch Loss= 0.0113, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 18:06:58,235 Iter 1055, Minibatch Loss= 0.0204, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 18:06:59,524 Iter 1056, Minibatch Loss= 0.0200, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:07:00,806 Iter 1057, Minibatch Loss= 0.0190, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:07:02,114 Iter 1058, Minibatch Loss= 0.0185, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 18:07:03,406 Iter 1059, Minibatch Loss= 0.0101, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 18:07:03,407 Epoch 52, Average loss: 0.0162, learning rate: 0.0010\n",
      "2018-05-27 18:07:12,925 Iter 1060, Minibatch Loss= 0.0137, Training Accuracy= 0.9772, Minibatch error= 2.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:07:15,529 Iter 1060, Minibatch Loss= 0.0049, Training Accuracy= 0.9925, Minibatch error= 0.8%\n",
      "2018-05-27 18:07:16,828 Iter 1061, Minibatch Loss= 0.0184, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:07:18,109 Iter 1062, Minibatch Loss= 0.0065, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 18:07:19,445 Iter 1063, Minibatch Loss= 0.0123, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 18:07:20,730 Iter 1064, Minibatch Loss= 0.0201, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:07:22,018 Iter 1065, Minibatch Loss= 0.0173, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:07:23,322 Iter 1066, Minibatch Loss= 0.0118, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:07:24,621 Iter 1067, Minibatch Loss= 0.0187, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:07:25,913 Iter 1068, Minibatch Loss= 0.0108, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:07:27,230 Iter 1069, Minibatch Loss= 0.0165, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:07:28,531 Iter 1070, Minibatch Loss= 0.0078, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:07:29,821 Iter 1071, Minibatch Loss= 0.0135, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 18:07:31,099 Iter 1072, Minibatch Loss= 0.0100, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:07:32,429 Iter 1073, Minibatch Loss= 0.0034, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 18:07:33,730 Iter 1074, Minibatch Loss= 0.0046, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 18:07:35,040 Iter 1075, Minibatch Loss= 0.0085, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:07:36,337 Iter 1076, Minibatch Loss= 0.0098, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:07:37,669 Iter 1077, Minibatch Loss= 0.0229, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:07:38,982 Iter 1078, Minibatch Loss= 0.0218, Training Accuracy= 0.9635, Minibatch error= 3.6%\n",
      "2018-05-27 18:07:40,287 Iter 1079, Minibatch Loss= 0.0202, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:07:40,288 Epoch 53, Average loss: 0.0136, learning rate: 0.0010\n",
      "2018-05-27 18:07:49,767 Iter 1080, Minibatch Loss= 0.0137, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 18:07:52,337 Iter 1080, Minibatch Loss= 0.0083, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 18:07:53,636 Iter 1081, Minibatch Loss= 0.0184, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:07:54,951 Iter 1082, Minibatch Loss= 0.0175, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:07:56,251 Iter 1083, Minibatch Loss= 0.0145, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:07:57,565 Iter 1084, Minibatch Loss= 0.0093, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 18:07:58,852 Iter 1085, Minibatch Loss= 0.0046, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 18:08:00,149 Iter 1086, Minibatch Loss= 0.0193, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:08:01,450 Iter 1087, Minibatch Loss= 0.0143, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 18:08:02,755 Iter 1088, Minibatch Loss= 0.0281, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 18:08:04,067 Iter 1089, Minibatch Loss= 0.0108, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 18:08:05,391 Iter 1090, Minibatch Loss= 0.0179, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:08:06,689 Iter 1091, Minibatch Loss= 0.0172, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 18:08:08,009 Iter 1092, Minibatch Loss= 0.0195, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:08:09,305 Iter 1093, Minibatch Loss= 0.0085, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:08:10,634 Iter 1094, Minibatch Loss= 0.0211, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 18:08:11,928 Iter 1095, Minibatch Loss= 0.0222, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:08:13,250 Iter 1096, Minibatch Loss= 0.0215, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 18:08:14,553 Iter 1097, Minibatch Loss= 0.0153, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 18:08:15,849 Iter 1098, Minibatch Loss= 0.0179, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:08:17,164 Iter 1099, Minibatch Loss= 0.0241, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 18:08:17,165 Epoch 54, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 18:08:26,680 Iter 1100, Minibatch Loss= 0.0138, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 18:08:29,317 Iter 1100, Minibatch Loss= 0.0016, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 18:08:30,630 Iter 1101, Minibatch Loss= 0.0056, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 18:08:31,920 Iter 1102, Minibatch Loss= 0.0210, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 18:08:33,250 Iter 1103, Minibatch Loss= 0.0108, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:08:34,571 Iter 1104, Minibatch Loss= 0.0192, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 18:08:35,889 Iter 1105, Minibatch Loss= 0.0143, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 18:08:37,200 Iter 1106, Minibatch Loss= 0.0180, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:08:38,526 Iter 1107, Minibatch Loss= 0.0056, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 18:08:39,852 Iter 1108, Minibatch Loss= 0.0175, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 18:08:41,168 Iter 1109, Minibatch Loss= 0.0075, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 18:08:42,490 Iter 1110, Minibatch Loss= 0.0072, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:08:43,783 Iter 1111, Minibatch Loss= 0.0070, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 18:08:45,086 Iter 1112, Minibatch Loss= 0.0201, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:08:46,388 Iter 1113, Minibatch Loss= 0.0050, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:08:47,696 Iter 1114, Minibatch Loss= 0.0061, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:08:49,003 Iter 1115, Minibatch Loss= 0.0088, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:08:50,349 Iter 1116, Minibatch Loss= 0.0166, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:08:51,688 Iter 1117, Minibatch Loss= 0.0130, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:08:53,018 Iter 1118, Minibatch Loss= 0.0203, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:08:54,337 Iter 1119, Minibatch Loss= 0.0030, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 18:08:54,338 Epoch 55, Average loss: 0.0117, learning rate: 0.0010\n",
      "2018-05-27 18:09:03,867 Iter 1120, Minibatch Loss= 0.0128, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 18:09:06,635 Iter 1120, Minibatch Loss= 0.0062, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:09:07,998 Iter 1121, Minibatch Loss= 0.0149, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 18:09:09,343 Iter 1122, Minibatch Loss= 0.0045, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 18:09:10,666 Iter 1123, Minibatch Loss= 0.0073, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 18:09:12,006 Iter 1124, Minibatch Loss= 0.0204, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:09:13,337 Iter 1125, Minibatch Loss= 0.0065, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 18:09:14,677 Iter 1126, Minibatch Loss= 0.0187, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:09:15,994 Iter 1127, Minibatch Loss= 0.0060, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 18:09:17,336 Iter 1128, Minibatch Loss= 0.0182, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 18:09:18,668 Iter 1129, Minibatch Loss= 0.0042, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 18:09:20,002 Iter 1130, Minibatch Loss= 0.0102, Training Accuracy= 0.9825, Minibatch error= 1.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:09:21,336 Iter 1131, Minibatch Loss= 0.0167, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:09:22,693 Iter 1132, Minibatch Loss= 0.0133, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 18:09:24,032 Iter 1133, Minibatch Loss= 0.0078, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:09:25,364 Iter 1134, Minibatch Loss= 0.0032, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 18:09:26,691 Iter 1135, Minibatch Loss= 0.0166, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:09:28,049 Iter 1136, Minibatch Loss= 0.0207, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:09:29,369 Iter 1137, Minibatch Loss= 0.0205, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 18:09:30,718 Iter 1138, Minibatch Loss= 0.0147, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:09:32,020 Iter 1139, Minibatch Loss= 0.0049, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:09:32,021 Epoch 56, Average loss: 0.0123, learning rate: 0.0010\n",
      "2018-05-27 18:09:41,531 Iter 1140, Minibatch Loss= 0.0143, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:09:44,178 Iter 1140, Minibatch Loss= 0.0100, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 18:09:45,529 Iter 1141, Minibatch Loss= 0.0082, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:09:46,836 Iter 1142, Minibatch Loss= 0.0139, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 18:09:48,156 Iter 1143, Minibatch Loss= 0.0178, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:09:49,469 Iter 1144, Minibatch Loss= 0.0207, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:09:50,815 Iter 1145, Minibatch Loss= 0.0101, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 18:09:52,110 Iter 1146, Minibatch Loss= 0.0020, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 18:09:53,453 Iter 1147, Minibatch Loss= 0.0214, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:09:54,763 Iter 1148, Minibatch Loss= 0.0162, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:09:56,056 Iter 1149, Minibatch Loss= 0.0199, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:09:57,394 Iter 1150, Minibatch Loss= 0.0062, Training Accuracy= 0.9895, Minibatch error= 1.1%\n",
      "2018-05-27 18:09:58,699 Iter 1151, Minibatch Loss= 0.0157, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:10:00,035 Iter 1152, Minibatch Loss= 0.0039, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 18:10:01,324 Iter 1153, Minibatch Loss= 0.0224, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 18:10:02,647 Iter 1154, Minibatch Loss= 0.0203, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 18:10:03,971 Iter 1155, Minibatch Loss= 0.0031, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 18:10:05,259 Iter 1156, Minibatch Loss= 0.0177, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:10:06,574 Iter 1157, Minibatch Loss= 0.0101, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:10:07,905 Iter 1158, Minibatch Loss= 0.0183, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:10:09,234 Iter 1159, Minibatch Loss= 0.0114, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:10:09,235 Epoch 57, Average loss: 0.0138, learning rate: 0.0010\n",
      "2018-05-27 18:10:18,739 Iter 1160, Minibatch Loss= 0.0130, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:10:21,451 Iter 1160, Minibatch Loss= 0.0202, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:10:22,794 Iter 1161, Minibatch Loss= 0.0162, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:10:24,129 Iter 1162, Minibatch Loss= 0.0154, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:10:25,469 Iter 1163, Minibatch Loss= 0.0191, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:10:26,771 Iter 1164, Minibatch Loss= 0.0104, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 18:10:28,100 Iter 1165, Minibatch Loss= 0.0066, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 18:10:29,418 Iter 1166, Minibatch Loss= 0.0213, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 18:10:30,746 Iter 1167, Minibatch Loss= 0.0203, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 18:10:32,046 Iter 1168, Minibatch Loss= 0.0198, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:10:33,352 Iter 1169, Minibatch Loss= 0.0188, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 18:10:34,689 Iter 1170, Minibatch Loss= 0.0171, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:10:36,030 Iter 1171, Minibatch Loss= 0.0177, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:10:37,343 Iter 1172, Minibatch Loss= 0.0080, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:10:38,701 Iter 1173, Minibatch Loss= 0.0191, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:10:40,031 Iter 1174, Minibatch Loss= 0.0191, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:10:41,358 Iter 1175, Minibatch Loss= 0.0155, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:10:42,664 Iter 1176, Minibatch Loss= 0.0177, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 18:10:43,950 Iter 1177, Minibatch Loss= 0.0198, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:10:45,252 Iter 1178, Minibatch Loss= 0.0071, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 18:10:46,595 Iter 1179, Minibatch Loss= 0.0122, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 18:10:46,596 Epoch 58, Average loss: 0.0164, learning rate: 0.0010\n",
      "2018-05-27 18:10:56,116 Iter 1180, Minibatch Loss= 0.0129, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:10:58,802 Iter 1180, Minibatch Loss= 0.0047, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 18:11:00,100 Iter 1181, Minibatch Loss= 0.0075, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 18:11:01,395 Iter 1182, Minibatch Loss= 0.0182, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:11:02,700 Iter 1183, Minibatch Loss= 0.0169, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:11:04,033 Iter 1184, Minibatch Loss= 0.0083, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:11:05,340 Iter 1185, Minibatch Loss= 0.0152, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:11:06,681 Iter 1186, Minibatch Loss= 0.0196, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:11:08,011 Iter 1187, Minibatch Loss= 0.0120, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:11:09,306 Iter 1188, Minibatch Loss= 0.0161, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:11:10,630 Iter 1189, Minibatch Loss= 0.0201, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:11:11,935 Iter 1190, Minibatch Loss= 0.0151, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 18:11:13,250 Iter 1191, Minibatch Loss= 0.0148, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:11:14,570 Iter 1192, Minibatch Loss= 0.0089, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 18:11:15,899 Iter 1193, Minibatch Loss= 0.0085, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:11:17,207 Iter 1194, Minibatch Loss= 0.0201, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:11:18,520 Iter 1195, Minibatch Loss= 0.0064, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 18:11:19,823 Iter 1196, Minibatch Loss= 0.0109, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 18:11:21,127 Iter 1197, Minibatch Loss= 0.0208, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:11:22,446 Iter 1198, Minibatch Loss= 0.0096, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 18:11:23,771 Iter 1199, Minibatch Loss= 0.0034, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 18:11:23,772 Epoch 59, Average loss: 0.0134, learning rate: 0.0010\n",
      "2018-05-27 18:11:33,196 Iter 1200, Minibatch Loss= 0.0128, Training Accuracy= 0.9785, Minibatch error= 2.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:11:35,886 Iter 1200, Minibatch Loss= 0.0189, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:11:37,189 Iter 1201, Minibatch Loss= 0.0082, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:11:38,531 Iter 1202, Minibatch Loss= 0.0076, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:11:39,854 Iter 1203, Minibatch Loss= 0.0016, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 18:11:41,184 Iter 1204, Minibatch Loss= 0.0110, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 18:11:42,510 Iter 1205, Minibatch Loss= 0.0231, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:11:43,840 Iter 1206, Minibatch Loss= 0.0175, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:11:45,158 Iter 1207, Minibatch Loss= 0.0113, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:11:46,459 Iter 1208, Minibatch Loss= 0.0099, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:11:47,753 Iter 1209, Minibatch Loss= 0.0213, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 18:11:49,074 Iter 1210, Minibatch Loss= 0.0164, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 18:11:50,373 Iter 1211, Minibatch Loss= 0.0016, Training Accuracy= 0.9986, Minibatch error= 0.1%\n",
      "2018-05-27 18:11:51,680 Iter 1212, Minibatch Loss= 0.0082, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:11:52,981 Iter 1213, Minibatch Loss= 0.0118, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 18:11:54,276 Iter 1214, Minibatch Loss= 0.0190, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 18:11:55,583 Iter 1215, Minibatch Loss= 0.0096, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 18:11:56,874 Iter 1216, Minibatch Loss= 0.0088, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:11:58,156 Iter 1217, Minibatch Loss= 0.0177, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:11:59,488 Iter 1218, Minibatch Loss= 0.0184, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:12:00,790 Iter 1219, Minibatch Loss= 0.0178, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:12:00,791 Epoch 60, Average loss: 0.0136, learning rate: 0.0010\n",
      "2018-05-27 18:12:10,224 Iter 1220, Minibatch Loss= 0.0132, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:12:12,869 Iter 1220, Minibatch Loss= 0.0184, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:12:14,165 Iter 1221, Minibatch Loss= 0.0090, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:12:15,468 Iter 1222, Minibatch Loss= 0.0071, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 18:12:16,756 Iter 1223, Minibatch Loss= 0.0195, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:12:18,059 Iter 1224, Minibatch Loss= 0.0099, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:12:19,407 Iter 1225, Minibatch Loss= 0.0173, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:12:20,707 Iter 1226, Minibatch Loss= 0.0198, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:12:22,037 Iter 1227, Minibatch Loss= 0.0060, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 18:12:23,363 Iter 1228, Minibatch Loss= 0.0215, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:12:24,684 Iter 1229, Minibatch Loss= 0.0044, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:12:25,997 Iter 1230, Minibatch Loss= 0.0171, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:12:27,338 Iter 1231, Minibatch Loss= 0.0169, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:12:28,647 Iter 1232, Minibatch Loss= 0.0122, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 18:12:29,969 Iter 1233, Minibatch Loss= 0.0261, Training Accuracy= 0.9320, Minibatch error= 6.8%\n",
      "2018-05-27 18:12:31,285 Iter 1234, Minibatch Loss= 0.0200, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 18:12:32,612 Iter 1235, Minibatch Loss= 0.0250, Training Accuracy= 0.9553, Minibatch error= 4.5%\n",
      "2018-05-27 18:12:33,917 Iter 1236, Minibatch Loss= 0.0068, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 18:12:35,211 Iter 1237, Minibatch Loss= 0.0097, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 18:12:36,530 Iter 1238, Minibatch Loss= 0.0079, Training Accuracy= 0.9899, Minibatch error= 1.0%\n",
      "2018-05-27 18:12:37,854 Iter 1239, Minibatch Loss= 0.0133, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:12:37,855 Epoch 61, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 18:12:47,313 Iter 1240, Minibatch Loss= 0.0182, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:12:50,027 Iter 1240, Minibatch Loss= 0.0373, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:12:51,359 Iter 1241, Minibatch Loss= 0.0169, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 18:12:52,737 Iter 1242, Minibatch Loss= 0.0172, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:12:54,075 Iter 1243, Minibatch Loss= 0.0079, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 18:12:55,384 Iter 1244, Minibatch Loss= 0.0076, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 18:12:56,726 Iter 1245, Minibatch Loss= 0.0050, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 18:12:58,060 Iter 1246, Minibatch Loss= 0.0302, Training Accuracy= 0.9330, Minibatch error= 6.7%\n",
      "2018-05-27 18:12:59,378 Iter 1247, Minibatch Loss= 0.0132, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 18:13:00,699 Iter 1248, Minibatch Loss= 0.0066, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 18:13:02,030 Iter 1249, Minibatch Loss= 0.0093, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 18:13:03,362 Iter 1250, Minibatch Loss= 0.0124, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 18:13:04,728 Iter 1251, Minibatch Loss= 0.0036, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 18:13:06,061 Iter 1252, Minibatch Loss= 0.0044, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:13:07,384 Iter 1253, Minibatch Loss= 0.0228, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 18:13:08,721 Iter 1254, Minibatch Loss= 0.0192, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 18:13:10,055 Iter 1255, Minibatch Loss= 0.0034, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 18:13:11,383 Iter 1256, Minibatch Loss= 0.0094, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:13:12,750 Iter 1257, Minibatch Loss= 0.0024, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 18:13:14,063 Iter 1258, Minibatch Loss= 0.0176, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:13:15,386 Iter 1259, Minibatch Loss= 0.0264, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:13:15,387 Epoch 62, Average loss: 0.0143, learning rate: 0.0010\n",
      "2018-05-27 18:13:24,864 Iter 1260, Minibatch Loss= 0.0139, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 18:13:27,589 Iter 1260, Minibatch Loss= 0.0212, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:13:28,901 Iter 1261, Minibatch Loss= 0.0089, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:13:30,246 Iter 1262, Minibatch Loss= 0.0184, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:13:31,573 Iter 1263, Minibatch Loss= 0.0191, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:13:32,936 Iter 1264, Minibatch Loss= 0.0214, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 18:13:34,256 Iter 1265, Minibatch Loss= 0.0103, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:13:35,591 Iter 1266, Minibatch Loss= 0.0048, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 18:13:36,925 Iter 1267, Minibatch Loss= 0.0231, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 18:13:38,306 Iter 1268, Minibatch Loss= 0.0190, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:13:39,667 Iter 1269, Minibatch Loss= 0.0189, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:13:41,019 Iter 1270, Minibatch Loss= 0.0126, Training Accuracy= 0.9803, Minibatch error= 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:13:42,381 Iter 1271, Minibatch Loss= 0.0184, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:13:43,739 Iter 1272, Minibatch Loss= 0.0102, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 18:13:45,052 Iter 1273, Minibatch Loss= 0.0194, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:13:46,372 Iter 1274, Minibatch Loss= 0.0177, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:13:47,730 Iter 1275, Minibatch Loss= 0.0178, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:13:49,073 Iter 1276, Minibatch Loss= 0.0063, Training Accuracy= 0.9910, Minibatch error= 0.9%\n",
      "2018-05-27 18:13:50,421 Iter 1277, Minibatch Loss= 0.0079, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:13:51,873 Iter 1278, Minibatch Loss= 0.0101, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 18:13:53,251 Iter 1279, Minibatch Loss= 0.0070, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 18:13:53,252 Epoch 63, Average loss: 0.0151, learning rate: 0.0010\n",
      "2018-05-27 18:14:02,748 Iter 1280, Minibatch Loss= 0.0128, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:14:05,546 Iter 1280, Minibatch Loss= 0.0167, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:14:06,910 Iter 1281, Minibatch Loss= 0.0103, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 18:14:08,247 Iter 1282, Minibatch Loss= 0.0116, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 18:14:09,588 Iter 1283, Minibatch Loss= 0.0121, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:14:10,919 Iter 1284, Minibatch Loss= 0.0163, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:14:12,277 Iter 1285, Minibatch Loss= 0.0201, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:14:13,659 Iter 1286, Minibatch Loss= 0.0184, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:14:15,038 Iter 1287, Minibatch Loss= 0.0198, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:14:16,408 Iter 1288, Minibatch Loss= 0.0197, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:14:17,851 Iter 1289, Minibatch Loss= 0.0151, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:14:19,295 Iter 1290, Minibatch Loss= 0.0073, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:14:20,629 Iter 1291, Minibatch Loss= 0.0107, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:14:21,992 Iter 1292, Minibatch Loss= 0.0197, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:14:23,361 Iter 1293, Minibatch Loss= 0.0090, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:14:24,715 Iter 1294, Minibatch Loss= 0.0019, Training Accuracy= 0.9984, Minibatch error= 0.2%\n",
      "2018-05-27 18:14:26,069 Iter 1295, Minibatch Loss= 0.0153, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 18:14:27,446 Iter 1296, Minibatch Loss= 0.0131, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:14:28,790 Iter 1297, Minibatch Loss= 0.0205, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:14:30,170 Iter 1298, Minibatch Loss= 0.0083, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 18:14:31,529 Iter 1299, Minibatch Loss= 0.0190, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:14:31,530 Epoch 64, Average loss: 0.0148, learning rate: 0.0010\n",
      "2018-05-27 18:14:41,084 Iter 1300, Minibatch Loss= 0.0124, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 18:14:44,146 Iter 1300, Minibatch Loss= 0.0215, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 18:14:45,497 Iter 1301, Minibatch Loss= 0.0249, Training Accuracy= 0.9542, Minibatch error= 4.6%\n",
      "2018-05-27 18:14:46,836 Iter 1302, Minibatch Loss= 0.0202, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:14:48,218 Iter 1303, Minibatch Loss= 0.0089, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 18:14:49,586 Iter 1304, Minibatch Loss= 0.0184, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:14:50,943 Iter 1305, Minibatch Loss= 0.0182, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:14:52,321 Iter 1306, Minibatch Loss= 0.0047, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:14:53,704 Iter 1307, Minibatch Loss= 0.0236, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 18:14:55,088 Iter 1308, Minibatch Loss= 0.0198, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:14:56,473 Iter 1309, Minibatch Loss= 0.0102, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:14:57,880 Iter 1310, Minibatch Loss= 0.0114, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 18:14:59,234 Iter 1311, Minibatch Loss= 0.0068, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 18:15:00,614 Iter 1312, Minibatch Loss= 0.0065, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:15:01,989 Iter 1313, Minibatch Loss= 0.0154, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 18:15:03,380 Iter 1314, Minibatch Loss= 0.0193, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:15:04,760 Iter 1315, Minibatch Loss= 0.0197, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:15:06,142 Iter 1316, Minibatch Loss= 0.0054, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:15:07,507 Iter 1317, Minibatch Loss= 0.0130, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 18:15:08,879 Iter 1318, Minibatch Loss= 0.0113, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:15:10,237 Iter 1319, Minibatch Loss= 0.0066, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 18:15:10,238 Epoch 65, Average loss: 0.0144, learning rate: 0.0010\n",
      "2018-05-27 18:15:19,742 Iter 1320, Minibatch Loss= 0.0149, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:15:22,614 Iter 1320, Minibatch Loss= 0.0151, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:15:23,993 Iter 1321, Minibatch Loss= 0.0198, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 18:15:25,357 Iter 1322, Minibatch Loss= 0.0185, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 18:15:26,722 Iter 1323, Minibatch Loss= 0.0130, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:15:28,085 Iter 1324, Minibatch Loss= 0.0040, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:15:29,436 Iter 1325, Minibatch Loss= 0.0075, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 18:15:30,814 Iter 1326, Minibatch Loss= 0.0064, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 18:15:32,196 Iter 1327, Minibatch Loss= 0.0218, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:15:33,626 Iter 1328, Minibatch Loss= 0.0170, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:15:35,040 Iter 1329, Minibatch Loss= 0.0174, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:15:36,448 Iter 1330, Minibatch Loss= 0.0205, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:15:37,863 Iter 1331, Minibatch Loss= 0.0202, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:15:39,261 Iter 1332, Minibatch Loss= 0.0235, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 18:15:40,674 Iter 1333, Minibatch Loss= 0.0103, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 18:15:42,063 Iter 1334, Minibatch Loss= 0.0222, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 18:15:43,483 Iter 1335, Minibatch Loss= 0.0080, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:15:44,902 Iter 1336, Minibatch Loss= 0.0072, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 18:15:46,299 Iter 1337, Minibatch Loss= 0.0176, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:15:47,696 Iter 1338, Minibatch Loss= 0.0075, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:15:49,130 Iter 1339, Minibatch Loss= 0.0177, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:15:49,131 Epoch 66, Average loss: 0.0150, learning rate: 0.0010\n",
      "2018-05-27 18:15:58,764 Iter 1340, Minibatch Loss= 0.0127, Training Accuracy= 0.9805, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:16:01,743 Iter 1340, Minibatch Loss= 0.0164, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:16:03,173 Iter 1341, Minibatch Loss= 0.0143, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:16:04,569 Iter 1342, Minibatch Loss= 0.0119, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:16:05,977 Iter 1343, Minibatch Loss= 0.0113, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:16:07,371 Iter 1344, Minibatch Loss= 0.0201, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:16:08,793 Iter 1345, Minibatch Loss= 0.0163, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:16:10,203 Iter 1346, Minibatch Loss= 0.0132, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:16:11,622 Iter 1347, Minibatch Loss= 0.0102, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 18:16:13,048 Iter 1348, Minibatch Loss= 0.0168, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:16:14,462 Iter 1349, Minibatch Loss= 0.0180, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 18:16:15,874 Iter 1350, Minibatch Loss= 0.0173, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 18:16:17,282 Iter 1351, Minibatch Loss= 0.0166, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 18:16:18,707 Iter 1352, Minibatch Loss= 0.0200, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:16:20,078 Iter 1353, Minibatch Loss= 0.0074, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 18:16:21,451 Iter 1354, Minibatch Loss= 0.0193, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:16:22,838 Iter 1355, Minibatch Loss= 0.0086, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:16:24,236 Iter 1356, Minibatch Loss= 0.0181, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:16:25,593 Iter 1357, Minibatch Loss= 0.0174, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 18:16:26,980 Iter 1358, Minibatch Loss= 0.0187, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 18:16:28,349 Iter 1359, Minibatch Loss= 0.0171, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 18:16:28,351 Epoch 67, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 18:16:37,856 Iter 1360, Minibatch Loss= 0.0125, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:16:40,738 Iter 1360, Minibatch Loss= 0.0099, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 18:16:42,110 Iter 1361, Minibatch Loss= 0.0183, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:16:43,488 Iter 1362, Minibatch Loss= 0.0176, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:16:44,855 Iter 1363, Minibatch Loss= 0.0081, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:16:46,238 Iter 1364, Minibatch Loss= 0.0192, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:16:47,606 Iter 1365, Minibatch Loss= 0.0128, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 18:16:48,967 Iter 1366, Minibatch Loss= 0.0041, Training Accuracy= 0.9945, Minibatch error= 0.6%\n",
      "2018-05-27 18:16:50,325 Iter 1367, Minibatch Loss= 0.0193, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:16:51,734 Iter 1368, Minibatch Loss= 0.0095, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:16:53,116 Iter 1369, Minibatch Loss= 0.0090, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 18:16:54,487 Iter 1370, Minibatch Loss= 0.0199, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 18:16:55,885 Iter 1371, Minibatch Loss= 0.0134, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 18:16:57,288 Iter 1372, Minibatch Loss= 0.0040, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 18:16:58,710 Iter 1373, Minibatch Loss= 0.0043, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:17:00,081 Iter 1374, Minibatch Loss= 0.0066, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 18:17:01,465 Iter 1375, Minibatch Loss= 0.0037, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 18:17:02,806 Iter 1376, Minibatch Loss= 0.0084, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 18:17:04,197 Iter 1377, Minibatch Loss= 0.0176, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:17:05,560 Iter 1378, Minibatch Loss= 0.0226, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 18:17:06,915 Iter 1379, Minibatch Loss= 0.0223, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:17:06,916 Epoch 68, Average loss: 0.0129, learning rate: 0.0010\n",
      "2018-05-27 18:17:16,537 Iter 1380, Minibatch Loss= 0.0136, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:17:19,460 Iter 1380, Minibatch Loss= 0.0243, Training Accuracy= 0.9544, Minibatch error= 4.6%\n",
      "2018-05-27 18:17:20,813 Iter 1381, Minibatch Loss= 0.0216, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 18:17:22,223 Iter 1382, Minibatch Loss= 0.0190, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:17:23,629 Iter 1383, Minibatch Loss= 0.0176, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:17:25,015 Iter 1384, Minibatch Loss= 0.0055, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 18:17:26,382 Iter 1385, Minibatch Loss= 0.0088, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 18:17:27,789 Iter 1386, Minibatch Loss= 0.0130, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:17:29,156 Iter 1387, Minibatch Loss= 0.0137, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:17:30,516 Iter 1388, Minibatch Loss= 0.0076, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 18:17:31,947 Iter 1389, Minibatch Loss= 0.0151, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 18:17:33,318 Iter 1390, Minibatch Loss= 0.0080, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 18:17:34,723 Iter 1391, Minibatch Loss= 0.0117, Training Accuracy= 0.9825, Minibatch error= 1.7%\n",
      "2018-05-27 18:17:36,086 Iter 1392, Minibatch Loss= 0.0156, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 18:17:37,513 Iter 1393, Minibatch Loss= 0.0181, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:17:38,931 Iter 1394, Minibatch Loss= 0.0062, Training Accuracy= 0.9910, Minibatch error= 0.9%\n",
      "2018-05-27 18:17:40,301 Iter 1395, Minibatch Loss= 0.0127, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:17:41,737 Iter 1396, Minibatch Loss= 0.0100, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 18:17:43,137 Iter 1397, Minibatch Loss= 0.0340, Training Accuracy= 0.8935, Minibatch error= 10.7%\n",
      "2018-05-27 18:17:44,532 Iter 1398, Minibatch Loss= 0.0164, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 18:17:45,895 Iter 1399, Minibatch Loss= 0.0231, Training Accuracy= 0.9595, Minibatch error= 4.0%\n",
      "2018-05-27 18:17:45,896 Epoch 69, Average loss: 0.0146, learning rate: 0.0010\n",
      "2018-05-27 18:17:55,474 Iter 1400, Minibatch Loss= 0.0152, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 18:17:58,398 Iter 1400, Minibatch Loss= 0.0114, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:17:59,752 Iter 1401, Minibatch Loss= 0.0177, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 18:18:01,089 Iter 1402, Minibatch Loss= 0.0092, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 18:18:02,478 Iter 1403, Minibatch Loss= 0.0240, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:18:03,844 Iter 1404, Minibatch Loss= 0.0069, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 18:18:05,254 Iter 1405, Minibatch Loss= 0.0020, Training Accuracy= 0.9985, Minibatch error= 0.2%\n",
      "2018-05-27 18:18:06,625 Iter 1406, Minibatch Loss= 0.0241, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 18:18:08,058 Iter 1407, Minibatch Loss= 0.0232, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 18:18:09,446 Iter 1408, Minibatch Loss= 0.0191, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:18:10,841 Iter 1409, Minibatch Loss= 0.0215, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 18:18:12,247 Iter 1410, Minibatch Loss= 0.0038, Training Accuracy= 0.9916, Minibatch error= 0.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:18:13,626 Iter 1411, Minibatch Loss= 0.0167, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 18:18:15,010 Iter 1412, Minibatch Loss= 0.0304, Training Accuracy= 0.9187, Minibatch error= 8.1%\n",
      "2018-05-27 18:18:16,359 Iter 1413, Minibatch Loss= 0.0253, Training Accuracy= 0.9258, Minibatch error= 7.4%\n",
      "2018-05-27 18:18:17,775 Iter 1414, Minibatch Loss= 0.0133, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:18:19,166 Iter 1415, Minibatch Loss= 0.0085, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 18:18:20,546 Iter 1416, Minibatch Loss= 0.0213, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:18:21,914 Iter 1417, Minibatch Loss= 0.0105, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:18:23,315 Iter 1418, Minibatch Loss= 0.0172, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:18:24,694 Iter 1419, Minibatch Loss= 0.0198, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:18:24,695 Epoch 70, Average loss: 0.0166, learning rate: 0.0010\n",
      "2018-05-27 18:18:34,264 Iter 1420, Minibatch Loss= 0.0130, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:18:37,214 Iter 1420, Minibatch Loss= 0.0068, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 18:18:38,608 Iter 1421, Minibatch Loss= 0.0225, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 18:18:39,992 Iter 1422, Minibatch Loss= 0.0172, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:18:41,382 Iter 1423, Minibatch Loss= 0.0239, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:18:42,833 Iter 1424, Minibatch Loss= 0.0194, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 18:18:44,199 Iter 1425, Minibatch Loss= 0.0160, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 18:18:45,615 Iter 1426, Minibatch Loss= 0.0049, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 18:18:46,982 Iter 1427, Minibatch Loss= 0.0234, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:18:48,398 Iter 1428, Minibatch Loss= 0.0037, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 18:18:49,805 Iter 1429, Minibatch Loss= 0.0210, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:18:51,193 Iter 1430, Minibatch Loss= 0.0207, Training Accuracy= 0.9695, Minibatch error= 3.1%\n",
      "2018-05-27 18:18:52,608 Iter 1431, Minibatch Loss= 0.0109, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 18:18:54,000 Iter 1432, Minibatch Loss= 0.0213, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:18:55,399 Iter 1433, Minibatch Loss= 0.0081, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 18:18:56,772 Iter 1434, Minibatch Loss= 0.0183, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:18:58,176 Iter 1435, Minibatch Loss= 0.0135, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:18:59,578 Iter 1436, Minibatch Loss= 0.0156, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:19:01,010 Iter 1437, Minibatch Loss= 0.0113, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 18:19:02,411 Iter 1438, Minibatch Loss= 0.0094, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:19:03,768 Iter 1439, Minibatch Loss= 0.0182, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:19:03,769 Epoch 71, Average loss: 0.0151, learning rate: 0.0010\n",
      "2018-05-27 18:19:13,367 Iter 1440, Minibatch Loss= 0.0138, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:19:16,323 Iter 1440, Minibatch Loss= 0.0108, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 18:19:17,721 Iter 1441, Minibatch Loss= 0.0235, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:19:19,145 Iter 1442, Minibatch Loss= 0.0110, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 18:19:20,549 Iter 1443, Minibatch Loss= 0.0091, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 18:19:21,923 Iter 1444, Minibatch Loss= 0.0209, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:19:23,339 Iter 1445, Minibatch Loss= 0.0195, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:19:24,725 Iter 1446, Minibatch Loss= 0.0182, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 18:19:26,142 Iter 1447, Minibatch Loss= 0.0201, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:19:27,506 Iter 1448, Minibatch Loss= 0.0173, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:19:28,920 Iter 1449, Minibatch Loss= 0.0036, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 18:19:30,313 Iter 1450, Minibatch Loss= 0.0100, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 18:19:31,692 Iter 1451, Minibatch Loss= 0.0191, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 18:19:33,066 Iter 1452, Minibatch Loss= 0.0162, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:19:34,440 Iter 1453, Minibatch Loss= 0.0179, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:19:35,810 Iter 1454, Minibatch Loss= 0.0108, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 18:19:37,193 Iter 1455, Minibatch Loss= 0.0016, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 18:19:38,582 Iter 1456, Minibatch Loss= 0.0081, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 18:19:39,944 Iter 1457, Minibatch Loss= 0.0020, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 18:19:41,355 Iter 1458, Minibatch Loss= 0.0079, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:19:42,752 Iter 1459, Minibatch Loss= 0.0175, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 18:19:42,753 Epoch 72, Average loss: 0.0132, learning rate: 0.0010\n",
      "2018-05-27 18:19:52,235 Iter 1460, Minibatch Loss= 0.0123, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:19:55,227 Iter 1460, Minibatch Loss= 0.0087, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 18:19:56,640 Iter 1461, Minibatch Loss= 0.0217, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:19:58,043 Iter 1462, Minibatch Loss= 0.0099, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 18:19:59,469 Iter 1463, Minibatch Loss= 0.0143, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:20:00,904 Iter 1464, Minibatch Loss= 0.0097, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 18:20:02,276 Iter 1465, Minibatch Loss= 0.0075, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:20:03,681 Iter 1466, Minibatch Loss= 0.0096, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:20:05,057 Iter 1467, Minibatch Loss= 0.0087, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:20:06,463 Iter 1468, Minibatch Loss= 0.0075, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 18:20:07,868 Iter 1469, Minibatch Loss= 0.0041, Training Accuracy= 0.9945, Minibatch error= 0.5%\n",
      "2018-05-27 18:20:09,284 Iter 1470, Minibatch Loss= 0.0184, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:20:10,671 Iter 1471, Minibatch Loss= 0.0158, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:20:12,053 Iter 1472, Minibatch Loss= 0.0122, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 18:20:13,476 Iter 1473, Minibatch Loss= 0.0045, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 18:20:14,859 Iter 1474, Minibatch Loss= 0.0085, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:20:16,266 Iter 1475, Minibatch Loss= 0.0016, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 18:20:17,658 Iter 1476, Minibatch Loss= 0.0138, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:20:19,059 Iter 1477, Minibatch Loss= 0.0067, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:20:20,453 Iter 1478, Minibatch Loss= 0.0179, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:20:21,819 Iter 1479, Minibatch Loss= 0.0154, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:20:21,820 Epoch 73, Average loss: 0.0112, learning rate: 0.0010\n",
      "2018-05-27 18:20:31,396 Iter 1480, Minibatch Loss= 0.0132, Training Accuracy= 0.9777, Minibatch error= 2.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:20:34,388 Iter 1480, Minibatch Loss= 0.0189, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 18:20:35,801 Iter 1481, Minibatch Loss= 0.0197, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:20:37,183 Iter 1482, Minibatch Loss= 0.0206, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:20:38,586 Iter 1483, Minibatch Loss= 0.0179, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:20:39,977 Iter 1484, Minibatch Loss= 0.0169, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:20:41,395 Iter 1485, Minibatch Loss= 0.0189, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 18:20:42,770 Iter 1486, Minibatch Loss= 0.0049, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 18:20:44,205 Iter 1487, Minibatch Loss= 0.0177, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:20:45,623 Iter 1488, Minibatch Loss= 0.0059, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 18:20:47,053 Iter 1489, Minibatch Loss= 0.0163, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:20:48,468 Iter 1490, Minibatch Loss= 0.0089, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 18:20:49,822 Iter 1491, Minibatch Loss= 0.0105, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 18:20:51,234 Iter 1492, Minibatch Loss= 0.0123, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:20:52,618 Iter 1493, Minibatch Loss= 0.0224, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:20:54,025 Iter 1494, Minibatch Loss= 0.0101, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:20:55,433 Iter 1495, Minibatch Loss= 0.0168, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 18:20:56,845 Iter 1496, Minibatch Loss= 0.0061, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 18:20:58,245 Iter 1497, Minibatch Loss= 0.0193, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 18:20:59,628 Iter 1498, Minibatch Loss= 0.0166, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 18:21:01,080 Iter 1499, Minibatch Loss= 0.0101, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 18:21:01,081 Epoch 74, Average loss: 0.0145, learning rate: 0.0010\n",
      "2018-05-27 18:21:10,619 Iter 1500, Minibatch Loss= 0.0125, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 18:21:13,893 Iter 1500, Minibatch Loss= 0.0179, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:21:15,275 Iter 1501, Minibatch Loss= 0.0176, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:21:16,667 Iter 1502, Minibatch Loss= 0.0083, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 18:21:18,044 Iter 1503, Minibatch Loss= 0.0130, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 18:21:19,474 Iter 1504, Minibatch Loss= 0.0156, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:21:20,847 Iter 1505, Minibatch Loss= 0.0175, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:21:22,241 Iter 1506, Minibatch Loss= 0.0058, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 18:21:23,653 Iter 1507, Minibatch Loss= 0.0122, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:21:25,054 Iter 1508, Minibatch Loss= 0.0182, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 18:21:26,468 Iter 1509, Minibatch Loss= 0.0108, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:21:27,853 Iter 1510, Minibatch Loss= 0.0168, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 18:21:29,260 Iter 1511, Minibatch Loss= 0.0107, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:21:30,717 Iter 1512, Minibatch Loss= 0.0199, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:21:32,134 Iter 1513, Minibatch Loss= 0.0213, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:21:33,546 Iter 1514, Minibatch Loss= 0.0052, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 18:21:34,978 Iter 1515, Minibatch Loss= 0.0077, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 18:21:36,374 Iter 1516, Minibatch Loss= 0.0180, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:21:37,789 Iter 1517, Minibatch Loss= 0.0169, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:21:39,231 Iter 1518, Minibatch Loss= 0.0140, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 18:21:40,636 Iter 1519, Minibatch Loss= 0.0206, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 18:21:40,637 Epoch 75, Average loss: 0.0146, learning rate: 0.0010\n",
      "2018-05-27 18:21:50,183 Iter 1520, Minibatch Loss= 0.0131, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 18:21:53,241 Iter 1520, Minibatch Loss= 0.0168, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:21:54,659 Iter 1521, Minibatch Loss= 0.0122, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 18:21:56,049 Iter 1522, Minibatch Loss= 0.0198, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 18:21:57,441 Iter 1523, Minibatch Loss= 0.0141, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 18:21:58,840 Iter 1524, Minibatch Loss= 0.0177, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:22:00,289 Iter 1525, Minibatch Loss= 0.0094, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:22:01,704 Iter 1526, Minibatch Loss= 0.0116, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 18:22:03,090 Iter 1527, Minibatch Loss= 0.0217, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:22:04,506 Iter 1528, Minibatch Loss= 0.0193, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:22:05,906 Iter 1529, Minibatch Loss= 0.0186, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 18:22:07,320 Iter 1530, Minibatch Loss= 0.0085, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:22:08,721 Iter 1531, Minibatch Loss= 0.0192, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:22:10,117 Iter 1532, Minibatch Loss= 0.0044, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:22:11,525 Iter 1533, Minibatch Loss= 0.0238, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 18:22:12,918 Iter 1534, Minibatch Loss= 0.0194, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:22:14,306 Iter 1535, Minibatch Loss= 0.0050, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 18:22:15,720 Iter 1536, Minibatch Loss= 0.0165, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 18:22:17,148 Iter 1537, Minibatch Loss= 0.0143, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 18:22:18,546 Iter 1538, Minibatch Loss= 0.0025, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 18:22:19,946 Iter 1539, Minibatch Loss= 0.0044, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 18:22:19,947 Epoch 76, Average loss: 0.0141, learning rate: 0.0010\n",
      "2018-05-27 18:22:29,479 Iter 1540, Minibatch Loss= 0.0121, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:22:32,641 Iter 1540, Minibatch Loss= 0.0064, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 18:22:34,055 Iter 1541, Minibatch Loss= 0.0175, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:22:35,502 Iter 1542, Minibatch Loss= 0.0184, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:22:36,901 Iter 1543, Minibatch Loss= 0.0037, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 18:22:38,304 Iter 1544, Minibatch Loss= 0.0191, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 18:22:39,740 Iter 1545, Minibatch Loss= 0.0169, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:22:41,140 Iter 1546, Minibatch Loss= 0.0023, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 18:22:42,591 Iter 1547, Minibatch Loss= 0.0117, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:22:43,981 Iter 1548, Minibatch Loss= 0.0322, Training Accuracy= 0.9414, Minibatch error= 5.9%\n",
      "2018-05-27 18:22:45,397 Iter 1549, Minibatch Loss= 0.0076, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 18:22:46,796 Iter 1550, Minibatch Loss= 0.0186, Training Accuracy= 0.9694, Minibatch error= 3.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:22:48,233 Iter 1551, Minibatch Loss= 0.0178, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:22:49,658 Iter 1552, Minibatch Loss= 0.0074, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 18:22:51,084 Iter 1553, Minibatch Loss= 0.0190, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:22:52,522 Iter 1554, Minibatch Loss= 0.0035, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 18:22:53,918 Iter 1555, Minibatch Loss= 0.0076, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:22:55,360 Iter 1556, Minibatch Loss= 0.0171, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 18:22:56,789 Iter 1557, Minibatch Loss= 0.0173, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 18:22:58,223 Iter 1558, Minibatch Loss= 0.0165, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 18:22:59,629 Iter 1559, Minibatch Loss= 0.0135, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 18:22:59,630 Epoch 77, Average loss: 0.0142, learning rate: 0.0010\n",
      "2018-05-27 18:23:09,197 Iter 1560, Minibatch Loss= 0.0141, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 18:23:12,267 Iter 1560, Minibatch Loss= 0.0212, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:23:13,707 Iter 1561, Minibatch Loss= 0.0124, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 18:23:15,142 Iter 1562, Minibatch Loss= 0.0069, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 18:23:16,596 Iter 1563, Minibatch Loss= 0.0162, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 18:23:18,002 Iter 1564, Minibatch Loss= 0.0195, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 18:23:19,437 Iter 1565, Minibatch Loss= 0.0064, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 18:23:20,868 Iter 1566, Minibatch Loss= 0.0176, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:23:22,275 Iter 1567, Minibatch Loss= 0.0177, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:23:23,716 Iter 1568, Minibatch Loss= 0.0161, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:23:25,117 Iter 1569, Minibatch Loss= 0.0180, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:23:26,547 Iter 1570, Minibatch Loss= 0.0213, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:23:27,954 Iter 1571, Minibatch Loss= 0.0182, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:23:29,398 Iter 1572, Minibatch Loss= 0.0119, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:23:30,810 Iter 1573, Minibatch Loss= 0.0144, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 18:23:32,221 Iter 1574, Minibatch Loss= 0.0162, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:23:33,635 Iter 1575, Minibatch Loss= 0.0081, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 18:23:35,044 Iter 1576, Minibatch Loss= 0.0023, Training Accuracy= 0.9979, Minibatch error= 0.2%\n",
      "2018-05-27 18:23:36,452 Iter 1577, Minibatch Loss= 0.0118, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 18:23:37,847 Iter 1578, Minibatch Loss= 0.0194, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:23:39,302 Iter 1579, Minibatch Loss= 0.0190, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:23:39,303 Epoch 78, Average loss: 0.0147, learning rate: 0.0010\n",
      "2018-05-27 18:23:48,876 Iter 1580, Minibatch Loss= 0.0123, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 18:23:52,093 Iter 1580, Minibatch Loss= 0.0081, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 18:23:53,521 Iter 1581, Minibatch Loss= 0.0025, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 18:23:54,990 Iter 1582, Minibatch Loss= 0.0025, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 18:23:56,433 Iter 1583, Minibatch Loss= 0.0130, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:23:57,830 Iter 1584, Minibatch Loss= 0.0234, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 18:23:59,233 Iter 1585, Minibatch Loss= 0.0036, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 18:24:00,648 Iter 1586, Minibatch Loss= 0.0195, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:24:02,057 Iter 1587, Minibatch Loss= 0.0045, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:24:03,491 Iter 1588, Minibatch Loss= 0.0179, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:24:04,956 Iter 1589, Minibatch Loss= 0.0223, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:24:06,397 Iter 1590, Minibatch Loss= 0.0185, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:24:07,807 Iter 1591, Minibatch Loss= 0.0208, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 18:24:09,231 Iter 1592, Minibatch Loss= 0.0214, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 18:24:10,664 Iter 1593, Minibatch Loss= 0.0126, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:24:12,113 Iter 1594, Minibatch Loss= 0.0103, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 18:24:13,546 Iter 1595, Minibatch Loss= 0.0181, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 18:24:15,022 Iter 1596, Minibatch Loss= 0.0043, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 18:24:16,502 Iter 1597, Minibatch Loss= 0.0140, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:24:17,901 Iter 1598, Minibatch Loss= 0.0069, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 18:24:19,337 Iter 1599, Minibatch Loss= 0.0066, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:24:19,338 Epoch 79, Average loss: 0.0127, learning rate: 0.0010\n",
      "2018-05-27 18:24:28,834 Iter 1600, Minibatch Loss= 0.0125, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:24:31,994 Iter 1600, Minibatch Loss= 0.0084, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:24:33,410 Iter 1601, Minibatch Loss= 0.0177, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 18:24:34,830 Iter 1602, Minibatch Loss= 0.0101, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 18:24:36,276 Iter 1603, Minibatch Loss= 0.0135, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:24:37,720 Iter 1604, Minibatch Loss= 0.0201, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:24:39,173 Iter 1605, Minibatch Loss= 0.0180, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:24:40,600 Iter 1606, Minibatch Loss= 0.0187, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 18:24:42,041 Iter 1607, Minibatch Loss= 0.0184, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:24:43,482 Iter 1608, Minibatch Loss= 0.0078, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 18:24:44,939 Iter 1609, Minibatch Loss= 0.0173, Training Accuracy= 0.9705, Minibatch error= 2.9%\n",
      "2018-05-27 18:24:46,427 Iter 1610, Minibatch Loss= 0.0089, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 18:24:47,889 Iter 1611, Minibatch Loss= 0.0162, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:24:49,326 Iter 1612, Minibatch Loss= 0.0045, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:24:50,746 Iter 1613, Minibatch Loss= 0.0169, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 18:24:52,196 Iter 1614, Minibatch Loss= 0.0180, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:24:53,634 Iter 1615, Minibatch Loss= 0.0082, Training Accuracy= 0.9875, Minibatch error= 1.2%\n",
      "2018-05-27 18:24:55,081 Iter 1616, Minibatch Loss= 0.0222, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 18:24:56,528 Iter 1617, Minibatch Loss= 0.0205, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:24:57,976 Iter 1618, Minibatch Loss= 0.0108, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 18:24:59,448 Iter 1619, Minibatch Loss= 0.0050, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 18:24:59,449 Epoch 80, Average loss: 0.0142, learning rate: 0.0010\n",
      "2018-05-27 18:25:08,997 Iter 1620, Minibatch Loss= 0.0121, Training Accuracy= 0.9811, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:25:12,145 Iter 1620, Minibatch Loss= 0.0070, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:25:13,561 Iter 1621, Minibatch Loss= 0.0121, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 18:25:15,003 Iter 1622, Minibatch Loss= 0.0069, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:25:16,489 Iter 1623, Minibatch Loss= 0.0190, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:25:17,950 Iter 1624, Minibatch Loss= 0.0147, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:25:19,409 Iter 1625, Minibatch Loss= 0.0144, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:25:20,817 Iter 1626, Minibatch Loss= 0.0158, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 18:25:22,283 Iter 1627, Minibatch Loss= 0.0137, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:25:23,707 Iter 1628, Minibatch Loss= 0.0180, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:25:25,154 Iter 1629, Minibatch Loss= 0.0185, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:25:26,581 Iter 1630, Minibatch Loss= 0.0078, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:25:28,002 Iter 1631, Minibatch Loss= 0.0060, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 18:25:29,450 Iter 1632, Minibatch Loss= 0.0167, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:25:30,852 Iter 1633, Minibatch Loss= 0.0145, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 18:25:32,289 Iter 1634, Minibatch Loss= 0.0163, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 18:25:33,744 Iter 1635, Minibatch Loss= 0.0184, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:25:35,199 Iter 1636, Minibatch Loss= 0.0050, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 18:25:36,640 Iter 1637, Minibatch Loss= 0.0150, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 18:25:38,099 Iter 1638, Minibatch Loss= 0.0154, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:25:39,581 Iter 1639, Minibatch Loss= 0.0133, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 18:25:39,582 Epoch 81, Average loss: 0.0137, learning rate: 0.0010\n",
      "2018-05-27 18:25:49,144 Iter 1640, Minibatch Loss= 0.0128, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 18:25:52,374 Iter 1640, Minibatch Loss= 0.0104, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:25:53,808 Iter 1641, Minibatch Loss= 0.0200, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:25:55,224 Iter 1642, Minibatch Loss= 0.0095, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 18:25:56,665 Iter 1643, Minibatch Loss= 0.0234, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 18:25:58,133 Iter 1644, Minibatch Loss= 0.0021, Training Accuracy= 0.9981, Minibatch error= 0.2%\n",
      "2018-05-27 18:25:59,571 Iter 1645, Minibatch Loss= 0.0176, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:26:00,996 Iter 1646, Minibatch Loss= 0.0164, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:26:02,446 Iter 1647, Minibatch Loss= 0.0067, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 18:26:03,897 Iter 1648, Minibatch Loss= 0.0095, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 18:26:05,348 Iter 1649, Minibatch Loss= 0.0214, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:06,805 Iter 1650, Minibatch Loss= 0.0149, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:08,272 Iter 1651, Minibatch Loss= 0.0142, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 18:26:09,721 Iter 1652, Minibatch Loss= 0.0220, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 18:26:11,166 Iter 1653, Minibatch Loss= 0.0109, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:26:12,620 Iter 1654, Minibatch Loss= 0.0189, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 18:26:14,037 Iter 1655, Minibatch Loss= 0.0128, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:26:15,489 Iter 1656, Minibatch Loss= 0.0050, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 18:26:16,956 Iter 1657, Minibatch Loss= 0.0163, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:18,426 Iter 1658, Minibatch Loss= 0.0176, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:19,875 Iter 1659, Minibatch Loss= 0.0028, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 18:26:19,876 Epoch 82, Average loss: 0.0138, learning rate: 0.0010\n",
      "2018-05-27 18:26:29,450 Iter 1660, Minibatch Loss= 0.0125, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:26:32,714 Iter 1660, Minibatch Loss= 0.0200, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:26:34,156 Iter 1661, Minibatch Loss= 0.0182, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:26:35,589 Iter 1662, Minibatch Loss= 0.0190, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:26:37,019 Iter 1663, Minibatch Loss= 0.0227, Training Accuracy= 0.9615, Minibatch error= 3.9%\n",
      "2018-05-27 18:26:38,489 Iter 1664, Minibatch Loss= 0.0118, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 18:26:39,906 Iter 1665, Minibatch Loss= 0.0163, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:41,345 Iter 1666, Minibatch Loss= 0.0192, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 18:26:42,822 Iter 1667, Minibatch Loss= 0.0092, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:26:44,245 Iter 1668, Minibatch Loss= 0.0163, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:26:45,681 Iter 1669, Minibatch Loss= 0.0165, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 18:26:47,123 Iter 1670, Minibatch Loss= 0.0181, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 18:26:48,609 Iter 1671, Minibatch Loss= 0.0042, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 18:26:50,073 Iter 1672, Minibatch Loss= 0.0045, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 18:26:51,512 Iter 1673, Minibatch Loss= 0.0026, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 18:26:52,990 Iter 1674, Minibatch Loss= 0.0048, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 18:26:54,444 Iter 1675, Minibatch Loss= 0.0112, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 18:26:55,916 Iter 1676, Minibatch Loss= 0.0156, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 18:26:57,378 Iter 1677, Minibatch Loss= 0.0065, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 18:26:58,808 Iter 1678, Minibatch Loss= 0.0210, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 18:27:00,266 Iter 1679, Minibatch Loss= 0.0071, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:27:00,267 Epoch 83, Average loss: 0.0136, learning rate: 0.0010\n",
      "2018-05-27 18:27:09,811 Iter 1680, Minibatch Loss= 0.0118, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:27:13,109 Iter 1680, Minibatch Loss= 0.0174, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:27:14,567 Iter 1681, Minibatch Loss= 0.0065, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 18:27:16,026 Iter 1682, Minibatch Loss= 0.0178, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:27:17,493 Iter 1683, Minibatch Loss= 0.0199, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:27:18,998 Iter 1684, Minibatch Loss= 0.0081, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 18:27:20,433 Iter 1685, Minibatch Loss= 0.0108, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 18:27:21,875 Iter 1686, Minibatch Loss= 0.0120, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 18:27:23,337 Iter 1687, Minibatch Loss= 0.0182, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:27:24,790 Iter 1688, Minibatch Loss= 0.0062, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 18:27:26,250 Iter 1689, Minibatch Loss= 0.0174, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:27:27,732 Iter 1690, Minibatch Loss= 0.0063, Training Accuracy= 0.9905, Minibatch error= 1.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:27:29,184 Iter 1691, Minibatch Loss= 0.0191, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 18:27:30,639 Iter 1692, Minibatch Loss= 0.0098, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 18:27:32,105 Iter 1693, Minibatch Loss= 0.0120, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:27:33,579 Iter 1694, Minibatch Loss= 0.0183, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:27:35,057 Iter 1695, Minibatch Loss= 0.0156, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:27:36,550 Iter 1696, Minibatch Loss= 0.0014, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 18:27:38,020 Iter 1697, Minibatch Loss= 0.0123, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 18:27:39,492 Iter 1698, Minibatch Loss= 0.0094, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:27:40,917 Iter 1699, Minibatch Loss= 0.0132, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 18:27:40,918 Epoch 84, Average loss: 0.0127, learning rate: 0.0010\n",
      "2018-05-27 18:27:50,534 Iter 1700, Minibatch Loss= 0.0116, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:27:54,026 Iter 1700, Minibatch Loss= 0.0178, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:27:55,512 Iter 1701, Minibatch Loss= 0.0188, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:27:56,983 Iter 1702, Minibatch Loss= 0.0093, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:27:58,457 Iter 1703, Minibatch Loss= 0.0198, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 18:27:59,894 Iter 1704, Minibatch Loss= 0.0173, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 18:28:01,363 Iter 1705, Minibatch Loss= 0.0128, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 18:28:02,806 Iter 1706, Minibatch Loss= 0.0167, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:28:04,271 Iter 1707, Minibatch Loss= 0.0091, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 18:28:05,744 Iter 1708, Minibatch Loss= 0.0155, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:28:07,183 Iter 1709, Minibatch Loss= 0.0124, Training Accuracy= 0.9802, Minibatch error= 2.0%\n",
      "2018-05-27 18:28:08,642 Iter 1710, Minibatch Loss= 0.0135, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:28:10,081 Iter 1711, Minibatch Loss= 0.0157, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:28:11,531 Iter 1712, Minibatch Loss= 0.0060, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 18:28:12,982 Iter 1713, Minibatch Loss= 0.0168, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:28:14,432 Iter 1714, Minibatch Loss= 0.0162, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:28:15,882 Iter 1715, Minibatch Loss= 0.0048, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 18:28:17,312 Iter 1716, Minibatch Loss= 0.0121, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:28:18,814 Iter 1717, Minibatch Loss= 0.0182, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:28:20,248 Iter 1718, Minibatch Loss= 0.0185, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:28:21,681 Iter 1719, Minibatch Loss= 0.0163, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:28:21,682 Epoch 85, Average loss: 0.0146, learning rate: 0.0010\n",
      "2018-05-27 18:28:31,206 Iter 1720, Minibatch Loss= 0.0122, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 18:28:34,509 Iter 1720, Minibatch Loss= 0.0154, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 18:28:35,974 Iter 1721, Minibatch Loss= 0.0184, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 18:28:37,417 Iter 1722, Minibatch Loss= 0.0133, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:28:38,917 Iter 1723, Minibatch Loss= 0.0186, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:28:40,360 Iter 1724, Minibatch Loss= 0.0018, Training Accuracy= 0.9986, Minibatch error= 0.1%\n",
      "2018-05-27 18:28:41,842 Iter 1725, Minibatch Loss= 0.0051, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 18:28:43,309 Iter 1726, Minibatch Loss= 0.0194, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 18:28:44,805 Iter 1727, Minibatch Loss= 0.0117, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:28:46,297 Iter 1728, Minibatch Loss= 0.0163, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:28:47,810 Iter 1729, Minibatch Loss= 0.0050, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 18:28:49,291 Iter 1730, Minibatch Loss= 0.0079, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:28:50,744 Iter 1731, Minibatch Loss= 0.0067, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:28:52,267 Iter 1732, Minibatch Loss= 0.0193, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:28:53,731 Iter 1733, Minibatch Loss= 0.0109, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:28:55,188 Iter 1734, Minibatch Loss= 0.0158, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:28:56,665 Iter 1735, Minibatch Loss= 0.0085, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 18:28:58,125 Iter 1736, Minibatch Loss= 0.0173, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:28:59,579 Iter 1737, Minibatch Loss= 0.0084, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:29:01,013 Iter 1738, Minibatch Loss= 0.0101, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 18:29:02,470 Iter 1739, Minibatch Loss= 0.0027, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 18:29:02,471 Epoch 86, Average loss: 0.0118, learning rate: 0.0010\n",
      "2018-05-27 18:29:12,046 Iter 1740, Minibatch Loss= 0.0117, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:29:15,404 Iter 1740, Minibatch Loss= 0.0181, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 18:29:16,847 Iter 1741, Minibatch Loss= 0.0074, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 18:29:18,318 Iter 1742, Minibatch Loss= 0.0077, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:29:19,756 Iter 1743, Minibatch Loss= 0.0176, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:29:21,190 Iter 1744, Minibatch Loss= 0.0058, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 18:29:22,680 Iter 1745, Minibatch Loss= 0.0126, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:29:24,129 Iter 1746, Minibatch Loss= 0.0205, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 18:29:25,599 Iter 1747, Minibatch Loss= 0.0073, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 18:29:27,057 Iter 1748, Minibatch Loss= 0.0100, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 18:29:28,498 Iter 1749, Minibatch Loss= 0.0033, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 18:29:29,959 Iter 1750, Minibatch Loss= 0.0103, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:29:31,404 Iter 1751, Minibatch Loss= 0.0125, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 18:29:32,859 Iter 1752, Minibatch Loss= 0.0070, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:29:34,332 Iter 1753, Minibatch Loss= 0.0140, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:29:35,796 Iter 1754, Minibatch Loss= 0.0165, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:29:37,290 Iter 1755, Minibatch Loss= 0.0110, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:29:38,747 Iter 1756, Minibatch Loss= 0.0015, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 18:29:40,259 Iter 1757, Minibatch Loss= 0.0169, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:29:41,720 Iter 1758, Minibatch Loss= 0.0130, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:29:43,198 Iter 1759, Minibatch Loss= 0.0037, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 18:29:43,199 Epoch 87, Average loss: 0.0110, learning rate: 0.0010\n",
      "2018-05-27 18:29:52,825 Iter 1760, Minibatch Loss= 0.0115, Training Accuracy= 0.9805, Minibatch error= 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:29:56,142 Iter 1760, Minibatch Loss= 0.0135, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:29:57,627 Iter 1761, Minibatch Loss= 0.0074, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 18:29:59,086 Iter 1762, Minibatch Loss= 0.0076, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:30:00,574 Iter 1763, Minibatch Loss= 0.0150, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:30:02,036 Iter 1764, Minibatch Loss= 0.0054, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 18:30:03,508 Iter 1765, Minibatch Loss= 0.0047, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 18:30:04,962 Iter 1766, Minibatch Loss= 0.0088, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:30:06,405 Iter 1767, Minibatch Loss= 0.0164, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:30:07,906 Iter 1768, Minibatch Loss= 0.0168, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:30:09,377 Iter 1769, Minibatch Loss= 0.0108, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 18:30:10,874 Iter 1770, Minibatch Loss= 0.0131, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 18:30:12,358 Iter 1771, Minibatch Loss= 0.0192, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 18:30:13,827 Iter 1772, Minibatch Loss= 0.0175, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:30:15,285 Iter 1773, Minibatch Loss= 0.0179, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:30:16,754 Iter 1774, Minibatch Loss= 0.0015, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 18:30:18,213 Iter 1775, Minibatch Loss= 0.0062, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 18:30:19,691 Iter 1776, Minibatch Loss= 0.0045, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 18:30:21,149 Iter 1777, Minibatch Loss= 0.0025, Training Accuracy= 0.9975, Minibatch error= 0.2%\n",
      "2018-05-27 18:30:22,657 Iter 1778, Minibatch Loss= 0.0256, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:30:24,124 Iter 1779, Minibatch Loss= 0.0170, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:30:24,125 Epoch 88, Average loss: 0.0122, learning rate: 0.0010\n",
      "2018-05-27 18:30:33,642 Iter 1780, Minibatch Loss= 0.0126, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 18:30:36,949 Iter 1780, Minibatch Loss= 0.0025, Training Accuracy= 0.9973, Minibatch error= 0.3%\n",
      "2018-05-27 18:30:38,451 Iter 1781, Minibatch Loss= 0.0160, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 18:30:39,912 Iter 1782, Minibatch Loss= 0.0087, Training Accuracy= 0.9870, Minibatch error= 1.3%\n",
      "2018-05-27 18:30:41,400 Iter 1783, Minibatch Loss= 0.0140, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:30:42,851 Iter 1784, Minibatch Loss= 0.0141, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 18:30:44,303 Iter 1785, Minibatch Loss= 0.0120, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:30:45,791 Iter 1786, Minibatch Loss= 0.0195, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:30:47,234 Iter 1787, Minibatch Loss= 0.0163, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:30:48,689 Iter 1788, Minibatch Loss= 0.0081, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 18:30:50,150 Iter 1789, Minibatch Loss= 0.0081, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:30:51,608 Iter 1790, Minibatch Loss= 0.0183, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:30:53,100 Iter 1791, Minibatch Loss= 0.0099, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:30:54,547 Iter 1792, Minibatch Loss= 0.0172, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:30:56,029 Iter 1793, Minibatch Loss= 0.0052, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 18:30:57,506 Iter 1794, Minibatch Loss= 0.0074, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:30:59,044 Iter 1795, Minibatch Loss= 0.0022, Training Accuracy= 0.9978, Minibatch error= 0.2%\n",
      "2018-05-27 18:31:00,552 Iter 1796, Minibatch Loss= 0.0033, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 18:31:02,034 Iter 1797, Minibatch Loss= 0.0165, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:31:03,535 Iter 1798, Minibatch Loss= 0.0108, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:31:05,007 Iter 1799, Minibatch Loss= 0.0054, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:31:05,008 Epoch 89, Average loss: 0.0107, learning rate: 0.0010\n",
      "2018-05-27 18:31:14,527 Iter 1800, Minibatch Loss= 0.0114, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:31:17,901 Iter 1800, Minibatch Loss= 0.0174, Training Accuracy= 0.9695, Minibatch error= 3.1%\n",
      "2018-05-27 18:31:19,405 Iter 1801, Minibatch Loss= 0.0158, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:31:20,877 Iter 1802, Minibatch Loss= 0.0035, Training Accuracy= 0.9953, Minibatch error= 0.5%\n",
      "2018-05-27 18:31:22,359 Iter 1803, Minibatch Loss= 0.0159, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:31:23,870 Iter 1804, Minibatch Loss= 0.0191, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:31:25,351 Iter 1805, Minibatch Loss= 0.0187, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:31:26,861 Iter 1806, Minibatch Loss= 0.0142, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 18:31:28,330 Iter 1807, Minibatch Loss= 0.0071, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 18:31:29,817 Iter 1808, Minibatch Loss= 0.0160, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:31:31,318 Iter 1809, Minibatch Loss= 0.0037, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 18:31:32,828 Iter 1810, Minibatch Loss= 0.0110, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 18:31:34,318 Iter 1811, Minibatch Loss= 0.0156, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:31:35,789 Iter 1812, Minibatch Loss= 0.0159, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 18:31:37,264 Iter 1813, Minibatch Loss= 0.0034, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 18:31:38,750 Iter 1814, Minibatch Loss= 0.0137, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:31:40,231 Iter 1815, Minibatch Loss= 0.0035, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 18:31:41,706 Iter 1816, Minibatch Loss= 0.0160, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:31:43,175 Iter 1817, Minibatch Loss= 0.0090, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 18:31:44,631 Iter 1818, Minibatch Loss= 0.0087, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:31:46,110 Iter 1819, Minibatch Loss= 0.0027, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 18:31:46,111 Epoch 90, Average loss: 0.0116, learning rate: 0.0010\n",
      "2018-05-27 18:31:55,763 Iter 1820, Minibatch Loss= 0.0113, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:31:59,170 Iter 1820, Minibatch Loss= 0.0108, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:32:00,644 Iter 1821, Minibatch Loss= 0.0167, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:32:02,150 Iter 1822, Minibatch Loss= 0.0174, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 18:32:03,628 Iter 1823, Minibatch Loss= 0.0076, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:32:05,085 Iter 1824, Minibatch Loss= 0.0020, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 18:32:06,574 Iter 1825, Minibatch Loss= 0.0185, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:32:08,040 Iter 1826, Minibatch Loss= 0.0169, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:32:09,564 Iter 1827, Minibatch Loss= 0.0203, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:32:11,062 Iter 1828, Minibatch Loss= 0.0138, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:32:12,577 Iter 1829, Minibatch Loss= 0.0166, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 18:32:14,058 Iter 1830, Minibatch Loss= 0.0091, Training Accuracy= 0.9852, Minibatch error= 1.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:32:15,554 Iter 1831, Minibatch Loss= 0.0146, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 18:32:17,046 Iter 1832, Minibatch Loss= 0.0128, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 18:32:18,520 Iter 1833, Minibatch Loss= 0.0148, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:32:20,076 Iter 1834, Minibatch Loss= 0.0132, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 18:32:21,569 Iter 1835, Minibatch Loss= 0.0169, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 18:32:23,057 Iter 1836, Minibatch Loss= 0.0163, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 18:32:24,554 Iter 1837, Minibatch Loss= 0.0146, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:32:26,029 Iter 1838, Minibatch Loss= 0.0067, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 18:32:27,543 Iter 1839, Minibatch Loss= 0.0152, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:32:27,544 Epoch 91, Average loss: 0.0140, learning rate: 0.0010\n",
      "2018-05-27 18:32:37,092 Iter 1840, Minibatch Loss= 0.0113, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:32:40,619 Iter 1840, Minibatch Loss= 0.0103, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:32:42,138 Iter 1841, Minibatch Loss= 0.0116, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 18:32:43,633 Iter 1842, Minibatch Loss= 0.0177, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:32:45,152 Iter 1843, Minibatch Loss= 0.0082, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 18:32:46,651 Iter 1844, Minibatch Loss= 0.0101, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:32:48,133 Iter 1845, Minibatch Loss= 0.0183, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 18:32:49,680 Iter 1846, Minibatch Loss= 0.0081, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:32:51,188 Iter 1847, Minibatch Loss= 0.0014, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 18:32:52,686 Iter 1848, Minibatch Loss= 0.0153, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:32:54,149 Iter 1849, Minibatch Loss= 0.0016, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 18:32:55,634 Iter 1850, Minibatch Loss= 0.0073, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 18:32:57,165 Iter 1851, Minibatch Loss= 0.0162, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 18:32:58,622 Iter 1852, Minibatch Loss= 0.0183, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:33:00,122 Iter 1853, Minibatch Loss= 0.0151, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:33:01,608 Iter 1854, Minibatch Loss= 0.0121, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:33:03,115 Iter 1855, Minibatch Loss= 0.0255, Training Accuracy= 0.9414, Minibatch error= 5.9%\n",
      "2018-05-27 18:33:04,598 Iter 1856, Minibatch Loss= 0.0230, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 18:33:06,068 Iter 1857, Minibatch Loss= 0.0120, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:33:07,605 Iter 1858, Minibatch Loss= 0.0162, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 18:33:09,099 Iter 1859, Minibatch Loss= 0.0173, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:33:09,100 Epoch 92, Average loss: 0.0134, learning rate: 0.0010\n",
      "2018-05-27 18:33:18,610 Iter 1860, Minibatch Loss= 0.0120, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 18:33:22,088 Iter 1860, Minibatch Loss= 0.0059, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:33:23,610 Iter 1861, Minibatch Loss= 0.0161, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:33:25,118 Iter 1862, Minibatch Loss= 0.0120, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 18:33:26,596 Iter 1863, Minibatch Loss= 0.0073, Training Accuracy= 0.9895, Minibatch error= 1.1%\n",
      "2018-05-27 18:33:28,076 Iter 1864, Minibatch Loss= 0.0091, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:33:29,566 Iter 1865, Minibatch Loss= 0.0051, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 18:33:31,045 Iter 1866, Minibatch Loss= 0.0219, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 18:33:32,555 Iter 1867, Minibatch Loss= 0.0102, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 18:33:34,030 Iter 1868, Minibatch Loss= 0.0138, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:33:35,531 Iter 1869, Minibatch Loss= 0.0084, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:33:37,022 Iter 1870, Minibatch Loss= 0.0040, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 18:33:38,499 Iter 1871, Minibatch Loss= 0.0139, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 18:33:40,010 Iter 1872, Minibatch Loss= 0.0118, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 18:33:41,538 Iter 1873, Minibatch Loss= 0.0136, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:33:43,062 Iter 1874, Minibatch Loss= 0.0117, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:33:44,571 Iter 1875, Minibatch Loss= 0.0083, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:33:46,040 Iter 1876, Minibatch Loss= 0.0174, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:33:47,557 Iter 1877, Minibatch Loss= 0.0222, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:33:49,072 Iter 1878, Minibatch Loss= 0.0165, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:33:50,641 Iter 1879, Minibatch Loss= 0.0120, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 18:33:50,642 Epoch 93, Average loss: 0.0125, learning rate: 0.0010\n",
      "2018-05-27 18:34:00,149 Iter 1880, Minibatch Loss= 0.0116, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:34:03,669 Iter 1880, Minibatch Loss= 0.0052, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:34:05,155 Iter 1881, Minibatch Loss= 0.0148, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 18:34:06,655 Iter 1882, Minibatch Loss= 0.0198, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:34:08,189 Iter 1883, Minibatch Loss= 0.0176, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 18:34:09,675 Iter 1884, Minibatch Loss= 0.0182, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:34:11,184 Iter 1885, Minibatch Loss= 0.0173, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:34:12,681 Iter 1886, Minibatch Loss= 0.0191, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:34:14,137 Iter 1887, Minibatch Loss= 0.0166, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:34:15,634 Iter 1888, Minibatch Loss= 0.0017, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 18:34:17,125 Iter 1889, Minibatch Loss= 0.0122, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 18:34:18,627 Iter 1890, Minibatch Loss= 0.0216, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 18:34:20,207 Iter 1891, Minibatch Loss= 0.0144, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 18:34:21,680 Iter 1892, Minibatch Loss= 0.0170, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:34:23,191 Iter 1893, Minibatch Loss= 0.0139, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 18:34:24,679 Iter 1894, Minibatch Loss= 0.0171, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 18:34:26,165 Iter 1895, Minibatch Loss= 0.0184, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:34:27,662 Iter 1896, Minibatch Loss= 0.0021, Training Accuracy= 0.9978, Minibatch error= 0.2%\n",
      "2018-05-27 18:34:29,176 Iter 1897, Minibatch Loss= 0.0076, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 18:34:30,689 Iter 1898, Minibatch Loss= 0.0170, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 18:34:32,160 Iter 1899, Minibatch Loss= 0.0169, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:34:32,161 Epoch 94, Average loss: 0.0146, learning rate: 0.0010\n",
      "2018-05-27 18:34:41,607 Iter 1900, Minibatch Loss= 0.0115, Training Accuracy= 0.9814, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:34:45,223 Iter 1900, Minibatch Loss= 0.0089, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 18:34:46,694 Iter 1901, Minibatch Loss= 0.0154, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:34:48,192 Iter 1902, Minibatch Loss= 0.0166, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 18:34:49,709 Iter 1903, Minibatch Loss= 0.0110, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:34:51,193 Iter 1904, Minibatch Loss= 0.0077, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:34:52,684 Iter 1905, Minibatch Loss= 0.0167, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:34:54,160 Iter 1906, Minibatch Loss= 0.0160, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:34:55,665 Iter 1907, Minibatch Loss= 0.0138, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:34:57,173 Iter 1908, Minibatch Loss= 0.0074, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 18:34:58,661 Iter 1909, Minibatch Loss= 0.0159, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 18:35:00,141 Iter 1910, Minibatch Loss= 0.0247, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:35:01,616 Iter 1911, Minibatch Loss= 0.0184, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:35:03,138 Iter 1912, Minibatch Loss= 0.0069, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 18:35:04,633 Iter 1913, Minibatch Loss= 0.0161, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:35:06,112 Iter 1914, Minibatch Loss= 0.0045, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 18:35:07,622 Iter 1915, Minibatch Loss= 0.0093, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 18:35:09,130 Iter 1916, Minibatch Loss= 0.0151, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:35:10,632 Iter 1917, Minibatch Loss= 0.0107, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 18:35:12,114 Iter 1918, Minibatch Loss= 0.0168, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:35:13,619 Iter 1919, Minibatch Loss= 0.0196, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:35:13,620 Epoch 95, Average loss: 0.0138, learning rate: 0.0010\n",
      "2018-05-27 18:35:23,173 Iter 1920, Minibatch Loss= 0.0120, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 18:35:26,714 Iter 1920, Minibatch Loss= 0.0173, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:35:28,220 Iter 1921, Minibatch Loss= 0.0053, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:35:29,716 Iter 1922, Minibatch Loss= 0.0084, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:35:31,199 Iter 1923, Minibatch Loss= 0.0186, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:35:32,713 Iter 1924, Minibatch Loss= 0.0157, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 18:35:34,235 Iter 1925, Minibatch Loss= 0.0162, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:35:35,743 Iter 1926, Minibatch Loss= 0.0157, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:35:37,249 Iter 1927, Minibatch Loss= 0.0154, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:35:38,773 Iter 1928, Minibatch Loss= 0.0053, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 18:35:40,279 Iter 1929, Minibatch Loss= 0.0178, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:35:41,801 Iter 1930, Minibatch Loss= 0.0152, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 18:35:43,328 Iter 1931, Minibatch Loss= 0.0083, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 18:35:44,821 Iter 1932, Minibatch Loss= 0.0169, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:35:46,345 Iter 1933, Minibatch Loss= 0.0114, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:35:47,838 Iter 1934, Minibatch Loss= 0.0161, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 18:35:49,389 Iter 1935, Minibatch Loss= 0.0153, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 18:35:50,881 Iter 1936, Minibatch Loss= 0.0142, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:35:52,386 Iter 1937, Minibatch Loss= 0.0026, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 18:35:53,894 Iter 1938, Minibatch Loss= 0.0176, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:35:55,421 Iter 1939, Minibatch Loss= 0.0070, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:35:55,422 Epoch 96, Average loss: 0.0133, learning rate: 0.0010\n",
      "2018-05-27 18:36:04,964 Iter 1940, Minibatch Loss= 0.0111, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 18:36:08,546 Iter 1940, Minibatch Loss= 0.0106, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 18:36:10,052 Iter 1941, Minibatch Loss= 0.0163, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:36:11,585 Iter 1942, Minibatch Loss= 0.0177, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:36:13,117 Iter 1943, Minibatch Loss= 0.0093, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 18:36:14,625 Iter 1944, Minibatch Loss= 0.0039, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 18:36:16,151 Iter 1945, Minibatch Loss= 0.0170, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:36:17,660 Iter 1946, Minibatch Loss= 0.0167, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:36:19,230 Iter 1947, Minibatch Loss= 0.0195, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:36:20,792 Iter 1948, Minibatch Loss= 0.0156, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:36:22,296 Iter 1949, Minibatch Loss= 0.0149, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:36:23,812 Iter 1950, Minibatch Loss= 0.0165, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:36:25,323 Iter 1951, Minibatch Loss= 0.0370, Training Accuracy= 0.9342, Minibatch error= 6.6%\n",
      "2018-05-27 18:36:26,844 Iter 1952, Minibatch Loss= 0.0157, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 18:36:28,383 Iter 1953, Minibatch Loss= 0.0079, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:36:29,884 Iter 1954, Minibatch Loss= 0.0202, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 18:36:31,381 Iter 1955, Minibatch Loss= 0.0153, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:36:32,895 Iter 1956, Minibatch Loss= 0.0188, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:36:34,408 Iter 1957, Minibatch Loss= 0.0114, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:36:35,931 Iter 1958, Minibatch Loss= 0.0076, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:36:37,454 Iter 1959, Minibatch Loss= 0.0058, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 18:36:37,455 Epoch 97, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 18:36:47,068 Iter 1960, Minibatch Loss= 0.0110, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:36:50,696 Iter 1960, Minibatch Loss= 0.0147, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:36:52,235 Iter 1961, Minibatch Loss= 0.0173, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:36:53,780 Iter 1962, Minibatch Loss= 0.0171, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:36:55,288 Iter 1963, Minibatch Loss= 0.0138, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 18:36:56,858 Iter 1964, Minibatch Loss= 0.0173, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:36:58,396 Iter 1965, Minibatch Loss= 0.0190, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:36:59,896 Iter 1966, Minibatch Loss= 0.0183, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 18:37:01,446 Iter 1967, Minibatch Loss= 0.0224, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 18:37:02,965 Iter 1968, Minibatch Loss= 0.0106, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 18:37:04,516 Iter 1969, Minibatch Loss= 0.0062, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:37:06,071 Iter 1970, Minibatch Loss= 0.0140, Training Accuracy= 0.9763, Minibatch error= 2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:37:07,591 Iter 1971, Minibatch Loss= 0.0066, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 18:37:09,131 Iter 1972, Minibatch Loss= 0.0075, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 18:37:10,647 Iter 1973, Minibatch Loss= 0.0155, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:37:12,191 Iter 1974, Minibatch Loss= 0.0175, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:37:13,729 Iter 1975, Minibatch Loss= 0.0196, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:37:15,246 Iter 1976, Minibatch Loss= 0.0101, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:37:16,794 Iter 1977, Minibatch Loss= 0.0089, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:37:18,314 Iter 1978, Minibatch Loss= 0.0037, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 18:37:19,882 Iter 1979, Minibatch Loss= 0.0126, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:37:19,883 Epoch 98, Average loss: 0.0138, learning rate: 0.0010\n",
      "2018-05-27 18:37:29,428 Iter 1980, Minibatch Loss= 0.0112, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 18:37:33,068 Iter 1980, Minibatch Loss= 0.0125, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 18:37:34,637 Iter 1981, Minibatch Loss= 0.0064, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 18:37:36,144 Iter 1982, Minibatch Loss= 0.0183, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 18:37:37,696 Iter 1983, Minibatch Loss= 0.0134, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 18:37:39,217 Iter 1984, Minibatch Loss= 0.0165, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 18:37:40,760 Iter 1985, Minibatch Loss= 0.0063, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:37:42,306 Iter 1986, Minibatch Loss= 0.0076, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 18:37:43,832 Iter 1987, Minibatch Loss= 0.0095, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 18:37:45,401 Iter 1988, Minibatch Loss= 0.0159, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:37:46,930 Iter 1989, Minibatch Loss= 0.0168, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:37:48,473 Iter 1990, Minibatch Loss= 0.0033, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 18:37:50,082 Iter 1991, Minibatch Loss= 0.0179, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:37:51,606 Iter 1992, Minibatch Loss= 0.0176, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 18:37:53,152 Iter 1993, Minibatch Loss= 0.0070, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:37:54,678 Iter 1994, Minibatch Loss= 0.0076, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:37:56,177 Iter 1995, Minibatch Loss= 0.0091, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 18:37:57,733 Iter 1996, Minibatch Loss= 0.0156, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 18:37:59,262 Iter 1997, Minibatch Loss= 0.0187, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:38:00,799 Iter 1998, Minibatch Loss= 0.0149, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 18:38:02,342 Iter 1999, Minibatch Loss= 0.0157, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 18:38:02,343 Epoch 99, Average loss: 0.0127, learning rate: 0.0010\n",
      "2018-05-27 18:38:11,860 Iter 2000, Minibatch Loss= 0.0113, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:38:13,937 Optimization Finished!\n",
      "2018-05-27 18:38:14,204 Layers 3, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach-- #epoch100-#iter20-lambda1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:38:16,936 Removing '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 18:38:14.199309-#epoch100-#iter20-lambda1e-06'\n",
      "2018-05-27 18:38:16,938 Allocating '/home/javi_fdez_093/deep-learn/outputs/appro100/output2018-05-27 18:38:14.199309-#epoch100-#iter20-lambda1e-06'\n",
      "2018-05-27 18:38:36,247 Verification error= 81.0%, loss= 0.2200\n",
      "2018-05-27 18:38:36,577 Start optimization\n",
      "2018-05-27 18:38:46,337 Iter 0, Minibatch Loss= 0.2200, Training Accuracy= 0.1900, Minibatch error= 81.0%\n",
      "2018-05-27 18:38:47,894 Iter 0, Minibatch Loss= 0.1541, Training Accuracy= 0.2614, Minibatch error= 73.9%\n",
      "2018-05-27 18:38:48,995 Iter 1, Minibatch Loss= 0.1934, Training Accuracy= 0.8659, Minibatch error= 13.4%\n",
      "2018-05-27 18:38:50,083 Iter 2, Minibatch Loss= 0.3017, Training Accuracy= 0.6359, Minibatch error= 36.4%\n",
      "2018-05-27 18:38:51,188 Iter 3, Minibatch Loss= 0.2860, Training Accuracy= 0.3652, Minibatch error= 63.5%\n",
      "2018-05-27 18:38:52,282 Iter 4, Minibatch Loss= 0.3077, Training Accuracy= 0.4425, Minibatch error= 55.8%\n",
      "2018-05-27 18:38:53,375 Iter 5, Minibatch Loss= 0.1948, Training Accuracy= 0.1246, Minibatch error= 87.5%\n",
      "2018-05-27 18:38:54,472 Iter 6, Minibatch Loss= 0.2268, Training Accuracy= 0.2228, Minibatch error= 77.7%\n",
      "2018-05-27 18:38:55,567 Iter 7, Minibatch Loss= 0.1927, Training Accuracy= 0.1211, Minibatch error= 87.9%\n",
      "2018-05-27 18:38:56,663 Iter 8, Minibatch Loss= 0.1608, Training Accuracy= 0.0322, Minibatch error= 96.8%\n",
      "2018-05-27 18:38:57,762 Iter 9, Minibatch Loss= 0.2976, Training Accuracy= 0.4414, Minibatch error= 55.9%\n",
      "2018-05-27 18:38:58,845 Iter 10, Minibatch Loss= 0.1772, Training Accuracy= 0.0847, Minibatch error= 91.5%\n",
      "2018-05-27 18:38:59,972 Iter 11, Minibatch Loss= 0.2785, Training Accuracy= 0.3956, Minibatch error= 60.4%\n",
      "2018-05-27 18:39:01,082 Iter 12, Minibatch Loss= 0.1575, Training Accuracy= 0.0254, Minibatch error= 97.5%\n",
      "2018-05-27 18:39:02,171 Iter 13, Minibatch Loss= 0.1566, Training Accuracy= 0.0311, Minibatch error= 96.9%\n",
      "2018-05-27 18:39:03,261 Iter 14, Minibatch Loss= 0.1531, Training Accuracy= 0.0286, Minibatch error= 97.1%\n",
      "2018-05-27 18:39:04,358 Iter 15, Minibatch Loss= 0.2823, Training Accuracy= 0.4466, Minibatch error= 55.3%\n",
      "2018-05-27 18:39:05,459 Iter 16, Minibatch Loss= 0.1441, Training Accuracy= 0.0032, Minibatch error= 99.7%\n",
      "2018-05-27 18:39:06,559 Iter 17, Minibatch Loss= 0.1508, Training Accuracy= 0.0474, Minibatch error= 95.3%\n",
      "2018-05-27 18:39:07,658 Iter 18, Minibatch Loss= 0.1895, Training Accuracy= 0.3702, Minibatch error= 63.0%\n",
      "2018-05-27 18:39:08,744 Iter 19, Minibatch Loss= 0.1277, Training Accuracy= 0.3034, Minibatch error= 69.7%\n",
      "2018-05-27 18:39:08,745 Epoch 0, Average loss: 0.2123, learning rate: 0.0010\n",
      "2018-05-27 18:39:18,343 Iter 20, Minibatch Loss= 0.1427, Training Accuracy= 0.1900, Minibatch error= 81.0%\n",
      "2018-05-27 18:39:19,889 Iter 20, Minibatch Loss= 0.0999, Training Accuracy= 0.4493, Minibatch error= 55.1%\n",
      "2018-05-27 18:39:20,985 Iter 21, Minibatch Loss= 0.1474, Training Accuracy= 0.9227, Minibatch error= 7.7%\n",
      "2018-05-27 18:39:22,094 Iter 22, Minibatch Loss= 0.1456, Training Accuracy= 0.9417, Minibatch error= 5.8%\n",
      "2018-05-27 18:39:23,197 Iter 23, Minibatch Loss= 0.1371, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:39:24,319 Iter 24, Minibatch Loss= 0.1351, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:39:25,408 Iter 25, Minibatch Loss= 0.1246, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:39:26,499 Iter 26, Minibatch Loss= 0.0911, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 18:39:27,590 Iter 27, Minibatch Loss= 0.1330, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:39:28,674 Iter 28, Minibatch Loss= 0.1277, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:39:29,813 Iter 29, Minibatch Loss= 0.1264, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:39:30,933 Iter 30, Minibatch Loss= 0.0968, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:39:32,034 Iter 31, Minibatch Loss= 0.1008, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 18:39:33,128 Iter 32, Minibatch Loss= 0.0873, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 18:39:34,226 Iter 33, Minibatch Loss= 0.1099, Training Accuracy= 0.9978, Minibatch error= 0.2%\n",
      "2018-05-27 18:39:35,335 Iter 34, Minibatch Loss= 0.1199, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:39:36,440 Iter 35, Minibatch Loss= 0.1073, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 18:39:37,542 Iter 36, Minibatch Loss= 0.0849, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 18:39:38,642 Iter 37, Minibatch Loss= 0.0745, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:39:39,742 Iter 38, Minibatch Loss= 0.0767, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:39:40,860 Iter 39, Minibatch Loss= 0.0649, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:39:40,861 Epoch 1, Average loss: 0.1220, learning rate: 0.0010\n",
      "2018-05-27 18:39:50,421 Iter 40, Minibatch Loss= 0.0633, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 18:39:51,927 Iter 40, Minibatch Loss= 0.0480, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 18:39:53,034 Iter 41, Minibatch Loss= 0.0502, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:39:54,122 Iter 42, Minibatch Loss= 0.0265, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 18:39:55,226 Iter 43, Minibatch Loss= 0.0188, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 18:39:56,336 Iter 44, Minibatch Loss= 0.0206, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 18:39:57,442 Iter 45, Minibatch Loss= 0.0569, Training Accuracy= 0.9735, Minibatch error= 2.7%\n",
      "2018-05-27 18:39:58,544 Iter 46, Minibatch Loss= 0.0243, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:39:59,634 Iter 47, Minibatch Loss= 0.0090, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 18:40:00,749 Iter 48, Minibatch Loss= 0.0149, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 18:40:01,846 Iter 49, Minibatch Loss= 0.0349, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:40:02,949 Iter 50, Minibatch Loss= 0.0583, Training Accuracy= 0.9523, Minibatch error= 4.8%\n",
      "2018-05-27 18:40:04,047 Iter 51, Minibatch Loss= 0.0317, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:40:05,153 Iter 52, Minibatch Loss= 0.0198, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:40:06,304 Iter 53, Minibatch Loss= 0.0258, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:40:07,393 Iter 54, Minibatch Loss= 0.0100, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 18:40:08,507 Iter 55, Minibatch Loss= 0.0155, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 18:40:09,603 Iter 56, Minibatch Loss= 0.0085, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 18:40:10,703 Iter 57, Minibatch Loss= 0.0426, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 18:40:11,808 Iter 58, Minibatch Loss= 0.0280, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 18:40:12,908 Iter 59, Minibatch Loss= 0.0134, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:40:12,909 Epoch 2, Average loss: 0.0350, learning rate: 0.0010\n",
      "2018-05-27 18:40:22,466 Iter 60, Minibatch Loss= 0.0206, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 18:40:24,012 Iter 60, Minibatch Loss= 0.0087, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:40:25,109 Iter 61, Minibatch Loss= 0.0197, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:40:26,211 Iter 62, Minibatch Loss= 0.0181, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:40:27,318 Iter 63, Minibatch Loss= 0.0133, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:40:28,418 Iter 64, Minibatch Loss= 0.0280, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 18:40:29,532 Iter 65, Minibatch Loss= 0.0103, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:40:30,636 Iter 66, Minibatch Loss= 0.0154, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:40:31,726 Iter 67, Minibatch Loss= 0.0271, Training Accuracy= 0.9552, Minibatch error= 4.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:40:32,821 Iter 68, Minibatch Loss= 0.0097, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 18:40:33,933 Iter 69, Minibatch Loss= 0.0264, Training Accuracy= 0.9544, Minibatch error= 4.6%\n",
      "2018-05-27 18:40:35,040 Iter 70, Minibatch Loss= 0.0109, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 18:40:36,148 Iter 71, Minibatch Loss= 0.0412, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 18:40:37,237 Iter 72, Minibatch Loss= 0.0275, Training Accuracy= 0.9497, Minibatch error= 5.0%\n",
      "2018-05-27 18:40:38,336 Iter 73, Minibatch Loss= 0.0294, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 18:40:39,447 Iter 74, Minibatch Loss= 0.0164, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:40:40,536 Iter 75, Minibatch Loss= 0.0262, Training Accuracy= 0.9509, Minibatch error= 4.9%\n",
      "2018-05-27 18:40:41,636 Iter 76, Minibatch Loss= 0.0271, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 18:40:42,726 Iter 77, Minibatch Loss= 0.0088, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 18:40:43,833 Iter 78, Minibatch Loss= 0.0244, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 18:40:44,947 Iter 79, Minibatch Loss= 0.0582, Training Accuracy= 0.9385, Minibatch error= 6.2%\n",
      "2018-05-27 18:40:44,948 Epoch 3, Average loss: 0.0240, learning rate: 0.0010\n",
      "2018-05-27 18:40:54,489 Iter 80, Minibatch Loss= 0.0474, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 18:40:56,051 Iter 80, Minibatch Loss= 0.0288, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 18:40:57,152 Iter 81, Minibatch Loss= 0.0322, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 18:40:58,248 Iter 82, Minibatch Loss= 0.0162, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:40:59,349 Iter 83, Minibatch Loss= 0.0142, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 18:41:00,454 Iter 84, Minibatch Loss= 0.0154, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 18:41:01,562 Iter 85, Minibatch Loss= 0.0376, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:41:02,686 Iter 86, Minibatch Loss= 0.0245, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:41:03,789 Iter 87, Minibatch Loss= 0.0312, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:41:04,891 Iter 88, Minibatch Loss= 0.0200, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 18:41:06,030 Iter 89, Minibatch Loss= 0.0253, Training Accuracy= 0.9535, Minibatch error= 4.7%\n",
      "2018-05-27 18:41:07,138 Iter 90, Minibatch Loss= 0.0122, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:41:08,232 Iter 91, Minibatch Loss= 0.0177, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 18:41:09,321 Iter 92, Minibatch Loss= 0.0172, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 18:41:10,437 Iter 93, Minibatch Loss= 0.0158, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:41:11,535 Iter 94, Minibatch Loss= 0.0281, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 18:41:12,639 Iter 95, Minibatch Loss= 0.0182, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:41:13,743 Iter 96, Minibatch Loss= 0.0054, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 18:41:14,844 Iter 97, Minibatch Loss= 0.0239, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:41:15,950 Iter 98, Minibatch Loss= 0.0395, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:41:17,083 Iter 99, Minibatch Loss= 0.0094, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 18:41:17,084 Epoch 4, Average loss: 0.0257, learning rate: 0.0010\n",
      "2018-05-27 18:41:26,567 Iter 100, Minibatch Loss= 0.0185, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 18:41:28,192 Iter 100, Minibatch Loss= 0.0280, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 18:41:29,276 Iter 101, Minibatch Loss= 0.0086, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 18:41:30,395 Iter 102, Minibatch Loss= 0.0075, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:41:31,498 Iter 103, Minibatch Loss= 0.0116, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 18:41:32,604 Iter 104, Minibatch Loss= 0.0151, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 18:41:33,698 Iter 105, Minibatch Loss= 0.0161, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:41:34,783 Iter 106, Minibatch Loss= 0.0195, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:41:35,901 Iter 107, Minibatch Loss= 0.0250, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 18:41:36,991 Iter 108, Minibatch Loss= 0.0225, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 18:41:38,063 Iter 109, Minibatch Loss= 0.0225, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:41:39,169 Iter 110, Minibatch Loss= 0.0223, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:41:40,291 Iter 111, Minibatch Loss= 0.0279, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 18:41:41,394 Iter 112, Minibatch Loss= 0.0106, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:41:42,500 Iter 113, Minibatch Loss= 0.0113, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:41:43,601 Iter 114, Minibatch Loss= 0.0165, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 18:41:44,695 Iter 115, Minibatch Loss= 0.0299, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 18:41:45,808 Iter 116, Minibatch Loss= 0.0045, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 18:41:46,893 Iter 117, Minibatch Loss= 0.0039, Training Accuracy= 0.9976, Minibatch error= 0.2%\n",
      "2018-05-27 18:41:47,985 Iter 118, Minibatch Loss= 0.0215, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:41:49,082 Iter 119, Minibatch Loss= 0.0107, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:41:49,083 Epoch 5, Average loss: 0.0188, learning rate: 0.0010\n",
      "2018-05-27 18:41:58,500 Iter 120, Minibatch Loss= 0.0164, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:42:00,065 Iter 120, Minibatch Loss= 0.0241, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 18:42:01,166 Iter 121, Minibatch Loss= 0.0235, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:42:02,262 Iter 122, Minibatch Loss= 0.0073, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 18:42:03,366 Iter 123, Minibatch Loss= 0.0131, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 18:42:04,447 Iter 124, Minibatch Loss= 0.0276, Training Accuracy= 0.9495, Minibatch error= 5.0%\n",
      "2018-05-27 18:42:05,550 Iter 125, Minibatch Loss= 0.0107, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 18:42:06,651 Iter 126, Minibatch Loss= 0.0121, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 18:42:07,750 Iter 127, Minibatch Loss= 0.0139, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 18:42:08,839 Iter 128, Minibatch Loss= 0.0279, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 18:42:09,942 Iter 129, Minibatch Loss= 0.0095, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:42:11,047 Iter 130, Minibatch Loss= 0.0095, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:42:12,150 Iter 131, Minibatch Loss= 0.0292, Training Accuracy= 0.9766, Minibatch error= 2.3%\n",
      "2018-05-27 18:42:13,260 Iter 132, Minibatch Loss= 0.0287, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 18:42:14,363 Iter 133, Minibatch Loss= 0.0146, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:42:15,452 Iter 134, Minibatch Loss= 0.0251, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 18:42:16,554 Iter 135, Minibatch Loss= 0.0227, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:42:17,649 Iter 136, Minibatch Loss= 0.0224, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 18:42:18,756 Iter 137, Minibatch Loss= 0.0195, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:42:19,850 Iter 138, Minibatch Loss= 0.0259, Training Accuracy= 0.9533, Minibatch error= 4.7%\n",
      "2018-05-27 18:42:20,950 Iter 139, Minibatch Loss= 0.0117, Training Accuracy= 0.9807, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:42:20,951 Epoch 6, Average loss: 0.0214, learning rate: 0.0010\n",
      "2018-05-27 18:42:30,350 Iter 140, Minibatch Loss= 0.0187, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:42:31,907 Iter 140, Minibatch Loss= 0.0146, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:42:33,009 Iter 141, Minibatch Loss= 0.0237, Training Accuracy= 0.9573, Minibatch error= 4.3%\n",
      "2018-05-27 18:42:34,109 Iter 142, Minibatch Loss= 0.0152, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 18:42:35,219 Iter 143, Minibatch Loss= 0.0118, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:42:36,311 Iter 144, Minibatch Loss= 0.0298, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 18:42:37,400 Iter 145, Minibatch Loss= 0.0196, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:42:38,497 Iter 146, Minibatch Loss= 0.0338, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 18:42:39,592 Iter 147, Minibatch Loss= 0.0204, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:42:40,688 Iter 148, Minibatch Loss= 0.0230, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:42:41,783 Iter 149, Minibatch Loss= 0.0271, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:42:42,881 Iter 150, Minibatch Loss= 0.0108, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:42:43,971 Iter 151, Minibatch Loss= 0.0216, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 18:42:45,067 Iter 152, Minibatch Loss= 0.0132, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:42:46,180 Iter 153, Minibatch Loss= 0.0242, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 18:42:47,279 Iter 154, Minibatch Loss= 0.0208, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 18:42:48,374 Iter 155, Minibatch Loss= 0.0092, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:42:49,467 Iter 156, Minibatch Loss= 0.0228, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:42:50,575 Iter 157, Minibatch Loss= 0.0195, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:42:51,679 Iter 158, Minibatch Loss= 0.0249, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:42:52,801 Iter 159, Minibatch Loss= 0.0212, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 18:42:52,802 Epoch 7, Average loss: 0.0225, learning rate: 0.0010\n",
      "2018-05-27 18:43:02,168 Iter 160, Minibatch Loss= 0.0163, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:43:03,794 Iter 160, Minibatch Loss= 0.0109, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 18:43:04,887 Iter 161, Minibatch Loss= 0.0358, Training Accuracy= 0.9445, Minibatch error= 5.6%\n",
      "2018-05-27 18:43:05,973 Iter 162, Minibatch Loss= 0.0103, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 18:43:07,069 Iter 163, Minibatch Loss= 0.0267, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:43:08,163 Iter 164, Minibatch Loss= 0.0256, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:43:09,249 Iter 165, Minibatch Loss= 0.0250, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 18:43:10,334 Iter 166, Minibatch Loss= 0.0067, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 18:43:11,424 Iter 167, Minibatch Loss= 0.0127, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:43:12,534 Iter 168, Minibatch Loss= 0.0252, Training Accuracy= 0.9654, Minibatch error= 3.5%\n",
      "2018-05-27 18:43:13,625 Iter 169, Minibatch Loss= 0.0048, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 18:43:14,719 Iter 170, Minibatch Loss= 0.0196, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 18:43:15,829 Iter 171, Minibatch Loss= 0.0278, Training Accuracy= 0.9583, Minibatch error= 4.2%\n",
      "2018-05-27 18:43:16,932 Iter 172, Minibatch Loss= 0.0164, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 18:43:18,016 Iter 173, Minibatch Loss= 0.0114, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:43:19,108 Iter 174, Minibatch Loss= 0.0159, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 18:43:20,220 Iter 175, Minibatch Loss= 0.0049, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 18:43:21,326 Iter 176, Minibatch Loss= 0.0285, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:43:22,436 Iter 177, Minibatch Loss= 0.0080, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:43:23,522 Iter 178, Minibatch Loss= 0.0048, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 18:43:24,619 Iter 179, Minibatch Loss= 0.0192, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 18:43:24,620 Epoch 8, Average loss: 0.0183, learning rate: 0.0010\n",
      "2018-05-27 18:43:33,927 Iter 180, Minibatch Loss= 0.0197, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 18:43:35,509 Iter 180, Minibatch Loss= 0.0136, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 18:43:36,606 Iter 181, Minibatch Loss= 0.0309, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:43:37,697 Iter 182, Minibatch Loss= 0.0050, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:43:38,799 Iter 183, Minibatch Loss= 0.0301, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:43:39,894 Iter 184, Minibatch Loss= 0.0245, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 18:43:40,988 Iter 185, Minibatch Loss= 0.0181, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:43:42,077 Iter 186, Minibatch Loss= 0.0287, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 18:43:43,174 Iter 187, Minibatch Loss= 0.0048, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 18:43:44,289 Iter 188, Minibatch Loss= 0.0119, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:43:45,396 Iter 189, Minibatch Loss= 0.0211, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 18:43:46,491 Iter 190, Minibatch Loss= 0.0234, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:43:47,616 Iter 191, Minibatch Loss= 0.0039, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:43:48,698 Iter 192, Minibatch Loss= 0.0246, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:43:49,787 Iter 193, Minibatch Loss= 0.0124, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:43:50,878 Iter 194, Minibatch Loss= 0.0146, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 18:43:52,028 Iter 195, Minibatch Loss= 0.0309, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:43:53,121 Iter 196, Minibatch Loss= 0.0224, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:43:54,230 Iter 197, Minibatch Loss= 0.0273, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 18:43:55,319 Iter 198, Minibatch Loss= 0.0099, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 18:43:56,417 Iter 199, Minibatch Loss= 0.0297, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:43:56,418 Epoch 9, Average loss: 0.0216, learning rate: 0.0010\n",
      "2018-05-27 18:44:05,696 Iter 200, Minibatch Loss= 0.0174, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:44:07,302 Iter 200, Minibatch Loss= 0.0274, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 18:44:08,394 Iter 201, Minibatch Loss= 0.0111, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:44:09,490 Iter 202, Minibatch Loss= 0.0237, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:44:10,588 Iter 203, Minibatch Loss= 0.0244, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:44:11,688 Iter 204, Minibatch Loss= 0.0263, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 18:44:12,792 Iter 205, Minibatch Loss= 0.0257, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 18:44:13,906 Iter 206, Minibatch Loss= 0.0174, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:44:15,025 Iter 207, Minibatch Loss= 0.0090, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 18:44:16,111 Iter 208, Minibatch Loss= 0.0094, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:44:17,212 Iter 209, Minibatch Loss= 0.0241, Training Accuracy= 0.9686, Minibatch error= 3.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:44:18,309 Iter 210, Minibatch Loss= 0.0177, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:44:19,422 Iter 211, Minibatch Loss= 0.0073, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 18:44:20,545 Iter 212, Minibatch Loss= 0.0229, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 18:44:21,644 Iter 213, Minibatch Loss= 0.0213, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:44:22,762 Iter 214, Minibatch Loss= 0.0119, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 18:44:23,866 Iter 215, Minibatch Loss= 0.0057, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 18:44:24,993 Iter 216, Minibatch Loss= 0.0107, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:44:26,091 Iter 217, Minibatch Loss= 0.0223, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:44:27,209 Iter 218, Minibatch Loss= 0.0083, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 18:44:28,322 Iter 219, Minibatch Loss= 0.0083, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 18:44:28,323 Epoch 10, Average loss: 0.0179, learning rate: 0.0010\n",
      "2018-05-27 18:44:37,600 Iter 220, Minibatch Loss= 0.0163, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:44:39,249 Iter 220, Minibatch Loss= 0.0246, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:44:40,334 Iter 221, Minibatch Loss= 0.0215, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:44:41,439 Iter 222, Minibatch Loss= 0.0087, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 18:44:42,553 Iter 223, Minibatch Loss= 0.0023, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 18:44:43,667 Iter 224, Minibatch Loss= 0.0166, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 18:44:44,755 Iter 225, Minibatch Loss= 0.0205, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:44:45,825 Iter 226, Minibatch Loss= 0.0130, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 18:44:46,919 Iter 227, Minibatch Loss= 0.0238, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 18:44:48,042 Iter 228, Minibatch Loss= 0.0238, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 18:44:49,142 Iter 229, Minibatch Loss= 0.0126, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 18:44:50,239 Iter 230, Minibatch Loss= 0.0256, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 18:44:51,351 Iter 231, Minibatch Loss= 0.0205, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:44:52,458 Iter 232, Minibatch Loss= 0.0265, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:44:53,543 Iter 233, Minibatch Loss= 0.0022, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 18:44:54,641 Iter 234, Minibatch Loss= 0.0231, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 18:44:55,718 Iter 235, Minibatch Loss= 0.0119, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:44:56,819 Iter 236, Minibatch Loss= 0.0244, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:44:57,930 Iter 237, Minibatch Loss= 0.0255, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 18:44:59,022 Iter 238, Minibatch Loss= 0.0284, Training Accuracy= 0.9591, Minibatch error= 4.1%\n",
      "2018-05-27 18:45:00,098 Iter 239, Minibatch Loss= 0.0239, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:45:00,100 Epoch 11, Average loss: 0.0202, learning rate: 0.0010\n",
      "2018-05-27 18:45:09,386 Iter 240, Minibatch Loss= 0.0162, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:45:11,102 Iter 240, Minibatch Loss= 0.0145, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:45:12,222 Iter 241, Minibatch Loss= 0.0204, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:45:13,320 Iter 242, Minibatch Loss= 0.0141, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:45:14,421 Iter 243, Minibatch Loss= 0.0340, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:45:15,559 Iter 244, Minibatch Loss= 0.0265, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 18:45:16,654 Iter 245, Minibatch Loss= 0.0241, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:45:17,748 Iter 246, Minibatch Loss= 0.0128, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 18:45:18,868 Iter 247, Minibatch Loss= 0.0229, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 18:45:19,991 Iter 248, Minibatch Loss= 0.0194, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 18:45:21,062 Iter 249, Minibatch Loss= 0.0217, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 18:45:22,167 Iter 250, Minibatch Loss= 0.0219, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 18:45:23,258 Iter 251, Minibatch Loss= 0.0211, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:45:24,367 Iter 252, Minibatch Loss= 0.0017, Training Accuracy= 0.9977, Minibatch error= 0.2%\n",
      "2018-05-27 18:45:25,475 Iter 253, Minibatch Loss= 0.0150, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:45:26,575 Iter 254, Minibatch Loss= 0.0255, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 18:45:27,666 Iter 255, Minibatch Loss= 0.0059, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:45:28,789 Iter 256, Minibatch Loss= 0.0110, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:45:29,896 Iter 257, Minibatch Loss= 0.0061, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 18:45:30,968 Iter 258, Minibatch Loss= 0.0137, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 18:45:32,054 Iter 259, Minibatch Loss= 0.0209, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 18:45:32,055 Epoch 12, Average loss: 0.0191, learning rate: 0.0010\n",
      "2018-05-27 18:45:41,245 Iter 260, Minibatch Loss= 0.0167, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 18:45:42,923 Iter 260, Minibatch Loss= 0.0251, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 18:45:44,014 Iter 261, Minibatch Loss= 0.0058, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:45:45,113 Iter 262, Minibatch Loss= 0.0074, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 18:45:46,202 Iter 263, Minibatch Loss= 0.0275, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 18:45:47,318 Iter 264, Minibatch Loss= 0.0225, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:45:48,442 Iter 265, Minibatch Loss= 0.0124, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 18:45:49,572 Iter 266, Minibatch Loss= 0.0118, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:45:50,691 Iter 267, Minibatch Loss= 0.0252, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 18:45:51,828 Iter 268, Minibatch Loss= 0.0151, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 18:45:52,956 Iter 269, Minibatch Loss= 0.0118, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 18:45:54,141 Iter 270, Minibatch Loss= 0.0015, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 18:45:55,275 Iter 271, Minibatch Loss= 0.0224, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:45:56,391 Iter 272, Minibatch Loss= 0.0219, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:45:57,504 Iter 273, Minibatch Loss= 0.0094, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 18:45:58,624 Iter 274, Minibatch Loss= 0.0125, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 18:45:59,739 Iter 275, Minibatch Loss= 0.0246, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 18:46:00,844 Iter 276, Minibatch Loss= 0.0142, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 18:46:01,956 Iter 277, Minibatch Loss= 0.0107, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:46:03,078 Iter 278, Minibatch Loss= 0.0241, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:46:04,191 Iter 279, Minibatch Loss= 0.0051, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:46:04,193 Epoch 13, Average loss: 0.0168, learning rate: 0.0010\n",
      "2018-05-27 18:46:13,522 Iter 280, Minibatch Loss= 0.0156, Training Accuracy= 0.9776, Minibatch error= 2.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:46:15,241 Iter 280, Minibatch Loss= 0.0030, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 18:46:16,350 Iter 281, Minibatch Loss= 0.0102, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:46:17,460 Iter 282, Minibatch Loss= 0.0126, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 18:46:18,573 Iter 283, Minibatch Loss= 0.0080, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 18:46:19,697 Iter 284, Minibatch Loss= 0.0232, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:46:20,802 Iter 285, Minibatch Loss= 0.0304, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 18:46:21,897 Iter 286, Minibatch Loss= 0.0191, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:46:22,993 Iter 287, Minibatch Loss= 0.0215, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:46:24,118 Iter 288, Minibatch Loss= 0.0134, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 18:46:25,225 Iter 289, Minibatch Loss= 0.0056, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 18:46:26,327 Iter 290, Minibatch Loss= 0.0245, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:46:27,422 Iter 291, Minibatch Loss= 0.0203, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:46:28,525 Iter 292, Minibatch Loss= 0.0105, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 18:46:29,613 Iter 293, Minibatch Loss= 0.0105, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:46:30,718 Iter 294, Minibatch Loss= 0.0230, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:46:31,836 Iter 295, Minibatch Loss= 0.0026, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 18:46:32,950 Iter 296, Minibatch Loss= 0.0237, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:46:34,066 Iter 297, Minibatch Loss= 0.0079, Training Accuracy= 0.9892, Minibatch error= 1.1%\n",
      "2018-05-27 18:46:35,165 Iter 298, Minibatch Loss= 0.0181, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:46:36,266 Iter 299, Minibatch Loss= 0.0116, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:46:36,267 Epoch 14, Average loss: 0.0160, learning rate: 0.0010\n",
      "2018-05-27 18:46:45,436 Iter 300, Minibatch Loss= 0.0157, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 18:46:47,163 Iter 300, Minibatch Loss= 0.0244, Training Accuracy= 0.9605, Minibatch error= 4.0%\n",
      "2018-05-27 18:46:48,261 Iter 301, Minibatch Loss= 0.0116, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 18:46:49,389 Iter 302, Minibatch Loss= 0.0051, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:46:50,501 Iter 303, Minibatch Loss= 0.0285, Training Accuracy= 0.9511, Minibatch error= 4.9%\n",
      "2018-05-27 18:46:51,644 Iter 304, Minibatch Loss= 0.0107, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 18:46:52,753 Iter 305, Minibatch Loss= 0.0241, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:46:53,878 Iter 306, Minibatch Loss= 0.0239, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:46:54,995 Iter 307, Minibatch Loss= 0.0109, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:46:56,126 Iter 308, Minibatch Loss= 0.0109, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 18:46:57,245 Iter 309, Minibatch Loss= 0.0230, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:46:58,363 Iter 310, Minibatch Loss= 0.0221, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 18:46:59,469 Iter 311, Minibatch Loss= 0.0202, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:47:00,581 Iter 312, Minibatch Loss= 0.0085, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 18:47:01,688 Iter 313, Minibatch Loss= 0.0107, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 18:47:02,803 Iter 314, Minibatch Loss= 0.0184, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:47:03,917 Iter 315, Minibatch Loss= 0.0151, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 18:47:05,025 Iter 316, Minibatch Loss= 0.0268, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 18:47:06,137 Iter 317, Minibatch Loss= 0.0086, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:47:07,240 Iter 318, Minibatch Loss= 0.0214, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 18:47:08,357 Iter 319, Minibatch Loss= 0.0026, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 18:47:08,358 Epoch 15, Average loss: 0.0173, learning rate: 0.0010\n",
      "2018-05-27 18:47:17,580 Iter 320, Minibatch Loss= 0.0154, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 18:47:19,295 Iter 320, Minibatch Loss= 0.0044, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 18:47:20,399 Iter 321, Minibatch Loss= 0.0052, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 18:47:21,517 Iter 322, Minibatch Loss= 0.0232, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:47:22,635 Iter 323, Minibatch Loss= 0.0246, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:47:23,750 Iter 324, Minibatch Loss= 0.0203, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 18:47:24,862 Iter 325, Minibatch Loss= 0.0072, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 18:47:25,991 Iter 326, Minibatch Loss= 0.0098, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 18:47:27,095 Iter 327, Minibatch Loss= 0.0252, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 18:47:28,199 Iter 328, Minibatch Loss= 0.0108, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:47:29,319 Iter 329, Minibatch Loss= 0.0079, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 18:47:30,430 Iter 330, Minibatch Loss= 0.0258, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 18:47:31,565 Iter 331, Minibatch Loss= 0.0291, Training Accuracy= 0.9494, Minibatch error= 5.1%\n",
      "2018-05-27 18:47:32,697 Iter 332, Minibatch Loss= 0.0021, Training Accuracy= 0.9992, Minibatch error= 0.1%\n",
      "2018-05-27 18:47:33,807 Iter 333, Minibatch Loss= 0.0236, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:47:34,920 Iter 334, Minibatch Loss= 0.0234, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:47:36,055 Iter 335, Minibatch Loss= 0.0181, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:47:37,187 Iter 336, Minibatch Loss= 0.0214, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 18:47:38,310 Iter 337, Minibatch Loss= 0.0208, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 18:47:39,422 Iter 338, Minibatch Loss= 0.0155, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:47:40,527 Iter 339, Minibatch Loss= 0.0207, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 18:47:40,528 Epoch 16, Average loss: 0.0182, learning rate: 0.0010\n",
      "2018-05-27 18:47:49,785 Iter 340, Minibatch Loss= 0.0155, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:47:51,580 Iter 340, Minibatch Loss= 0.0184, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 18:47:52,701 Iter 341, Minibatch Loss= 0.0232, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 18:47:53,817 Iter 342, Minibatch Loss= 0.0216, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:47:54,924 Iter 343, Minibatch Loss= 0.0230, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 18:47:56,044 Iter 344, Minibatch Loss= 0.0085, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 18:47:57,155 Iter 345, Minibatch Loss= 0.0243, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 18:47:58,275 Iter 346, Minibatch Loss= 0.0246, Training Accuracy= 0.9595, Minibatch error= 4.1%\n",
      "2018-05-27 18:47:59,410 Iter 347, Minibatch Loss= 0.0242, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:48:00,535 Iter 348, Minibatch Loss= 0.0224, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:48:01,657 Iter 349, Minibatch Loss= 0.0085, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 18:48:02,782 Iter 350, Minibatch Loss= 0.0137, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:48:03,920 Iter 351, Minibatch Loss= 0.0212, Training Accuracy= 0.9651, Minibatch error= 3.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:48:05,030 Iter 352, Minibatch Loss= 0.0227, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 18:48:06,145 Iter 353, Minibatch Loss= 0.0107, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 18:48:07,273 Iter 354, Minibatch Loss= 0.0127, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:48:08,385 Iter 355, Minibatch Loss= 0.0273, Training Accuracy= 0.9615, Minibatch error= 3.9%\n",
      "2018-05-27 18:48:09,490 Iter 356, Minibatch Loss= 0.0155, Training Accuracy= 0.9775, Minibatch error= 2.3%\n",
      "2018-05-27 18:48:10,612 Iter 357, Minibatch Loss= 0.0120, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:48:11,712 Iter 358, Minibatch Loss= 0.0107, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:48:12,817 Iter 359, Minibatch Loss= 0.0298, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 18:48:12,818 Epoch 17, Average loss: 0.0197, learning rate: 0.0010\n",
      "2018-05-27 18:48:22,031 Iter 360, Minibatch Loss= 0.0149, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:48:23,820 Iter 360, Minibatch Loss= 0.0118, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:48:24,919 Iter 361, Minibatch Loss= 0.0105, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 18:48:26,024 Iter 362, Minibatch Loss= 0.0096, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:48:27,128 Iter 363, Minibatch Loss= 0.0079, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 18:48:28,261 Iter 364, Minibatch Loss= 0.0110, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 18:48:29,398 Iter 365, Minibatch Loss= 0.0247, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 18:48:30,526 Iter 366, Minibatch Loss= 0.0239, Training Accuracy= 0.9656, Minibatch error= 3.4%\n",
      "2018-05-27 18:48:31,615 Iter 367, Minibatch Loss= 0.0269, Training Accuracy= 0.9578, Minibatch error= 4.2%\n",
      "2018-05-27 18:48:32,716 Iter 368, Minibatch Loss= 0.0109, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 18:48:33,839 Iter 369, Minibatch Loss= 0.0099, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 18:48:34,934 Iter 370, Minibatch Loss= 0.0202, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 18:48:36,068 Iter 371, Minibatch Loss= 0.0205, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 18:48:37,186 Iter 372, Minibatch Loss= 0.0206, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:48:38,320 Iter 373, Minibatch Loss= 0.0159, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 18:48:39,447 Iter 374, Minibatch Loss= 0.0161, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:48:40,571 Iter 375, Minibatch Loss= 0.0226, Training Accuracy= 0.9566, Minibatch error= 4.3%\n",
      "2018-05-27 18:48:41,699 Iter 376, Minibatch Loss= 0.0163, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:48:42,790 Iter 377, Minibatch Loss= 0.0191, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 18:48:43,940 Iter 378, Minibatch Loss= 0.0257, Training Accuracy= 0.9530, Minibatch error= 4.7%\n",
      "2018-05-27 18:48:45,041 Iter 379, Minibatch Loss= 0.0015, Training Accuracy= 0.9981, Minibatch error= 0.2%\n",
      "2018-05-27 18:48:45,043 Epoch 18, Average loss: 0.0167, learning rate: 0.0010\n",
      "2018-05-27 18:48:54,293 Iter 380, Minibatch Loss= 0.0152, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:48:56,108 Iter 380, Minibatch Loss= 0.0236, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 18:48:57,224 Iter 381, Minibatch Loss= 0.0129, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:48:58,343 Iter 382, Minibatch Loss= 0.0033, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 18:48:59,459 Iter 383, Minibatch Loss= 0.0106, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 18:49:00,563 Iter 384, Minibatch Loss= 0.0126, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 18:49:01,652 Iter 385, Minibatch Loss= 0.0049, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 18:49:02,743 Iter 386, Minibatch Loss= 0.0063, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:49:03,895 Iter 387, Minibatch Loss= 0.0238, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 18:49:05,004 Iter 388, Minibatch Loss= 0.0170, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 18:49:06,130 Iter 389, Minibatch Loss= 0.0255, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 18:49:07,223 Iter 390, Minibatch Loss= 0.0131, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 18:49:08,337 Iter 391, Minibatch Loss= 0.0266, Training Accuracy= 0.9490, Minibatch error= 5.1%\n",
      "2018-05-27 18:49:09,415 Iter 392, Minibatch Loss= 0.0212, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 18:49:10,502 Iter 393, Minibatch Loss= 0.0249, Training Accuracy= 0.9577, Minibatch error= 4.2%\n",
      "2018-05-27 18:49:11,580 Iter 394, Minibatch Loss= 0.0424, Training Accuracy= 0.9297, Minibatch error= 7.0%\n",
      "2018-05-27 18:49:12,699 Iter 395, Minibatch Loss= 0.0174, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 18:49:13,806 Iter 396, Minibatch Loss= 0.0166, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 18:49:14,910 Iter 397, Minibatch Loss= 0.0159, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 18:49:15,994 Iter 398, Minibatch Loss= 0.0287, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 18:49:17,067 Iter 399, Minibatch Loss= 0.0140, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 18:49:17,068 Epoch 19, Average loss: 0.0185, learning rate: 0.0010\n",
      "2018-05-27 18:49:26,124 Iter 400, Minibatch Loss= 0.0154, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 18:49:27,890 Iter 400, Minibatch Loss= 0.0075, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 18:49:29,010 Iter 401, Minibatch Loss= 0.0146, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 18:49:30,127 Iter 402, Minibatch Loss= 0.0159, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:49:31,217 Iter 403, Minibatch Loss= 0.0019, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 18:49:32,325 Iter 404, Minibatch Loss= 0.0109, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:49:33,502 Iter 405, Minibatch Loss= 0.0092, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 18:49:34,641 Iter 406, Minibatch Loss= 0.0267, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 18:49:35,776 Iter 407, Minibatch Loss= 0.0242, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 18:49:36,881 Iter 408, Minibatch Loss= 0.0075, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 18:49:37,992 Iter 409, Minibatch Loss= 0.0213, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 18:49:39,120 Iter 410, Minibatch Loss= 0.0131, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:49:40,222 Iter 411, Minibatch Loss= 0.0223, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 18:49:41,350 Iter 412, Minibatch Loss= 0.0216, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:49:42,477 Iter 413, Minibatch Loss= 0.0045, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 18:49:43,601 Iter 414, Minibatch Loss= 0.0206, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:49:44,709 Iter 415, Minibatch Loss= 0.0133, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 18:49:45,839 Iter 416, Minibatch Loss= 0.0096, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:49:46,970 Iter 417, Minibatch Loss= 0.0217, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 18:49:48,087 Iter 418, Minibatch Loss= 0.0231, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:49:49,219 Iter 419, Minibatch Loss= 0.0217, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 18:49:49,220 Epoch 20, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 18:49:58,358 Iter 420, Minibatch Loss= 0.0159, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:50:00,125 Iter 420, Minibatch Loss= 0.0043, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 18:50:01,273 Iter 421, Minibatch Loss= 0.0076, Training Accuracy= 0.9880, Minibatch error= 1.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:50:02,395 Iter 422, Minibatch Loss= 0.0200, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:50:03,600 Iter 423, Minibatch Loss= 0.0282, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 18:50:04,744 Iter 424, Minibatch Loss= 0.0137, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 18:50:05,907 Iter 425, Minibatch Loss= 0.0113, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:50:07,040 Iter 426, Minibatch Loss= 0.0212, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:50:08,179 Iter 427, Minibatch Loss= 0.0089, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:50:09,307 Iter 428, Minibatch Loss= 0.0215, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 18:50:10,439 Iter 429, Minibatch Loss= 0.0047, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 18:50:11,574 Iter 430, Minibatch Loss= 0.0034, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 18:50:12,701 Iter 431, Minibatch Loss= 0.0213, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 18:50:13,836 Iter 432, Minibatch Loss= 0.0120, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 18:50:14,977 Iter 433, Minibatch Loss= 0.0092, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:50:16,135 Iter 434, Minibatch Loss= 0.0143, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:50:17,271 Iter 435, Minibatch Loss= 0.0108, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 18:50:18,414 Iter 436, Minibatch Loss= 0.0221, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:50:19,553 Iter 437, Minibatch Loss= 0.0128, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 18:50:20,680 Iter 438, Minibatch Loss= 0.0223, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 18:50:21,777 Iter 439, Minibatch Loss= 0.0210, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:50:21,778 Epoch 21, Average loss: 0.0134, learning rate: 0.0010\n",
      "2018-05-27 18:50:31,022 Iter 440, Minibatch Loss= 0.0151, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 18:50:32,839 Iter 440, Minibatch Loss= 0.0263, Training Accuracy= 0.9512, Minibatch error= 4.9%\n",
      "2018-05-27 18:50:34,063 Iter 441, Minibatch Loss= 0.0244, Training Accuracy= 0.9528, Minibatch error= 4.7%\n",
      "2018-05-27 18:50:35,185 Iter 442, Minibatch Loss= 0.0222, Training Accuracy= 0.9538, Minibatch error= 4.6%\n",
      "2018-05-27 18:50:36,334 Iter 443, Minibatch Loss= 0.0150, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 18:50:37,452 Iter 444, Minibatch Loss= 0.0216, Training Accuracy= 0.9582, Minibatch error= 4.2%\n",
      "2018-05-27 18:50:38,591 Iter 445, Minibatch Loss= 0.0101, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 18:50:39,727 Iter 446, Minibatch Loss= 0.0234, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 18:50:40,861 Iter 447, Minibatch Loss= 0.0172, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:50:41,991 Iter 448, Minibatch Loss= 0.0229, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:50:43,125 Iter 449, Minibatch Loss= 0.0160, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 18:50:44,242 Iter 450, Minibatch Loss= 0.0030, Training Accuracy= 0.9961, Minibatch error= 0.4%\n",
      "2018-05-27 18:50:45,379 Iter 451, Minibatch Loss= 0.0241, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 18:50:46,492 Iter 452, Minibatch Loss= 0.0063, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:50:47,638 Iter 453, Minibatch Loss= 0.0166, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 18:50:48,759 Iter 454, Minibatch Loss= 0.0134, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 18:50:49,857 Iter 455, Minibatch Loss= 0.0130, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 18:50:50,957 Iter 456, Minibatch Loss= 0.0211, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:50:52,070 Iter 457, Minibatch Loss= 0.0115, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:50:53,189 Iter 458, Minibatch Loss= 0.0042, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 18:50:54,311 Iter 459, Minibatch Loss= 0.0210, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 18:50:54,312 Epoch 22, Average loss: 0.0172, learning rate: 0.0010\n",
      "2018-05-27 18:51:03,378 Iter 460, Minibatch Loss= 0.0150, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 18:51:05,172 Iter 460, Minibatch Loss= 0.0023, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 18:51:06,272 Iter 461, Minibatch Loss= 0.0083, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 18:51:07,371 Iter 462, Minibatch Loss= 0.0225, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:51:08,492 Iter 463, Minibatch Loss= 0.0113, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 18:51:09,604 Iter 464, Minibatch Loss= 0.0219, Training Accuracy= 0.9603, Minibatch error= 4.0%\n",
      "2018-05-27 18:51:10,751 Iter 465, Minibatch Loss= 0.0033, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 18:51:11,849 Iter 466, Minibatch Loss= 0.0233, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 18:51:12,989 Iter 467, Minibatch Loss= 0.0031, Training Accuracy= 0.9943, Minibatch error= 0.6%\n",
      "2018-05-27 18:51:14,098 Iter 468, Minibatch Loss= 0.0107, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 18:51:15,217 Iter 469, Minibatch Loss= 0.0091, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 18:51:16,324 Iter 470, Minibatch Loss= 0.0200, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 18:51:17,476 Iter 471, Minibatch Loss= 0.0232, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 18:51:18,565 Iter 472, Minibatch Loss= 0.0130, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:51:19,706 Iter 473, Minibatch Loss= 0.0234, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 18:51:20,810 Iter 474, Minibatch Loss= 0.0238, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 18:51:21,888 Iter 475, Minibatch Loss= 0.0201, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:51:22,993 Iter 476, Minibatch Loss= 0.0072, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 18:51:24,086 Iter 477, Minibatch Loss= 0.0082, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:51:25,208 Iter 478, Minibatch Loss= 0.0236, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 18:51:26,323 Iter 479, Minibatch Loss= 0.0105, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 18:51:26,324 Epoch 23, Average loss: 0.0148, learning rate: 0.0010\n",
      "2018-05-27 18:51:35,436 Iter 480, Minibatch Loss= 0.0145, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 18:51:37,276 Iter 480, Minibatch Loss= 0.0209, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 18:51:38,439 Iter 481, Minibatch Loss= 0.0121, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 18:51:39,573 Iter 482, Minibatch Loss= 0.0215, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 18:51:40,671 Iter 483, Minibatch Loss= 0.0097, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:51:41,768 Iter 484, Minibatch Loss= 0.0113, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:51:42,865 Iter 485, Minibatch Loss= 0.0255, Training Accuracy= 0.9529, Minibatch error= 4.7%\n",
      "2018-05-27 18:51:43,957 Iter 486, Minibatch Loss= 0.0204, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 18:51:45,043 Iter 487, Minibatch Loss= 0.0055, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:51:46,153 Iter 488, Minibatch Loss= 0.0225, Training Accuracy= 0.9516, Minibatch error= 4.8%\n",
      "2018-05-27 18:51:47,272 Iter 489, Minibatch Loss= 0.0216, Training Accuracy= 0.9585, Minibatch error= 4.1%\n",
      "2018-05-27 18:51:48,393 Iter 490, Minibatch Loss= 0.0189, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:51:49,487 Iter 491, Minibatch Loss= 0.0094, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 18:51:50,590 Iter 492, Minibatch Loss= 0.0242, Training Accuracy= 0.9525, Minibatch error= 4.8%\n",
      "2018-05-27 18:51:51,674 Iter 493, Minibatch Loss= 0.0055, Training Accuracy= 0.9896, Minibatch error= 1.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:51:52,786 Iter 494, Minibatch Loss= 0.0213, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 18:51:53,910 Iter 495, Minibatch Loss= 0.0089, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 18:51:55,010 Iter 496, Minibatch Loss= 0.0246, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 18:51:56,106 Iter 497, Minibatch Loss= 0.0231, Training Accuracy= 0.9616, Minibatch error= 3.8%\n",
      "2018-05-27 18:51:57,217 Iter 498, Minibatch Loss= 0.0167, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:51:58,297 Iter 499, Minibatch Loss= 0.0180, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:51:58,298 Epoch 24, Average loss: 0.0172, learning rate: 0.0010\n",
      "2018-05-27 18:52:07,141 Iter 500, Minibatch Loss= 0.0143, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 18:52:08,973 Iter 500, Minibatch Loss= 0.0232, Training Accuracy= 0.9621, Minibatch error= 3.8%\n",
      "2018-05-27 18:52:10,065 Iter 501, Minibatch Loss= 0.0117, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 18:52:11,240 Iter 502, Minibatch Loss= 0.0206, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:52:12,347 Iter 503, Minibatch Loss= 0.0641, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 18:52:13,454 Iter 504, Minibatch Loss= 0.0270, Training Accuracy= 0.9546, Minibatch error= 4.5%\n",
      "2018-05-27 18:52:14,564 Iter 505, Minibatch Loss= 0.0095, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 18:52:15,693 Iter 506, Minibatch Loss= 0.0101, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 18:52:16,827 Iter 507, Minibatch Loss= 0.0047, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 18:52:17,917 Iter 508, Minibatch Loss= 0.0026, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 18:52:19,054 Iter 509, Minibatch Loss= 0.0211, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 18:52:20,195 Iter 510, Minibatch Loss= 0.0065, Training Accuracy= 0.9897, Minibatch error= 1.0%\n",
      "2018-05-27 18:52:21,278 Iter 511, Minibatch Loss= 0.0220, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 18:52:22,394 Iter 512, Minibatch Loss= 0.0182, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:52:23,546 Iter 513, Minibatch Loss= 0.0240, Training Accuracy= 0.9557, Minibatch error= 4.4%\n",
      "2018-05-27 18:52:24,641 Iter 514, Minibatch Loss= 0.0345, Training Accuracy= 0.9456, Minibatch error= 5.4%\n",
      "2018-05-27 18:52:25,718 Iter 515, Minibatch Loss= 0.0207, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 18:52:26,836 Iter 516, Minibatch Loss= 0.0708, Training Accuracy= 0.9423, Minibatch error= 5.8%\n",
      "2018-05-27 18:52:27,952 Iter 517, Minibatch Loss= 0.0801, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 18:52:29,049 Iter 518, Minibatch Loss= 0.0194, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:52:30,162 Iter 519, Minibatch Loss= 0.0220, Training Accuracy= 0.9601, Minibatch error= 4.0%\n",
      "2018-05-27 18:52:30,163 Epoch 25, Average loss: 0.0161, learning rate: 0.0010\n",
      "2018-05-27 18:52:39,144 Iter 520, Minibatch Loss= 0.0149, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 18:52:41,082 Iter 520, Minibatch Loss= 0.0249, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 18:52:42,215 Iter 521, Minibatch Loss= 0.0221, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 18:52:43,317 Iter 522, Minibatch Loss= 0.0204, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:52:44,436 Iter 523, Minibatch Loss= 0.0091, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:52:45,546 Iter 524, Minibatch Loss= 0.0197, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 18:52:46,662 Iter 525, Minibatch Loss= 0.0024, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 18:52:47,763 Iter 526, Minibatch Loss= 0.0236, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 18:52:48,838 Iter 527, Minibatch Loss= 0.0052, Training Accuracy= 0.9918, Minibatch error= 0.8%\n",
      "2018-05-27 18:52:49,941 Iter 528, Minibatch Loss= 0.0100, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 18:52:51,038 Iter 529, Minibatch Loss= 0.0103, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 18:52:52,184 Iter 530, Minibatch Loss= 0.0048, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:52:53,293 Iter 531, Minibatch Loss= 0.0058, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 18:52:54,424 Iter 532, Minibatch Loss= 0.0227, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 18:52:55,519 Iter 533, Minibatch Loss= 0.0224, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 18:52:56,632 Iter 534, Minibatch Loss= 0.0218, Training Accuracy= 0.9583, Minibatch error= 4.2%\n",
      "2018-05-27 18:52:57,740 Iter 535, Minibatch Loss= 0.0237, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 18:52:58,851 Iter 536, Minibatch Loss= 0.0015, Training Accuracy= 0.9977, Minibatch error= 0.2%\n",
      "2018-05-27 18:52:59,955 Iter 537, Minibatch Loss= 0.0229, Training Accuracy= 0.9564, Minibatch error= 4.4%\n",
      "2018-05-27 18:53:01,036 Iter 538, Minibatch Loss= 0.0244, Training Accuracy= 0.9512, Minibatch error= 4.9%\n",
      "2018-05-27 18:53:02,118 Iter 539, Minibatch Loss= 0.0111, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:53:02,119 Epoch 26, Average loss: 0.0155, learning rate: 0.0010\n",
      "2018-05-27 18:53:10,919 Iter 540, Minibatch Loss= 0.0147, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 18:53:12,755 Iter 540, Minibatch Loss= 0.0126, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 18:53:13,901 Iter 541, Minibatch Loss= 0.0223, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 18:53:15,023 Iter 542, Minibatch Loss= 0.0162, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 18:53:16,130 Iter 543, Minibatch Loss= 0.0187, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 18:53:17,254 Iter 544, Minibatch Loss= 0.0216, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 18:53:18,334 Iter 545, Minibatch Loss= 0.0206, Training Accuracy= 0.9628, Minibatch error= 3.7%\n",
      "2018-05-27 18:53:19,454 Iter 546, Minibatch Loss= 0.0167, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 18:53:20,552 Iter 547, Minibatch Loss= 0.0119, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 18:53:21,740 Iter 548, Minibatch Loss= 0.0222, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 18:53:22,868 Iter 549, Minibatch Loss= 0.0138, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:53:23,989 Iter 550, Minibatch Loss= 0.0093, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 18:53:25,099 Iter 551, Minibatch Loss= 0.0211, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:53:26,213 Iter 552, Minibatch Loss= 0.0189, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:53:27,331 Iter 553, Minibatch Loss= 0.0153, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 18:53:28,472 Iter 554, Minibatch Loss= 0.0046, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 18:53:29,615 Iter 555, Minibatch Loss= 0.0122, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:53:30,755 Iter 556, Minibatch Loss= 0.0179, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 18:53:31,892 Iter 557, Minibatch Loss= 0.0020, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 18:53:33,003 Iter 558, Minibatch Loss= 0.0411, Training Accuracy= 0.9579, Minibatch error= 4.2%\n",
      "2018-05-27 18:53:34,120 Iter 559, Minibatch Loss= 0.0196, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 18:53:34,121 Epoch 27, Average loss: 0.0157, learning rate: 0.0010\n",
      "2018-05-27 18:53:43,215 Iter 560, Minibatch Loss= 0.0170, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 18:53:45,120 Iter 560, Minibatch Loss= 0.0498, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 18:53:46,269 Iter 561, Minibatch Loss= 0.0878, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 18:53:47,373 Iter 562, Minibatch Loss= 0.0816, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:53:48,531 Iter 563, Minibatch Loss= 0.0622, Training Accuracy= 0.9766, Minibatch error= 2.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:53:49,676 Iter 564, Minibatch Loss= 0.0084, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 18:53:50,804 Iter 565, Minibatch Loss= 0.0629, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 18:53:52,013 Iter 566, Minibatch Loss= 0.0992, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 18:53:53,157 Iter 567, Minibatch Loss= 0.0500, Training Accuracy= 0.9526, Minibatch error= 4.7%\n",
      "2018-05-27 18:53:54,290 Iter 568, Minibatch Loss= 0.0079, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:53:55,433 Iter 569, Minibatch Loss= 0.0055, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 18:53:56,577 Iter 570, Minibatch Loss= 0.0194, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 18:53:57,729 Iter 571, Minibatch Loss= 0.0203, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:53:58,880 Iter 572, Minibatch Loss= 0.0237, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 18:54:00,026 Iter 573, Minibatch Loss= 0.0199, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 18:54:01,187 Iter 574, Minibatch Loss= 0.0655, Training Accuracy= 0.9481, Minibatch error= 5.2%\n",
      "2018-05-27 18:54:02,367 Iter 575, Minibatch Loss= 0.0672, Training Accuracy= 0.9501, Minibatch error= 5.0%\n",
      "2018-05-27 18:54:03,546 Iter 576, Minibatch Loss= 0.0402, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 18:54:04,726 Iter 577, Minibatch Loss= 0.0196, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 18:54:05,863 Iter 578, Minibatch Loss= 0.0087, Training Accuracy= 0.9845, Minibatch error= 1.6%\n",
      "2018-05-27 18:54:07,029 Iter 579, Minibatch Loss= 0.0021, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 18:54:07,030 Epoch 28, Average loss: 0.0144, learning rate: 0.0010\n",
      "2018-05-27 18:54:16,149 Iter 580, Minibatch Loss= 0.0139, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 18:54:18,052 Iter 580, Minibatch Loss= 0.0080, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 18:54:19,202 Iter 581, Minibatch Loss= 0.0132, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:54:20,337 Iter 582, Minibatch Loss= 0.0200, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:54:21,460 Iter 583, Minibatch Loss= 0.0165, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:54:22,611 Iter 584, Minibatch Loss= 0.0145, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 18:54:23,736 Iter 585, Minibatch Loss= 0.0209, Training Accuracy= 0.9590, Minibatch error= 4.1%\n",
      "2018-05-27 18:54:24,861 Iter 586, Minibatch Loss= 0.0107, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 18:54:26,008 Iter 587, Minibatch Loss= 0.0232, Training Accuracy= 0.9454, Minibatch error= 5.5%\n",
      "2018-05-27 18:54:27,175 Iter 588, Minibatch Loss= 0.0157, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:54:28,320 Iter 589, Minibatch Loss= 0.0221, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 18:54:29,513 Iter 590, Minibatch Loss= 0.0049, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 18:54:30,650 Iter 591, Minibatch Loss= 0.0217, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:54:31,780 Iter 592, Minibatch Loss= 0.0212, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 18:54:32,921 Iter 593, Minibatch Loss= 0.0193, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 18:54:34,079 Iter 594, Minibatch Loss= 0.0210, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 18:54:35,210 Iter 595, Minibatch Loss= 0.0242, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 18:54:36,359 Iter 596, Minibatch Loss= 0.0220, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 18:54:37,489 Iter 597, Minibatch Loss= 0.0236, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 18:54:38,649 Iter 598, Minibatch Loss= 0.0174, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 18:54:39,786 Iter 599, Minibatch Loss= 0.0095, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 18:54:39,787 Epoch 29, Average loss: 0.0177, learning rate: 0.0010\n",
      "2018-05-27 18:54:48,887 Iter 600, Minibatch Loss= 0.0145, Training Accuracy= 0.9755, Minibatch error= 2.5%\n",
      "2018-05-27 18:54:50,825 Iter 600, Minibatch Loss= 0.0207, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:54:51,954 Iter 601, Minibatch Loss= 0.0211, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 18:54:53,094 Iter 602, Minibatch Loss= 0.0238, Training Accuracy= 0.9580, Minibatch error= 4.2%\n",
      "2018-05-27 18:54:54,296 Iter 603, Minibatch Loss= 0.0083, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 18:54:55,428 Iter 604, Minibatch Loss= 0.0130, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:54:56,615 Iter 605, Minibatch Loss= 0.0297, Training Accuracy= 0.9503, Minibatch error= 5.0%\n",
      "2018-05-27 18:54:57,760 Iter 606, Minibatch Loss= 0.0077, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 18:54:58,917 Iter 607, Minibatch Loss= 0.0186, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:55:00,029 Iter 608, Minibatch Loss= 0.0058, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 18:55:01,188 Iter 609, Minibatch Loss= 0.0229, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 18:55:02,352 Iter 610, Minibatch Loss= 0.0127, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 18:55:03,543 Iter 611, Minibatch Loss= 0.0226, Training Accuracy= 0.9538, Minibatch error= 4.6%\n",
      "2018-05-27 18:55:04,704 Iter 612, Minibatch Loss= 0.0135, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:55:05,848 Iter 613, Minibatch Loss= 0.0212, Training Accuracy= 0.9519, Minibatch error= 4.8%\n",
      "2018-05-27 18:55:07,012 Iter 614, Minibatch Loss= 0.0177, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:55:08,147 Iter 615, Minibatch Loss= 0.0114, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:55:09,286 Iter 616, Minibatch Loss= 0.0221, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 18:55:10,438 Iter 617, Minibatch Loss= 0.0174, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 18:55:11,600 Iter 618, Minibatch Loss= 0.0139, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 18:55:12,730 Iter 619, Minibatch Loss= 0.0189, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 18:55:12,731 Epoch 30, Average loss: 0.0174, learning rate: 0.0010\n",
      "2018-05-27 18:55:21,677 Iter 620, Minibatch Loss= 0.0140, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 18:55:23,661 Iter 620, Minibatch Loss= 0.0099, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 18:55:24,798 Iter 621, Minibatch Loss= 0.0248, Training Accuracy= 0.9520, Minibatch error= 4.8%\n",
      "2018-05-27 18:55:25,956 Iter 622, Minibatch Loss= 0.0274, Training Accuracy= 0.9427, Minibatch error= 5.7%\n",
      "2018-05-27 18:55:27,060 Iter 623, Minibatch Loss= 0.0161, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 18:55:28,179 Iter 624, Minibatch Loss= 0.0248, Training Accuracy= 0.9549, Minibatch error= 4.5%\n",
      "2018-05-27 18:55:29,329 Iter 625, Minibatch Loss= 0.0201, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 18:55:30,481 Iter 626, Minibatch Loss= 0.0108, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 18:55:31,627 Iter 627, Minibatch Loss= 0.0217, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 18:55:32,786 Iter 628, Minibatch Loss= 0.0077, Training Accuracy= 0.9865, Minibatch error= 1.4%\n",
      "2018-05-27 18:55:33,960 Iter 629, Minibatch Loss= 0.0106, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 18:55:35,085 Iter 630, Minibatch Loss= 0.0240, Training Accuracy= 0.9622, Minibatch error= 3.8%\n",
      "2018-05-27 18:55:36,220 Iter 631, Minibatch Loss= 0.0161, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 18:55:37,381 Iter 632, Minibatch Loss= 0.0197, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 18:55:38,573 Iter 633, Minibatch Loss= 0.0202, Training Accuracy= 0.9641, Minibatch error= 3.6%\n",
      "2018-05-27 18:55:39,744 Iter 634, Minibatch Loss= 0.0286, Training Accuracy= 0.9409, Minibatch error= 5.9%\n",
      "2018-05-27 18:55:40,913 Iter 635, Minibatch Loss= 0.1136, Training Accuracy= 0.9722, Minibatch error= 2.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:55:42,066 Iter 636, Minibatch Loss= 0.0248, Training Accuracy= 0.9519, Minibatch error= 4.8%\n",
      "2018-05-27 18:55:43,253 Iter 637, Minibatch Loss= 0.0215, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 18:55:44,425 Iter 638, Minibatch Loss= 0.0208, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:55:45,592 Iter 639, Minibatch Loss= 0.0028, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 18:55:45,593 Epoch 31, Average loss: 0.0179, learning rate: 0.0010\n",
      "2018-05-27 18:55:54,695 Iter 640, Minibatch Loss= 0.0147, Training Accuracy= 0.9806, Minibatch error= 1.9%\n",
      "2018-05-27 18:55:56,706 Iter 640, Minibatch Loss= 0.0107, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 18:55:57,853 Iter 641, Minibatch Loss= 0.0387, Training Accuracy= 0.9458, Minibatch error= 5.4%\n",
      "2018-05-27 18:55:59,030 Iter 642, Minibatch Loss= 0.0203, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 18:56:00,155 Iter 643, Minibatch Loss= 0.0210, Training Accuracy= 0.9536, Minibatch error= 4.6%\n",
      "2018-05-27 18:56:01,340 Iter 644, Minibatch Loss= 0.0129, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 18:56:02,518 Iter 645, Minibatch Loss= 0.0260, Training Accuracy= 0.9346, Minibatch error= 6.5%\n",
      "2018-05-27 18:56:03,683 Iter 646, Minibatch Loss= 0.0031, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 18:56:04,824 Iter 647, Minibatch Loss= 0.0083, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 18:56:05,960 Iter 648, Minibatch Loss= 0.0130, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 18:56:07,088 Iter 649, Minibatch Loss= 0.0021, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 18:56:08,238 Iter 650, Minibatch Loss= 0.0206, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:56:09,409 Iter 651, Minibatch Loss= 0.0211, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 18:56:10,557 Iter 652, Minibatch Loss= 0.0236, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 18:56:11,722 Iter 653, Minibatch Loss= 0.0113, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 18:56:12,884 Iter 654, Minibatch Loss= 0.0239, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 18:56:14,028 Iter 655, Minibatch Loss= 0.0243, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 18:56:15,161 Iter 656, Minibatch Loss= 0.0078, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 18:56:16,327 Iter 657, Minibatch Loss= 0.0103, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 18:56:17,495 Iter 658, Minibatch Loss= 0.0250, Training Accuracy= 0.9556, Minibatch error= 4.4%\n",
      "2018-05-27 18:56:18,650 Iter 659, Minibatch Loss= 0.0109, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:56:18,651 Epoch 32, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 18:56:27,639 Iter 660, Minibatch Loss= 0.0154, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 18:56:29,622 Iter 660, Minibatch Loss= 0.0055, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 18:56:30,768 Iter 661, Minibatch Loss= 0.0148, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 18:56:31,931 Iter 662, Minibatch Loss= 0.0168, Training Accuracy= 0.9771, Minibatch error= 2.3%\n",
      "2018-05-27 18:56:33,067 Iter 663, Minibatch Loss= 0.0238, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 18:56:34,191 Iter 664, Minibatch Loss= 0.0076, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 18:56:35,324 Iter 665, Minibatch Loss= 0.0090, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 18:56:36,465 Iter 666, Minibatch Loss= 0.0213, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 18:56:37,590 Iter 667, Minibatch Loss= 0.0218, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:56:38,757 Iter 668, Minibatch Loss= 0.0140, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 18:56:39,937 Iter 669, Minibatch Loss= 0.0110, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 18:56:41,087 Iter 670, Minibatch Loss= 0.0161, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:56:42,257 Iter 671, Minibatch Loss= 0.0237, Training Accuracy= 0.9492, Minibatch error= 5.1%\n",
      "2018-05-27 18:56:43,424 Iter 672, Minibatch Loss= 0.0207, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 18:56:44,571 Iter 673, Minibatch Loss= 0.0190, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 18:56:45,738 Iter 674, Minibatch Loss= 0.0088, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:56:46,896 Iter 675, Minibatch Loss= 0.0218, Training Accuracy= 0.9618, Minibatch error= 3.8%\n",
      "2018-05-27 18:56:48,035 Iter 676, Minibatch Loss= 0.0121, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 18:56:49,191 Iter 677, Minibatch Loss= 0.0099, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:56:50,344 Iter 678, Minibatch Loss= 0.0114, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 18:56:51,497 Iter 679, Minibatch Loss= 0.0198, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 18:56:51,497 Epoch 33, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 18:57:00,618 Iter 680, Minibatch Loss= 0.0138, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 18:57:02,900 Iter 680, Minibatch Loss= 0.0121, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 18:57:04,045 Iter 681, Minibatch Loss= 0.0213, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 18:57:05,207 Iter 682, Minibatch Loss= 0.0048, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 18:57:06,395 Iter 683, Minibatch Loss= 0.0240, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 18:57:07,579 Iter 684, Minibatch Loss= 0.0207, Training Accuracy= 0.9535, Minibatch error= 4.6%\n",
      "2018-05-27 18:57:08,760 Iter 685, Minibatch Loss= 0.0259, Training Accuracy= 0.9412, Minibatch error= 5.9%\n",
      "2018-05-27 18:57:09,927 Iter 686, Minibatch Loss= 0.0125, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 18:57:11,070 Iter 687, Minibatch Loss= 0.0275, Training Accuracy= 0.9524, Minibatch error= 4.8%\n",
      "2018-05-27 18:57:12,238 Iter 688, Minibatch Loss= 0.0101, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:57:13,406 Iter 689, Minibatch Loss= 0.0205, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:57:14,596 Iter 690, Minibatch Loss= 0.0237, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 18:57:15,779 Iter 691, Minibatch Loss= 0.0228, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:57:16,968 Iter 692, Minibatch Loss= 0.0186, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:57:18,163 Iter 693, Minibatch Loss= 0.0023, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 18:57:19,356 Iter 694, Minibatch Loss= 0.0161, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 18:57:20,557 Iter 695, Minibatch Loss= 0.0200, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 18:57:21,740 Iter 696, Minibatch Loss= 0.0039, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 18:57:22,957 Iter 697, Minibatch Loss= 0.0218, Training Accuracy= 0.9649, Minibatch error= 3.5%\n",
      "2018-05-27 18:57:24,129 Iter 698, Minibatch Loss= 0.0126, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 18:57:25,303 Iter 699, Minibatch Loss= 0.0193, Training Accuracy= 0.9755, Minibatch error= 2.4%\n",
      "2018-05-27 18:57:25,304 Epoch 34, Average loss: 0.0172, learning rate: 0.0010\n",
      "2018-05-27 18:57:34,407 Iter 700, Minibatch Loss= 0.0140, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 18:57:36,403 Iter 700, Minibatch Loss= 0.0050, Training Accuracy= 0.9925, Minibatch error= 0.8%\n",
      "2018-05-27 18:57:37,575 Iter 701, Minibatch Loss= 0.0121, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 18:57:38,727 Iter 702, Minibatch Loss= 0.0237, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 18:57:39,885 Iter 703, Minibatch Loss= 0.0200, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 18:57:41,034 Iter 704, Minibatch Loss= 0.0096, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:57:42,163 Iter 705, Minibatch Loss= 0.0197, Training Accuracy= 0.9570, Minibatch error= 4.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:57:43,339 Iter 706, Minibatch Loss= 0.0305, Training Accuracy= 0.9283, Minibatch error= 7.2%\n",
      "2018-05-27 18:57:44,496 Iter 707, Minibatch Loss= 0.0175, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 18:57:45,624 Iter 708, Minibatch Loss= 0.0136, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 18:57:46,755 Iter 709, Minibatch Loss= 0.0241, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 18:57:47,875 Iter 710, Minibatch Loss= 0.0150, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 18:57:49,026 Iter 711, Minibatch Loss= 0.0214, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 18:57:50,126 Iter 712, Minibatch Loss= 0.0055, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 18:57:51,282 Iter 713, Minibatch Loss= 0.0230, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 18:57:52,462 Iter 714, Minibatch Loss= 0.0184, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 18:57:53,608 Iter 715, Minibatch Loss= 0.0116, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 18:57:54,748 Iter 716, Minibatch Loss= 0.0092, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 18:57:55,897 Iter 717, Minibatch Loss= 0.0194, Training Accuracy= 0.9570, Minibatch error= 4.3%\n",
      "2018-05-27 18:57:57,034 Iter 718, Minibatch Loss= 0.0160, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 18:57:58,198 Iter 719, Minibatch Loss= 0.0030, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 18:57:58,199 Epoch 35, Average loss: 0.0162, learning rate: 0.0010\n",
      "2018-05-27 18:58:07,287 Iter 720, Minibatch Loss= 0.0136, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 18:58:09,368 Iter 720, Minibatch Loss= 0.0257, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 18:58:10,553 Iter 721, Minibatch Loss= 0.0038, Training Accuracy= 0.9945, Minibatch error= 0.5%\n",
      "2018-05-27 18:58:11,720 Iter 722, Minibatch Loss= 0.0296, Training Accuracy= 0.9545, Minibatch error= 4.5%\n",
      "2018-05-27 18:58:12,905 Iter 723, Minibatch Loss= 0.0220, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 18:58:14,104 Iter 724, Minibatch Loss= 0.0219, Training Accuracy= 0.9544, Minibatch error= 4.6%\n",
      "2018-05-27 18:58:15,303 Iter 725, Minibatch Loss= 0.0207, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 18:58:16,474 Iter 726, Minibatch Loss= 0.0077, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 18:58:17,644 Iter 727, Minibatch Loss= 0.0079, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 18:58:18,804 Iter 728, Minibatch Loss= 0.0267, Training Accuracy= 0.9571, Minibatch error= 4.3%\n",
      "2018-05-27 18:58:19,980 Iter 729, Minibatch Loss= 0.0112, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 18:58:21,166 Iter 730, Minibatch Loss= 0.0223, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 18:58:22,347 Iter 731, Minibatch Loss= 0.0209, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 18:58:23,532 Iter 732, Minibatch Loss= 0.0063, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 18:58:24,731 Iter 733, Minibatch Loss= 0.0196, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 18:58:25,901 Iter 734, Minibatch Loss= 0.0137, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 18:58:27,069 Iter 735, Minibatch Loss= 0.0198, Training Accuracy= 0.9597, Minibatch error= 4.0%\n",
      "2018-05-27 18:58:28,274 Iter 736, Minibatch Loss= 0.0172, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:58:29,474 Iter 737, Minibatch Loss= 0.0065, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 18:58:30,658 Iter 738, Minibatch Loss= 0.0222, Training Accuracy= 0.9584, Minibatch error= 4.2%\n",
      "2018-05-27 18:58:31,856 Iter 739, Minibatch Loss= 0.0177, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 18:58:31,857 Epoch 36, Average loss: 0.0175, learning rate: 0.0010\n",
      "2018-05-27 18:58:41,044 Iter 740, Minibatch Loss= 0.0137, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 18:58:43,207 Iter 740, Minibatch Loss= 0.0199, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 18:58:44,399 Iter 741, Minibatch Loss= 0.0239, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 18:58:45,624 Iter 742, Minibatch Loss= 0.0225, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 18:58:46,821 Iter 743, Minibatch Loss= 0.0236, Training Accuracy= 0.9575, Minibatch error= 4.2%\n",
      "2018-05-27 18:58:48,018 Iter 744, Minibatch Loss= 0.0144, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 18:58:49,235 Iter 745, Minibatch Loss= 0.0183, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 18:58:50,480 Iter 746, Minibatch Loss= 0.0201, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 18:58:51,691 Iter 747, Minibatch Loss= 0.0144, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 18:58:52,911 Iter 748, Minibatch Loss= 0.0205, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 18:58:54,100 Iter 749, Minibatch Loss= 0.0189, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 18:58:55,333 Iter 750, Minibatch Loss= 0.0161, Training Accuracy= 0.9769, Minibatch error= 2.3%\n",
      "2018-05-27 18:58:56,534 Iter 751, Minibatch Loss= 0.0248, Training Accuracy= 0.9667, Minibatch error= 3.3%\n",
      "2018-05-27 18:58:57,741 Iter 752, Minibatch Loss= 0.0069, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 18:58:58,935 Iter 753, Minibatch Loss= 0.0117, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 18:59:00,144 Iter 754, Minibatch Loss= 0.0188, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 18:59:01,336 Iter 755, Minibatch Loss= 0.0166, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 18:59:02,526 Iter 756, Minibatch Loss= 0.0216, Training Accuracy= 0.9554, Minibatch error= 4.5%\n",
      "2018-05-27 18:59:03,691 Iter 757, Minibatch Loss= 0.0217, Training Accuracy= 0.9548, Minibatch error= 4.5%\n",
      "2018-05-27 18:59:04,900 Iter 758, Minibatch Loss= 0.0051, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 18:59:06,066 Iter 759, Minibatch Loss= 0.0191, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 18:59:06,067 Epoch 37, Average loss: 0.0181, learning rate: 0.0010\n",
      "2018-05-27 18:59:15,327 Iter 760, Minibatch Loss= 0.0136, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 18:59:17,431 Iter 760, Minibatch Loss= 0.0093, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 18:59:18,613 Iter 761, Minibatch Loss= 0.0009, Training Accuracy= 0.9988, Minibatch error= 0.1%\n",
      "2018-05-27 18:59:19,782 Iter 762, Minibatch Loss= 0.0184, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 18:59:20,980 Iter 763, Minibatch Loss= 0.0222, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 18:59:22,151 Iter 764, Minibatch Loss= 0.0018, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 18:59:23,346 Iter 765, Minibatch Loss= 0.0213, Training Accuracy= 0.9537, Minibatch error= 4.6%\n",
      "2018-05-27 18:59:24,527 Iter 766, Minibatch Loss= 0.0113, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 18:59:25,734 Iter 767, Minibatch Loss= 0.0147, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 18:59:26,931 Iter 768, Minibatch Loss= 0.0101, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 18:59:28,139 Iter 769, Minibatch Loss= 0.0218, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 18:59:29,346 Iter 770, Minibatch Loss= 0.0098, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 18:59:30,567 Iter 771, Minibatch Loss= 0.0220, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 18:59:31,736 Iter 772, Minibatch Loss= 0.0234, Training Accuracy= 0.9568, Minibatch error= 4.3%\n",
      "2018-05-27 18:59:32,922 Iter 773, Minibatch Loss= 0.0226, Training Accuracy= 0.9539, Minibatch error= 4.6%\n",
      "2018-05-27 18:59:34,117 Iter 774, Minibatch Loss= 0.0253, Training Accuracy= 0.9486, Minibatch error= 5.1%\n",
      "2018-05-27 18:59:35,318 Iter 775, Minibatch Loss= 0.0085, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 18:59:36,514 Iter 776, Minibatch Loss= 0.0047, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 18:59:37,709 Iter 777, Minibatch Loss= 0.0215, Training Accuracy= 0.9661, Minibatch error= 3.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 18:59:38,910 Iter 778, Minibatch Loss= 0.0055, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 18:59:40,107 Iter 779, Minibatch Loss= 0.0107, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 18:59:40,108 Epoch 38, Average loss: 0.0143, learning rate: 0.0010\n",
      "2018-05-27 18:59:49,388 Iter 780, Minibatch Loss= 0.0134, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 18:59:51,568 Iter 780, Minibatch Loss= 0.0068, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 18:59:52,764 Iter 781, Minibatch Loss= 0.0087, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 18:59:53,950 Iter 782, Minibatch Loss= 0.0034, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 18:59:55,164 Iter 783, Minibatch Loss= 0.0113, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 18:59:56,369 Iter 784, Minibatch Loss= 0.0152, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 18:59:57,564 Iter 785, Minibatch Loss= 0.0093, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 18:59:58,727 Iter 786, Minibatch Loss= 0.0176, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 18:59:59,914 Iter 787, Minibatch Loss= 0.0138, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 19:00:01,086 Iter 788, Minibatch Loss= 0.0104, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 19:00:02,294 Iter 789, Minibatch Loss= 0.0217, Training Accuracy= 0.9612, Minibatch error= 3.9%\n",
      "2018-05-27 19:00:03,546 Iter 790, Minibatch Loss= 0.0108, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:00:04,786 Iter 791, Minibatch Loss= 0.0108, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 19:00:06,003 Iter 792, Minibatch Loss= 0.0201, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 19:00:07,205 Iter 793, Minibatch Loss= 0.0083, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:00:08,385 Iter 794, Minibatch Loss= 0.0042, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:00:09,596 Iter 795, Minibatch Loss= 0.0200, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:00:10,787 Iter 796, Minibatch Loss= 0.0087, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:00:12,008 Iter 797, Minibatch Loss= 0.0136, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:00:13,175 Iter 798, Minibatch Loss= 0.0230, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 19:00:14,357 Iter 799, Minibatch Loss= 0.0178, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:00:14,358 Epoch 39, Average loss: 0.0131, learning rate: 0.0010\n",
      "2018-05-27 19:00:23,514 Iter 800, Minibatch Loss= 0.0138, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 19:00:25,708 Iter 800, Minibatch Loss= 0.0046, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 19:00:26,905 Iter 801, Minibatch Loss= 0.0131, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 19:00:28,103 Iter 802, Minibatch Loss= 0.0160, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 19:00:29,296 Iter 803, Minibatch Loss= 0.0097, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:00:30,530 Iter 804, Minibatch Loss= 0.0104, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 19:00:31,738 Iter 805, Minibatch Loss= 0.0235, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 19:00:32,960 Iter 806, Minibatch Loss= 0.0042, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 19:00:34,145 Iter 807, Minibatch Loss= 0.0191, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 19:00:35,338 Iter 808, Minibatch Loss= 0.0182, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:00:36,549 Iter 809, Minibatch Loss= 0.0208, Training Accuracy= 0.9486, Minibatch error= 5.1%\n",
      "2018-05-27 19:00:37,789 Iter 810, Minibatch Loss= 0.0061, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 19:00:38,987 Iter 811, Minibatch Loss= 0.0225, Training Accuracy= 0.9490, Minibatch error= 5.1%\n",
      "2018-05-27 19:00:40,178 Iter 812, Minibatch Loss= 0.0197, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:00:41,390 Iter 813, Minibatch Loss= 0.0010, Training Accuracy= 0.9990, Minibatch error= 0.1%\n",
      "2018-05-27 19:00:42,592 Iter 814, Minibatch Loss= 0.0073, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 19:00:43,757 Iter 815, Minibatch Loss= 0.0239, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 19:00:44,988 Iter 816, Minibatch Loss= 0.0191, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:00:46,182 Iter 817, Minibatch Loss= 0.0197, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 19:00:47,431 Iter 818, Minibatch Loss= 0.0125, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 19:00:48,636 Iter 819, Minibatch Loss= 0.0230, Training Accuracy= 0.9469, Minibatch error= 5.3%\n",
      "2018-05-27 19:00:48,637 Epoch 40, Average loss: 0.0149, learning rate: 0.0010\n",
      "2018-05-27 19:00:57,923 Iter 820, Minibatch Loss= 0.0147, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 19:01:00,131 Iter 820, Minibatch Loss= 0.0082, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 19:01:01,358 Iter 821, Minibatch Loss= 0.0192, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 19:01:02,608 Iter 822, Minibatch Loss= 0.0042, Training Accuracy= 0.9938, Minibatch error= 0.6%\n",
      "2018-05-27 19:01:03,808 Iter 823, Minibatch Loss= 0.0075, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 19:01:05,000 Iter 824, Minibatch Loss= 0.0142, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 19:01:06,197 Iter 825, Minibatch Loss= 0.0192, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 19:01:07,407 Iter 826, Minibatch Loss= 0.0207, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:01:08,613 Iter 827, Minibatch Loss= 0.0129, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 19:01:09,830 Iter 828, Minibatch Loss= 0.0066, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 19:01:11,034 Iter 829, Minibatch Loss= 0.0093, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 19:01:12,262 Iter 830, Minibatch Loss= 0.0247, Training Accuracy= 0.9514, Minibatch error= 4.9%\n",
      "2018-05-27 19:01:13,481 Iter 831, Minibatch Loss= 0.0114, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 19:01:14,681 Iter 832, Minibatch Loss= 0.0014, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 19:01:15,896 Iter 833, Minibatch Loss= 0.0185, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 19:01:17,125 Iter 834, Minibatch Loss= 0.0215, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 19:01:18,328 Iter 835, Minibatch Loss= 0.0086, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 19:01:19,570 Iter 836, Minibatch Loss= 0.0109, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:01:20,807 Iter 837, Minibatch Loss= 0.0065, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 19:01:22,027 Iter 838, Minibatch Loss= 0.0088, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:01:23,258 Iter 839, Minibatch Loss= 0.0049, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:01:23,259 Epoch 41, Average loss: 0.0123, learning rate: 0.0010\n",
      "2018-05-27 19:01:32,360 Iter 840, Minibatch Loss= 0.0131, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:01:34,561 Iter 840, Minibatch Loss= 0.0203, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:01:35,723 Iter 841, Minibatch Loss= 0.0040, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 19:01:36,888 Iter 842, Minibatch Loss= 0.0197, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 19:01:38,089 Iter 843, Minibatch Loss= 0.0116, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 19:01:39,313 Iter 844, Minibatch Loss= 0.0208, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 19:01:40,485 Iter 845, Minibatch Loss= 0.0051, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 19:01:41,666 Iter 846, Minibatch Loss= 0.0368, Training Accuracy= 0.9417, Minibatch error= 5.8%\n",
      "2018-05-27 19:01:42,865 Iter 847, Minibatch Loss= 0.0129, Training Accuracy= 0.9794, Minibatch error= 2.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:01:44,043 Iter 848, Minibatch Loss= 0.0234, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:01:45,286 Iter 849, Minibatch Loss= 0.0113, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 19:01:46,504 Iter 850, Minibatch Loss= 0.0192, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 19:01:47,694 Iter 851, Minibatch Loss= 0.0241, Training Accuracy= 0.9473, Minibatch error= 5.3%\n",
      "2018-05-27 19:01:48,904 Iter 852, Minibatch Loss= 0.0178, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 19:01:50,118 Iter 853, Minibatch Loss= 0.0206, Training Accuracy= 0.9592, Minibatch error= 4.1%\n",
      "2018-05-27 19:01:51,316 Iter 854, Minibatch Loss= 0.0103, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 19:01:52,564 Iter 855, Minibatch Loss= 0.0199, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 19:01:53,782 Iter 856, Minibatch Loss= 0.0125, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:01:54,973 Iter 857, Minibatch Loss= 0.0212, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 19:01:56,162 Iter 858, Minibatch Loss= 0.0079, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 19:01:57,377 Iter 859, Minibatch Loss= 0.0082, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 19:01:57,378 Epoch 42, Average loss: 0.0168, learning rate: 0.0010\n",
      "2018-05-27 19:02:06,515 Iter 860, Minibatch Loss= 0.0144, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 19:02:08,885 Iter 860, Minibatch Loss= 0.0068, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 19:02:10,106 Iter 861, Minibatch Loss= 0.0220, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 19:02:11,300 Iter 862, Minibatch Loss= 0.0181, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:02:12,527 Iter 863, Minibatch Loss= 0.0119, Training Accuracy= 0.9833, Minibatch error= 1.7%\n",
      "2018-05-27 19:02:13,708 Iter 864, Minibatch Loss= 0.0144, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 19:02:14,972 Iter 865, Minibatch Loss= 0.0222, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 19:02:16,192 Iter 866, Minibatch Loss= 0.0191, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 19:02:17,449 Iter 867, Minibatch Loss= 0.0182, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 19:02:18,648 Iter 868, Minibatch Loss= 0.0211, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 19:02:19,869 Iter 869, Minibatch Loss= 0.0201, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 19:02:21,054 Iter 870, Minibatch Loss= 0.0097, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:02:22,303 Iter 871, Minibatch Loss= 0.0074, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:23,498 Iter 872, Minibatch Loss= 0.0081, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:24,680 Iter 873, Minibatch Loss= 0.0211, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 19:02:25,891 Iter 874, Minibatch Loss= 0.0250, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 19:02:27,079 Iter 875, Minibatch Loss= 0.0164, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 19:02:28,261 Iter 876, Minibatch Loss= 0.0206, Training Accuracy= 0.9596, Minibatch error= 4.0%\n",
      "2018-05-27 19:02:29,500 Iter 877, Minibatch Loss= 0.0143, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 19:02:30,694 Iter 878, Minibatch Loss= 0.0124, Training Accuracy= 0.9789, Minibatch error= 2.1%\n",
      "2018-05-27 19:02:31,887 Iter 879, Minibatch Loss= 0.0028, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 19:02:31,889 Epoch 43, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 19:02:40,785 Iter 880, Minibatch Loss= 0.0128, Training Accuracy= 0.9785, Minibatch error= 2.1%\n",
      "2018-05-27 19:02:42,998 Iter 880, Minibatch Loss= 0.0038, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 19:02:44,212 Iter 881, Minibatch Loss= 0.0071, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:45,432 Iter 882, Minibatch Loss= 0.0079, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:46,635 Iter 883, Minibatch Loss= 0.0187, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 19:02:47,810 Iter 884, Minibatch Loss= 0.0169, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:02:49,038 Iter 885, Minibatch Loss= 0.0076, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:50,234 Iter 886, Minibatch Loss= 0.0075, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:51,462 Iter 887, Minibatch Loss= 0.0182, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 19:02:52,688 Iter 888, Minibatch Loss= 0.0151, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 19:02:53,915 Iter 889, Minibatch Loss= 0.0105, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 19:02:55,142 Iter 890, Minibatch Loss= 0.0028, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 19:02:56,368 Iter 891, Minibatch Loss= 0.0074, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 19:02:57,582 Iter 892, Minibatch Loss= 0.0245, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 19:02:58,791 Iter 893, Minibatch Loss= 0.0224, Training Accuracy= 0.9634, Minibatch error= 3.7%\n",
      "2018-05-27 19:03:00,003 Iter 894, Minibatch Loss= 0.0199, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 19:03:01,217 Iter 895, Minibatch Loss= 0.0208, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 19:03:02,430 Iter 896, Minibatch Loss= 0.0212, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 19:03:03,673 Iter 897, Minibatch Loss= 0.0041, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 19:03:04,894 Iter 898, Minibatch Loss= 0.0134, Training Accuracy= 0.9760, Minibatch error= 2.4%\n",
      "2018-05-27 19:03:06,142 Iter 899, Minibatch Loss= 0.0232, Training Accuracy= 0.9593, Minibatch error= 4.1%\n",
      "2018-05-27 19:03:06,144 Epoch 44, Average loss: 0.0139, learning rate: 0.0010\n",
      "2018-05-27 19:03:15,319 Iter 900, Minibatch Loss= 0.0132, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 19:03:17,667 Iter 900, Minibatch Loss= 0.0187, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:03:18,904 Iter 901, Minibatch Loss= 0.0184, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 19:03:20,148 Iter 902, Minibatch Loss= 0.0127, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 19:03:21,391 Iter 903, Minibatch Loss= 0.0175, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 19:03:22,627 Iter 904, Minibatch Loss= 0.0120, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 19:03:23,876 Iter 905, Minibatch Loss= 0.0134, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 19:03:25,101 Iter 906, Minibatch Loss= 0.0057, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 19:03:26,325 Iter 907, Minibatch Loss= 0.0083, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:03:27,559 Iter 908, Minibatch Loss= 0.0028, Training Accuracy= 0.9955, Minibatch error= 0.4%\n",
      "2018-05-27 19:03:28,814 Iter 909, Minibatch Loss= 0.0200, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 19:03:30,048 Iter 910, Minibatch Loss= 0.0086, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 19:03:31,294 Iter 911, Minibatch Loss= 0.0137, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 19:03:32,549 Iter 912, Minibatch Loss= 0.0190, Training Accuracy= 0.9666, Minibatch error= 3.3%\n",
      "2018-05-27 19:03:33,776 Iter 913, Minibatch Loss= 0.0054, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 19:03:35,014 Iter 914, Minibatch Loss= 0.0211, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:03:36,237 Iter 915, Minibatch Loss= 0.0092, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:03:37,478 Iter 916, Minibatch Loss= 0.0197, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 19:03:38,736 Iter 917, Minibatch Loss= 0.0197, Training Accuracy= 0.9576, Minibatch error= 4.2%\n",
      "2018-05-27 19:03:39,969 Iter 918, Minibatch Loss= 0.0140, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 19:03:41,199 Iter 919, Minibatch Loss= 0.0219, Training Accuracy= 0.9369, Minibatch error= 6.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:03:41,200 Epoch 45, Average loss: 0.0143, learning rate: 0.0010\n",
      "2018-05-27 19:03:50,491 Iter 920, Minibatch Loss= 0.0168, Training Accuracy= 0.9569, Minibatch error= 4.3%\n",
      "2018-05-27 19:03:52,850 Iter 920, Minibatch Loss= 0.0088, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 19:03:54,097 Iter 921, Minibatch Loss= 0.0029, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 19:03:55,342 Iter 922, Minibatch Loss= 0.0196, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:03:56,598 Iter 923, Minibatch Loss= 0.0194, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 19:03:57,828 Iter 924, Minibatch Loss= 0.0200, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:03:59,071 Iter 925, Minibatch Loss= 0.0070, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 19:04:00,300 Iter 926, Minibatch Loss= 0.0207, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:04:01,541 Iter 927, Minibatch Loss= 0.0152, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:04:02,774 Iter 928, Minibatch Loss= 0.0205, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:04:04,035 Iter 929, Minibatch Loss= 0.0212, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 19:04:05,272 Iter 930, Minibatch Loss= 0.0268, Training Accuracy= 0.9509, Minibatch error= 4.9%\n",
      "2018-05-27 19:04:06,521 Iter 931, Minibatch Loss= 0.0098, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 19:04:07,760 Iter 932, Minibatch Loss= 0.0096, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 19:04:09,030 Iter 933, Minibatch Loss= 0.0152, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 19:04:10,286 Iter 934, Minibatch Loss= 0.0226, Training Accuracy= 0.9572, Minibatch error= 4.3%\n",
      "2018-05-27 19:04:11,554 Iter 935, Minibatch Loss= 0.0190, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:04:12,799 Iter 936, Minibatch Loss= 0.0128, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 19:04:14,068 Iter 937, Minibatch Loss= 0.0281, Training Accuracy= 0.9607, Minibatch error= 3.9%\n",
      "2018-05-27 19:04:15,305 Iter 938, Minibatch Loss= 0.0090, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:04:16,552 Iter 939, Minibatch Loss= 0.0236, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:04:16,553 Epoch 46, Average loss: 0.0170, learning rate: 0.0010\n",
      "2018-05-27 19:04:25,861 Iter 940, Minibatch Loss= 0.0132, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 19:04:28,236 Iter 940, Minibatch Loss= 0.0135, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:04:29,487 Iter 941, Minibatch Loss= 0.0181, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:04:30,714 Iter 942, Minibatch Loss= 0.0126, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 19:04:31,957 Iter 943, Minibatch Loss= 0.0105, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:04:33,199 Iter 944, Minibatch Loss= 0.0101, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 19:04:34,445 Iter 945, Minibatch Loss= 0.0252, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 19:04:35,650 Iter 946, Minibatch Loss= 0.0095, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:04:36,903 Iter 947, Minibatch Loss= 0.0078, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:04:38,099 Iter 948, Minibatch Loss= 0.0190, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:04:39,344 Iter 949, Minibatch Loss= 0.0130, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 19:04:40,596 Iter 950, Minibatch Loss= 0.0163, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 19:04:41,852 Iter 951, Minibatch Loss= 0.0123, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 19:04:43,091 Iter 952, Minibatch Loss= 0.0143, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 19:04:44,325 Iter 953, Minibatch Loss= 0.0097, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 19:04:45,529 Iter 954, Minibatch Loss= 0.0162, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 19:04:46,761 Iter 955, Minibatch Loss= 0.0196, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 19:04:48,000 Iter 956, Minibatch Loss= 0.0192, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 19:04:49,264 Iter 957, Minibatch Loss= 0.0082, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 19:04:50,524 Iter 958, Minibatch Loss= 0.0182, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 19:04:51,756 Iter 959, Minibatch Loss= 0.0078, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:04:51,757 Epoch 47, Average loss: 0.0141, learning rate: 0.0010\n",
      "2018-05-27 19:05:00,987 Iter 960, Minibatch Loss= 0.0125, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 19:05:03,388 Iter 960, Minibatch Loss= 0.0027, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 19:05:04,637 Iter 961, Minibatch Loss= 0.0214, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 19:05:05,869 Iter 962, Minibatch Loss= 0.0077, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:05:07,140 Iter 963, Minibatch Loss= 0.0157, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:05:08,371 Iter 964, Minibatch Loss= 0.0103, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 19:05:09,618 Iter 965, Minibatch Loss= 0.0085, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:05:10,864 Iter 966, Minibatch Loss= 0.0100, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 19:05:12,129 Iter 967, Minibatch Loss= 0.0092, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:05:13,333 Iter 968, Minibatch Loss= 0.0172, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:05:14,533 Iter 969, Minibatch Loss= 0.0163, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 19:05:15,717 Iter 970, Minibatch Loss= 0.0102, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 19:05:16,940 Iter 971, Minibatch Loss= 0.0094, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:05:18,149 Iter 972, Minibatch Loss= 0.0124, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:05:19,399 Iter 973, Minibatch Loss= 0.0081, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:05:20,588 Iter 974, Minibatch Loss= 0.0107, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 19:05:21,857 Iter 975, Minibatch Loss= 0.0126, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 19:05:23,082 Iter 976, Minibatch Loss= 0.0072, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:05:24,306 Iter 977, Minibatch Loss= 0.0072, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 19:05:25,523 Iter 978, Minibatch Loss= 0.0125, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 19:05:26,720 Iter 979, Minibatch Loss= 0.0227, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 19:05:26,721 Epoch 48, Average loss: 0.0116, learning rate: 0.0010\n",
      "2018-05-27 19:05:35,765 Iter 980, Minibatch Loss= 0.0120, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 19:05:38,104 Iter 980, Minibatch Loss= 0.0084, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:05:39,341 Iter 981, Minibatch Loss= 0.0116, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 19:05:40,542 Iter 982, Minibatch Loss= 0.0197, Training Accuracy= 0.9599, Minibatch error= 4.0%\n",
      "2018-05-27 19:05:41,791 Iter 983, Minibatch Loss= 0.0194, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:05:42,968 Iter 984, Minibatch Loss= 0.0198, Training Accuracy= 0.9637, Minibatch error= 3.6%\n",
      "2018-05-27 19:05:44,163 Iter 985, Minibatch Loss= 0.0095, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 19:05:45,350 Iter 986, Minibatch Loss= 0.0188, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 19:05:46,569 Iter 987, Minibatch Loss= 0.0196, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 19:05:47,774 Iter 988, Minibatch Loss= 0.0188, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:05:49,012 Iter 989, Minibatch Loss= 0.0198, Training Accuracy= 0.9578, Minibatch error= 4.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:05:50,195 Iter 990, Minibatch Loss= 0.0162, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 19:05:51,377 Iter 991, Minibatch Loss= 0.0183, Training Accuracy= 0.9525, Minibatch error= 4.8%\n",
      "2018-05-27 19:05:52,591 Iter 992, Minibatch Loss= 0.0150, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 19:05:53,835 Iter 993, Minibatch Loss= 0.0184, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:05:55,062 Iter 994, Minibatch Loss= 0.0196, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 19:05:56,276 Iter 995, Minibatch Loss= 0.0196, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 19:05:57,493 Iter 996, Minibatch Loss= 0.0079, Training Accuracy= 0.9895, Minibatch error= 1.0%\n",
      "2018-05-27 19:05:58,731 Iter 997, Minibatch Loss= 0.0204, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:05:59,953 Iter 998, Minibatch Loss= 0.0057, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 19:06:01,179 Iter 999, Minibatch Loss= 0.0074, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 19:06:01,180 Epoch 49, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 19:06:10,175 Iter 1000, Minibatch Loss= 0.0131, Training Accuracy= 0.9805, Minibatch error= 1.9%\n",
      "2018-05-27 19:06:12,502 Iter 1000, Minibatch Loss= 0.0111, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 19:06:13,677 Iter 1001, Minibatch Loss= 0.0178, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:06:14,854 Iter 1002, Minibatch Loss= 0.0094, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 19:06:16,037 Iter 1003, Minibatch Loss= 0.0075, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 19:06:17,236 Iter 1004, Minibatch Loss= 0.0170, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:06:18,415 Iter 1005, Minibatch Loss= 0.0148, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 19:06:19,609 Iter 1006, Minibatch Loss= 0.0188, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 19:06:20,764 Iter 1007, Minibatch Loss= 0.0091, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:06:21,955 Iter 1008, Minibatch Loss= 0.0194, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 19:06:23,186 Iter 1009, Minibatch Loss= 0.0123, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 19:06:24,394 Iter 1010, Minibatch Loss= 0.0094, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 19:06:25,567 Iter 1011, Minibatch Loss= 0.0094, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:06:26,729 Iter 1012, Minibatch Loss= 0.0072, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 19:06:27,899 Iter 1013, Minibatch Loss= 0.0094, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:06:29,102 Iter 1014, Minibatch Loss= 0.0077, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:06:30,304 Iter 1015, Minibatch Loss= 0.0203, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 19:06:31,562 Iter 1016, Minibatch Loss= 0.0198, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 19:06:32,731 Iter 1017, Minibatch Loss= 0.0145, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:06:33,912 Iter 1018, Minibatch Loss= 0.0125, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 19:06:35,131 Iter 1019, Minibatch Loss= 0.0108, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:06:35,132 Epoch 50, Average loss: 0.0131, learning rate: 0.0010\n",
      "2018-05-27 19:06:44,194 Iter 1020, Minibatch Loss= 0.0133, Training Accuracy= 0.9763, Minibatch error= 2.4%\n",
      "2018-05-27 19:06:46,574 Iter 1020, Minibatch Loss= 0.0177, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:06:47,787 Iter 1021, Minibatch Loss= 0.0187, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 19:06:48,991 Iter 1022, Minibatch Loss= 0.0140, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:06:50,236 Iter 1023, Minibatch Loss= 0.0228, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:06:51,480 Iter 1024, Minibatch Loss= 0.0170, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:06:52,713 Iter 1025, Minibatch Loss= 0.0057, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 19:06:53,924 Iter 1026, Minibatch Loss= 0.0099, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:06:55,145 Iter 1027, Minibatch Loss= 0.0208, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 19:06:56,383 Iter 1028, Minibatch Loss= 0.0103, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 19:06:57,621 Iter 1029, Minibatch Loss= 0.0168, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 19:06:58,835 Iter 1030, Minibatch Loss= 0.0190, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 19:07:00,076 Iter 1031, Minibatch Loss= 0.0225, Training Accuracy= 0.9615, Minibatch error= 3.8%\n",
      "2018-05-27 19:07:01,288 Iter 1032, Minibatch Loss= 0.0211, Training Accuracy= 0.9638, Minibatch error= 3.6%\n",
      "2018-05-27 19:07:02,494 Iter 1033, Minibatch Loss= 0.0192, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:07:03,732 Iter 1034, Minibatch Loss= 0.0203, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 19:07:04,983 Iter 1035, Minibatch Loss= 0.0186, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:07:06,209 Iter 1036, Minibatch Loss= 0.0184, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 19:07:07,452 Iter 1037, Minibatch Loss= 0.0091, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:07:08,689 Iter 1038, Minibatch Loss= 0.0149, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 19:07:09,929 Iter 1039, Minibatch Loss= 0.0118, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:07:09,930 Epoch 51, Average loss: 0.0163, learning rate: 0.0010\n",
      "2018-05-27 19:07:19,086 Iter 1040, Minibatch Loss= 0.0120, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 19:07:21,682 Iter 1040, Minibatch Loss= 0.0211, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 19:07:22,948 Iter 1041, Minibatch Loss= 0.0134, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 19:07:24,147 Iter 1042, Minibatch Loss= 0.0088, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 19:07:25,446 Iter 1043, Minibatch Loss= 0.0083, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:07:26,665 Iter 1044, Minibatch Loss= 0.0235, Training Accuracy= 0.9588, Minibatch error= 4.1%\n",
      "2018-05-27 19:07:27,916 Iter 1045, Minibatch Loss= 0.0013, Training Accuracy= 0.9979, Minibatch error= 0.2%\n",
      "2018-05-27 19:07:29,163 Iter 1046, Minibatch Loss= 0.0155, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:07:30,435 Iter 1047, Minibatch Loss= 0.0193, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 19:07:31,654 Iter 1048, Minibatch Loss= 0.0201, Training Accuracy= 0.9611, Minibatch error= 3.9%\n",
      "2018-05-27 19:07:32,888 Iter 1049, Minibatch Loss= 0.0115, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 19:07:34,124 Iter 1050, Minibatch Loss= 0.0148, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 19:07:35,407 Iter 1051, Minibatch Loss= 0.0201, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 19:07:36,660 Iter 1052, Minibatch Loss= 0.0209, Training Accuracy= 0.9619, Minibatch error= 3.8%\n",
      "2018-05-27 19:07:37,927 Iter 1053, Minibatch Loss= 0.0146, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 19:07:39,180 Iter 1054, Minibatch Loss= 0.0149, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 19:07:40,439 Iter 1055, Minibatch Loss= 0.0205, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 19:07:41,670 Iter 1056, Minibatch Loss= 0.0133, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 19:07:42,925 Iter 1057, Minibatch Loss= 0.0114, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:07:44,171 Iter 1058, Minibatch Loss= 0.0184, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 19:07:45,415 Iter 1059, Minibatch Loss= 0.0199, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:07:45,416 Epoch 52, Average loss: 0.0158, learning rate: 0.0010\n",
      "2018-05-27 19:07:54,420 Iter 1060, Minibatch Loss= 0.0124, Training Accuracy= 0.9808, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:07:56,807 Iter 1060, Minibatch Loss= 0.0182, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:07:58,071 Iter 1061, Minibatch Loss= 0.0106, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 19:07:59,317 Iter 1062, Minibatch Loss= 0.0201, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:08:00,551 Iter 1063, Minibatch Loss= 0.0208, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 19:08:01,763 Iter 1064, Minibatch Loss= 0.0106, Training Accuracy= 0.9830, Minibatch error= 1.7%\n",
      "2018-05-27 19:08:02,995 Iter 1065, Minibatch Loss= 0.0057, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:08:04,216 Iter 1066, Minibatch Loss= 0.0168, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 19:08:05,568 Iter 1067, Minibatch Loss= 0.0185, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:08:06,811 Iter 1068, Minibatch Loss= 0.0205, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 19:08:08,092 Iter 1069, Minibatch Loss= 0.0085, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 19:08:09,301 Iter 1070, Minibatch Loss= 0.0189, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 19:08:10,530 Iter 1071, Minibatch Loss= 0.0136, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 19:08:11,771 Iter 1072, Minibatch Loss= 0.0075, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:08:13,037 Iter 1073, Minibatch Loss= 0.0071, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 19:08:14,245 Iter 1074, Minibatch Loss= 0.0076, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 19:08:15,482 Iter 1075, Minibatch Loss= 0.0194, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:08:16,727 Iter 1076, Minibatch Loss= 0.0080, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:08:17,946 Iter 1077, Minibatch Loss= 0.0132, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 19:08:19,194 Iter 1078, Minibatch Loss= 0.0214, Training Accuracy= 0.9567, Minibatch error= 4.3%\n",
      "2018-05-27 19:08:20,471 Iter 1079, Minibatch Loss= 0.0138, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 19:08:20,472 Epoch 53, Average loss: 0.0143, learning rate: 0.0010\n",
      "2018-05-27 19:08:29,564 Iter 1080, Minibatch Loss= 0.0164, Training Accuracy= 0.9561, Minibatch error= 4.4%\n",
      "2018-05-27 19:08:31,937 Iter 1080, Minibatch Loss= 0.0037, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 19:08:33,152 Iter 1081, Minibatch Loss= 0.0225, Training Accuracy= 0.9413, Minibatch error= 5.9%\n",
      "2018-05-27 19:08:34,358 Iter 1082, Minibatch Loss= 0.0100, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:08:35,652 Iter 1083, Minibatch Loss= 0.0157, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:08:36,856 Iter 1084, Minibatch Loss= 0.0167, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 19:08:38,115 Iter 1085, Minibatch Loss= 0.0101, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:08:39,367 Iter 1086, Minibatch Loss= 0.0235, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 19:08:40,636 Iter 1087, Minibatch Loss= 0.0044, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 19:08:41,865 Iter 1088, Minibatch Loss= 0.0164, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:08:43,091 Iter 1089, Minibatch Loss= 0.0105, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 19:08:44,340 Iter 1090, Minibatch Loss= 0.0043, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 19:08:45,595 Iter 1091, Minibatch Loss= 0.0078, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:08:46,854 Iter 1092, Minibatch Loss= 0.0046, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:08:48,066 Iter 1093, Minibatch Loss= 0.0025, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 19:08:49,320 Iter 1094, Minibatch Loss= 0.0190, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:08:50,555 Iter 1095, Minibatch Loss= 0.0037, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 19:08:51,836 Iter 1096, Minibatch Loss= 0.0157, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:08:53,104 Iter 1097, Minibatch Loss= 0.0187, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 19:08:54,330 Iter 1098, Minibatch Loss= 0.0236, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 19:08:55,591 Iter 1099, Minibatch Loss= 0.0110, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:08:55,592 Epoch 54, Average loss: 0.0131, learning rate: 0.0010\n",
      "2018-05-27 19:09:04,822 Iter 1100, Minibatch Loss= 0.0130, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 19:09:07,312 Iter 1100, Minibatch Loss= 0.0054, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:09:08,577 Iter 1101, Minibatch Loss= 0.0114, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 19:09:09,821 Iter 1102, Minibatch Loss= 0.0177, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 19:09:11,105 Iter 1103, Minibatch Loss= 0.0098, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 19:09:12,367 Iter 1104, Minibatch Loss= 0.0180, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 19:09:13,631 Iter 1105, Minibatch Loss= 0.0206, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 19:09:14,886 Iter 1106, Minibatch Loss= 0.0059, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:09:16,139 Iter 1107, Minibatch Loss= 0.0083, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:09:17,406 Iter 1108, Minibatch Loss= 0.0057, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 19:09:18,672 Iter 1109, Minibatch Loss= 0.0094, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:09:19,975 Iter 1110, Minibatch Loss= 0.0049, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 19:09:21,230 Iter 1111, Minibatch Loss= 0.0039, Training Accuracy= 0.9938, Minibatch error= 0.6%\n",
      "2018-05-27 19:09:22,559 Iter 1112, Minibatch Loss= 0.0188, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:09:23,793 Iter 1113, Minibatch Loss= 0.0193, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:09:25,082 Iter 1114, Minibatch Loss= 0.0222, Training Accuracy= 0.9510, Minibatch error= 4.9%\n",
      "2018-05-27 19:09:26,374 Iter 1115, Minibatch Loss= 0.0213, Training Accuracy= 0.9412, Minibatch error= 5.9%\n",
      "2018-05-27 19:09:27,676 Iter 1116, Minibatch Loss= 0.0037, Training Accuracy= 0.9905, Minibatch error= 0.9%\n",
      "2018-05-27 19:09:28,954 Iter 1117, Minibatch Loss= 0.0268, Training Accuracy= 0.9307, Minibatch error= 6.9%\n",
      "2018-05-27 19:09:30,182 Iter 1118, Minibatch Loss= 0.0200, Training Accuracy= 0.9602, Minibatch error= 4.0%\n",
      "2018-05-27 19:09:31,442 Iter 1119, Minibatch Loss= 0.0107, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 19:09:31,443 Epoch 55, Average loss: 0.0132, learning rate: 0.0010\n",
      "2018-05-27 19:09:40,463 Iter 1120, Minibatch Loss= 0.0126, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:09:42,939 Iter 1120, Minibatch Loss= 0.0192, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:09:44,206 Iter 1121, Minibatch Loss= 0.0140, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 19:09:45,456 Iter 1122, Minibatch Loss= 0.0203, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 19:09:46,747 Iter 1123, Minibatch Loss= 0.0214, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:09:48,001 Iter 1124, Minibatch Loss= 0.0132, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 19:09:49,242 Iter 1125, Minibatch Loss= 0.0159, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 19:09:50,472 Iter 1126, Minibatch Loss= 0.0038, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 19:09:51,724 Iter 1127, Minibatch Loss= 0.0218, Training Accuracy= 0.9598, Minibatch error= 4.0%\n",
      "2018-05-27 19:09:52,971 Iter 1128, Minibatch Loss= 0.0118, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:09:54,212 Iter 1129, Minibatch Loss= 0.0154, Training Accuracy= 0.9709, Minibatch error= 2.9%\n",
      "2018-05-27 19:09:55,494 Iter 1130, Minibatch Loss= 0.0186, Training Accuracy= 0.9681, Minibatch error= 3.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:09:56,767 Iter 1131, Minibatch Loss= 0.0069, Training Accuracy= 0.9904, Minibatch error= 1.0%\n",
      "2018-05-27 19:09:58,015 Iter 1132, Minibatch Loss= 0.0076, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 19:09:59,259 Iter 1133, Minibatch Loss= 0.0052, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:10:00,503 Iter 1134, Minibatch Loss= 0.0008, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 19:10:01,750 Iter 1135, Minibatch Loss= 0.0084, Training Accuracy= 0.9889, Minibatch error= 1.1%\n",
      "2018-05-27 19:10:03,023 Iter 1136, Minibatch Loss= 0.0067, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 19:10:04,270 Iter 1137, Minibatch Loss= 0.0234, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 19:10:05,525 Iter 1138, Minibatch Loss= 0.0219, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 19:10:06,789 Iter 1139, Minibatch Loss= 0.0087, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:10:06,790 Epoch 56, Average loss: 0.0136, learning rate: 0.0010\n",
      "2018-05-27 19:10:15,917 Iter 1140, Minibatch Loss= 0.0128, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:10:18,428 Iter 1140, Minibatch Loss= 0.0154, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 19:10:19,706 Iter 1141, Minibatch Loss= 0.0213, Training Accuracy= 0.9627, Minibatch error= 3.7%\n",
      "2018-05-27 19:10:20,963 Iter 1142, Minibatch Loss= 0.0199, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 19:10:22,276 Iter 1143, Minibatch Loss= 0.0113, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 19:10:23,570 Iter 1144, Minibatch Loss= 0.0162, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:10:24,833 Iter 1145, Minibatch Loss= 0.0159, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:10:26,097 Iter 1146, Minibatch Loss= 0.0012, Training Accuracy= 0.9983, Minibatch error= 0.2%\n",
      "2018-05-27 19:10:27,421 Iter 1147, Minibatch Loss= 0.0230, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 19:10:28,681 Iter 1148, Minibatch Loss= 0.0083, Training Accuracy= 0.9854, Minibatch error= 1.5%\n",
      "2018-05-27 19:10:29,996 Iter 1149, Minibatch Loss= 0.0053, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:10:31,267 Iter 1150, Minibatch Loss= 0.0218, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:10:32,545 Iter 1151, Minibatch Loss= 0.0190, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 19:10:33,812 Iter 1152, Minibatch Loss= 0.0076, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:10:35,066 Iter 1153, Minibatch Loss= 0.0045, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 19:10:36,274 Iter 1154, Minibatch Loss= 0.0156, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 19:10:37,525 Iter 1155, Minibatch Loss= 0.0084, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:10:38,797 Iter 1156, Minibatch Loss= 0.0014, Training Accuracy= 0.9979, Minibatch error= 0.2%\n",
      "2018-05-27 19:10:40,096 Iter 1157, Minibatch Loss= 0.0173, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 19:10:41,339 Iter 1158, Minibatch Loss= 0.0144, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:10:42,569 Iter 1159, Minibatch Loss= 0.0132, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 19:10:42,571 Epoch 57, Average loss: 0.0132, learning rate: 0.0010\n",
      "2018-05-27 19:10:51,685 Iter 1160, Minibatch Loss= 0.0142, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:10:54,257 Iter 1160, Minibatch Loss= 0.0195, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 19:10:55,508 Iter 1161, Minibatch Loss= 0.0181, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 19:10:56,816 Iter 1162, Minibatch Loss= 0.0199, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 19:10:58,078 Iter 1163, Minibatch Loss= 0.0186, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:10:59,351 Iter 1164, Minibatch Loss= 0.0103, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:11:00,620 Iter 1165, Minibatch Loss= 0.0220, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 19:11:01,912 Iter 1166, Minibatch Loss= 0.0180, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:11:03,181 Iter 1167, Minibatch Loss= 0.0197, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 19:11:04,420 Iter 1168, Minibatch Loss= 0.0226, Training Accuracy= 0.9566, Minibatch error= 4.3%\n",
      "2018-05-27 19:11:05,675 Iter 1169, Minibatch Loss= 0.0125, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:11:06,900 Iter 1170, Minibatch Loss= 0.0225, Training Accuracy= 0.9695, Minibatch error= 3.1%\n",
      "2018-05-27 19:11:08,172 Iter 1171, Minibatch Loss= 0.0169, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:11:09,432 Iter 1172, Minibatch Loss= 0.0032, Training Accuracy= 0.9949, Minibatch error= 0.5%\n",
      "2018-05-27 19:11:10,668 Iter 1173, Minibatch Loss= 0.0079, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 19:11:11,957 Iter 1174, Minibatch Loss= 0.0069, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 19:11:13,180 Iter 1175, Minibatch Loss= 0.0070, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 19:11:14,429 Iter 1176, Minibatch Loss= 0.0204, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:11:15,705 Iter 1177, Minibatch Loss= 0.0169, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:11:16,938 Iter 1178, Minibatch Loss= 0.0176, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:11:18,149 Iter 1179, Minibatch Loss= 0.0200, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 19:11:18,150 Epoch 58, Average loss: 0.0169, learning rate: 0.0010\n",
      "2018-05-27 19:11:27,154 Iter 1180, Minibatch Loss= 0.0150, Training Accuracy= 0.9665, Minibatch error= 3.4%\n",
      "2018-05-27 19:11:29,690 Iter 1180, Minibatch Loss= 0.0310, Training Accuracy= 0.9414, Minibatch error= 5.9%\n",
      "2018-05-27 19:11:30,948 Iter 1181, Minibatch Loss= 0.0188, Training Accuracy= 0.9482, Minibatch error= 5.2%\n",
      "2018-05-27 19:11:32,178 Iter 1182, Minibatch Loss= 0.0279, Training Accuracy= 0.9252, Minibatch error= 7.5%\n",
      "2018-05-27 19:11:33,407 Iter 1183, Minibatch Loss= 0.0261, Training Accuracy= 0.9304, Minibatch error= 7.0%\n",
      "2018-05-27 19:11:34,632 Iter 1184, Minibatch Loss= 0.0182, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 19:11:35,839 Iter 1185, Minibatch Loss= 0.0202, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:11:37,101 Iter 1186, Minibatch Loss= 0.0125, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 19:11:38,362 Iter 1187, Minibatch Loss= 0.0222, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 19:11:39,569 Iter 1188, Minibatch Loss= 0.0201, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:11:40,819 Iter 1189, Minibatch Loss= 0.0168, Training Accuracy= 0.9749, Minibatch error= 2.5%\n",
      "2018-05-27 19:11:42,069 Iter 1190, Minibatch Loss= 0.0178, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 19:11:43,304 Iter 1191, Minibatch Loss= 0.0165, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 19:11:44,558 Iter 1192, Minibatch Loss= 0.0151, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 19:11:45,858 Iter 1193, Minibatch Loss= 0.0121, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 19:11:47,117 Iter 1194, Minibatch Loss= 0.0104, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:11:48,404 Iter 1195, Minibatch Loss= 0.0210, Training Accuracy= 0.9661, Minibatch error= 3.4%\n",
      "2018-05-27 19:11:49,681 Iter 1196, Minibatch Loss= 0.0072, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 19:11:50,943 Iter 1197, Minibatch Loss= 0.0231, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 19:11:52,209 Iter 1198, Minibatch Loss= 0.0112, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 19:11:53,458 Iter 1199, Minibatch Loss= 0.0076, Training Accuracy= 0.9894, Minibatch error= 1.1%\n",
      "2018-05-27 19:11:53,459 Epoch 59, Average loss: 0.0177, learning rate: 0.0010\n",
      "2018-05-27 19:12:02,533 Iter 1200, Minibatch Loss= 0.0133, Training Accuracy= 0.9808, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:12:05,096 Iter 1200, Minibatch Loss= 0.0190, Training Accuracy= 0.9731, Minibatch error= 2.7%\n",
      "2018-05-27 19:12:06,373 Iter 1201, Minibatch Loss= 0.0053, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:12:07,647 Iter 1202, Minibatch Loss= 0.0090, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:12:08,909 Iter 1203, Minibatch Loss= 0.0198, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 19:12:10,168 Iter 1204, Minibatch Loss= 0.0087, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 19:12:11,437 Iter 1205, Minibatch Loss= 0.0023, Training Accuracy= 0.9970, Minibatch error= 0.3%\n",
      "2018-05-27 19:12:12,693 Iter 1206, Minibatch Loss= 0.0117, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:12:13,969 Iter 1207, Minibatch Loss= 0.0207, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:12:15,237 Iter 1208, Minibatch Loss= 0.0163, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 19:12:16,488 Iter 1209, Minibatch Loss= 0.0205, Training Accuracy= 0.9643, Minibatch error= 3.6%\n",
      "2018-05-27 19:12:17,776 Iter 1210, Minibatch Loss= 0.0170, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 19:12:19,075 Iter 1211, Minibatch Loss= 0.0182, Training Accuracy= 0.9659, Minibatch error= 3.4%\n",
      "2018-05-27 19:12:20,338 Iter 1212, Minibatch Loss= 0.0201, Training Accuracy= 0.9623, Minibatch error= 3.8%\n",
      "2018-05-27 19:12:21,611 Iter 1213, Minibatch Loss= 0.0072, Training Accuracy= 0.9873, Minibatch error= 1.3%\n",
      "2018-05-27 19:12:22,921 Iter 1214, Minibatch Loss= 0.0025, Training Accuracy= 0.9965, Minibatch error= 0.4%\n",
      "2018-05-27 19:12:24,176 Iter 1215, Minibatch Loss= 0.0049, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 19:12:25,465 Iter 1216, Minibatch Loss= 0.0171, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:12:26,719 Iter 1217, Minibatch Loss= 0.0135, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 19:12:28,022 Iter 1218, Minibatch Loss= 0.0183, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 19:12:29,335 Iter 1219, Minibatch Loss= 0.0188, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:12:29,336 Epoch 60, Average loss: 0.0135, learning rate: 0.0010\n",
      "2018-05-27 19:12:38,532 Iter 1220, Minibatch Loss= 0.0129, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 19:12:41,252 Iter 1220, Minibatch Loss= 0.0186, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 19:12:42,551 Iter 1221, Minibatch Loss= 0.0201, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 19:12:43,830 Iter 1222, Minibatch Loss= 0.0119, Training Accuracy= 0.9813, Minibatch error= 1.9%\n",
      "2018-05-27 19:12:45,071 Iter 1223, Minibatch Loss= 0.0036, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 19:12:46,336 Iter 1224, Minibatch Loss= 0.0153, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 19:12:47,613 Iter 1225, Minibatch Loss= 0.0094, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 19:12:48,849 Iter 1226, Minibatch Loss= 0.0190, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 19:12:50,146 Iter 1227, Minibatch Loss= 0.0052, Training Accuracy= 0.9915, Minibatch error= 0.8%\n",
      "2018-05-27 19:12:51,438 Iter 1228, Minibatch Loss= 0.0058, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 19:12:52,699 Iter 1229, Minibatch Loss= 0.0077, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 19:12:53,978 Iter 1230, Minibatch Loss= 0.0219, Training Accuracy= 0.9604, Minibatch error= 4.0%\n",
      "2018-05-27 19:12:55,250 Iter 1231, Minibatch Loss= 0.0161, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:12:56,519 Iter 1232, Minibatch Loss= 0.0191, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 19:12:57,812 Iter 1233, Minibatch Loss= 0.0042, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:12:59,106 Iter 1234, Minibatch Loss= 0.0197, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 19:13:00,377 Iter 1235, Minibatch Loss= 0.0186, Training Accuracy= 0.9678, Minibatch error= 3.2%\n",
      "2018-05-27 19:13:01,614 Iter 1236, Minibatch Loss= 0.0052, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 19:13:02,869 Iter 1237, Minibatch Loss= 0.0226, Training Accuracy= 0.9606, Minibatch error= 3.9%\n",
      "2018-05-27 19:13:04,134 Iter 1238, Minibatch Loss= 0.0106, Training Accuracy= 0.9843, Minibatch error= 1.6%\n",
      "2018-05-27 19:13:05,425 Iter 1239, Minibatch Loss= 0.0212, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:13:05,426 Epoch 61, Average loss: 0.0139, learning rate: 0.0010\n",
      "2018-05-27 19:13:14,509 Iter 1240, Minibatch Loss= 0.0125, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 19:13:17,098 Iter 1240, Minibatch Loss= 0.0097, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 19:13:18,421 Iter 1241, Minibatch Loss= 0.0215, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:13:19,727 Iter 1242, Minibatch Loss= 0.0170, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:13:21,011 Iter 1243, Minibatch Loss= 0.0062, Training Accuracy= 0.9896, Minibatch error= 1.0%\n",
      "2018-05-27 19:13:22,287 Iter 1244, Minibatch Loss= 0.0206, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:13:23,579 Iter 1245, Minibatch Loss= 0.0204, Training Accuracy= 0.9633, Minibatch error= 3.7%\n",
      "2018-05-27 19:13:24,842 Iter 1246, Minibatch Loss= 0.0099, Training Accuracy= 0.9841, Minibatch error= 1.6%\n",
      "2018-05-27 19:13:26,145 Iter 1247, Minibatch Loss= 0.0049, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:13:27,421 Iter 1248, Minibatch Loss= 0.0184, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 19:13:28,690 Iter 1249, Minibatch Loss= 0.0042, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:13:30,043 Iter 1250, Minibatch Loss= 0.0155, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:13:31,303 Iter 1251, Minibatch Loss= 0.0111, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 19:13:32,589 Iter 1252, Minibatch Loss= 0.0084, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 19:13:33,875 Iter 1253, Minibatch Loss= 0.0056, Training Accuracy= 0.9902, Minibatch error= 1.0%\n",
      "2018-05-27 19:13:35,190 Iter 1254, Minibatch Loss= 0.0101, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 19:13:36,494 Iter 1255, Minibatch Loss= 0.0012, Training Accuracy= 0.9985, Minibatch error= 0.1%\n",
      "2018-05-27 19:13:37,766 Iter 1256, Minibatch Loss= 0.0186, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 19:13:39,068 Iter 1257, Minibatch Loss= 0.0081, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 19:13:40,343 Iter 1258, Minibatch Loss= 0.0195, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 19:13:41,643 Iter 1259, Minibatch Loss= 0.0060, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 19:13:41,644 Epoch 62, Average loss: 0.0118, learning rate: 0.0010\n",
      "2018-05-27 19:13:50,811 Iter 1260, Minibatch Loss= 0.0122, Training Accuracy= 0.9787, Minibatch error= 2.1%\n",
      "2018-05-27 19:13:53,482 Iter 1260, Minibatch Loss= 0.0021, Training Accuracy= 0.9967, Minibatch error= 0.3%\n",
      "2018-05-27 19:13:54,777 Iter 1261, Minibatch Loss= 0.0091, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 19:13:56,088 Iter 1262, Minibatch Loss= 0.0038, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:13:57,404 Iter 1263, Minibatch Loss= 0.0102, Training Accuracy= 0.9825, Minibatch error= 1.8%\n",
      "2018-05-27 19:13:58,708 Iter 1264, Minibatch Loss= 0.0206, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 19:14:00,036 Iter 1265, Minibatch Loss= 0.0109, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 19:14:01,347 Iter 1266, Minibatch Loss= 0.0020, Training Accuracy= 0.9971, Minibatch error= 0.3%\n",
      "2018-05-27 19:14:02,688 Iter 1267, Minibatch Loss= 0.0144, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 19:14:04,004 Iter 1268, Minibatch Loss= 0.0180, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:14:05,295 Iter 1269, Minibatch Loss= 0.0168, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:14:06,575 Iter 1270, Minibatch Loss= 0.0130, Training Accuracy= 0.9769, Minibatch error= 2.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:14:07,858 Iter 1271, Minibatch Loss= 0.0107, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 19:14:09,124 Iter 1272, Minibatch Loss= 0.0095, Training Accuracy= 0.9832, Minibatch error= 1.7%\n",
      "2018-05-27 19:14:10,431 Iter 1273, Minibatch Loss= 0.0119, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:14:11,733 Iter 1274, Minibatch Loss= 0.0183, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:14:13,067 Iter 1275, Minibatch Loss= 0.0061, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 19:14:14,373 Iter 1276, Minibatch Loss= 0.0177, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:14:15,706 Iter 1277, Minibatch Loss= 0.0193, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 19:14:17,000 Iter 1278, Minibatch Loss= 0.0176, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:14:18,345 Iter 1279, Minibatch Loss= 0.0195, Training Accuracy= 0.9648, Minibatch error= 3.5%\n",
      "2018-05-27 19:14:18,346 Epoch 63, Average loss: 0.0127, learning rate: 0.0010\n",
      "2018-05-27 19:14:27,501 Iter 1280, Minibatch Loss= 0.0123, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 19:14:30,216 Iter 1280, Minibatch Loss= 0.0068, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 19:14:31,530 Iter 1281, Minibatch Loss= 0.0153, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:14:32,876 Iter 1282, Minibatch Loss= 0.0196, Training Accuracy= 0.9686, Minibatch error= 3.1%\n",
      "2018-05-27 19:14:34,175 Iter 1283, Minibatch Loss= 0.0104, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 19:14:35,499 Iter 1284, Minibatch Loss= 0.0174, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:14:36,794 Iter 1285, Minibatch Loss= 0.0120, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 19:14:38,137 Iter 1286, Minibatch Loss= 0.0160, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:14:39,460 Iter 1287, Minibatch Loss= 0.0009, Training Accuracy= 0.9987, Minibatch error= 0.1%\n",
      "2018-05-27 19:14:40,775 Iter 1288, Minibatch Loss= 0.0163, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 19:14:42,083 Iter 1289, Minibatch Loss= 0.0147, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 19:14:43,364 Iter 1290, Minibatch Loss= 0.0080, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:14:44,640 Iter 1291, Minibatch Loss= 0.0083, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:14:45,920 Iter 1292, Minibatch Loss= 0.0084, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 19:14:47,206 Iter 1293, Minibatch Loss= 0.0104, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 19:14:48,504 Iter 1294, Minibatch Loss= 0.0152, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 19:14:49,784 Iter 1295, Minibatch Loss= 0.0184, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:14:51,064 Iter 1296, Minibatch Loss= 0.0200, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 19:14:52,363 Iter 1297, Minibatch Loss= 0.0090, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 19:14:53,663 Iter 1298, Minibatch Loss= 0.0221, Training Accuracy= 0.9480, Minibatch error= 5.2%\n",
      "2018-05-27 19:14:54,980 Iter 1299, Minibatch Loss= 0.0213, Training Accuracy= 0.9587, Minibatch error= 4.1%\n",
      "2018-05-27 19:14:54,981 Epoch 64, Average loss: 0.0135, learning rate: 0.0010\n",
      "2018-05-27 19:15:04,123 Iter 1300, Minibatch Loss= 0.0129, Training Accuracy= 0.9756, Minibatch error= 2.4%\n",
      "2018-05-27 19:15:06,796 Iter 1300, Minibatch Loss= 0.0200, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 19:15:08,055 Iter 1301, Minibatch Loss= 0.0079, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 19:15:09,332 Iter 1302, Minibatch Loss= 0.0160, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:15:10,603 Iter 1303, Minibatch Loss= 0.0037, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 19:15:11,883 Iter 1304, Minibatch Loss= 0.0296, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 19:15:13,156 Iter 1305, Minibatch Loss= 0.0167, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 19:15:14,443 Iter 1306, Minibatch Loss= 0.0112, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 19:15:15,706 Iter 1307, Minibatch Loss= 0.0207, Training Accuracy= 0.9586, Minibatch error= 4.1%\n",
      "2018-05-27 19:15:16,999 Iter 1308, Minibatch Loss= 0.0187, Training Accuracy= 0.9519, Minibatch error= 4.8%\n",
      "2018-05-27 19:15:18,264 Iter 1309, Minibatch Loss= 0.0025, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 19:15:19,601 Iter 1310, Minibatch Loss= 0.0132, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 19:15:20,892 Iter 1311, Minibatch Loss= 0.0237, Training Accuracy= 0.9473, Minibatch error= 5.3%\n",
      "2018-05-27 19:15:22,160 Iter 1312, Minibatch Loss= 0.0192, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:15:23,501 Iter 1313, Minibatch Loss= 0.0049, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:15:24,805 Iter 1314, Minibatch Loss= 0.0169, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 19:15:26,092 Iter 1315, Minibatch Loss= 0.0139, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 19:15:27,431 Iter 1316, Minibatch Loss= 0.0065, Training Accuracy= 0.9911, Minibatch error= 0.9%\n",
      "2018-05-27 19:15:28,758 Iter 1317, Minibatch Loss= 0.0067, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 19:15:30,139 Iter 1318, Minibatch Loss= 0.0197, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 19:15:31,454 Iter 1319, Minibatch Loss= 0.0121, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:15:31,455 Epoch 65, Average loss: 0.0151, learning rate: 0.0010\n",
      "2018-05-27 19:15:40,652 Iter 1320, Minibatch Loss= 0.0161, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 19:15:43,380 Iter 1320, Minibatch Loss= 0.0085, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 19:15:44,699 Iter 1321, Minibatch Loss= 0.0141, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 19:15:46,020 Iter 1322, Minibatch Loss= 0.0284, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 19:15:47,390 Iter 1323, Minibatch Loss= 0.0168, Training Accuracy= 0.9775, Minibatch error= 2.2%\n",
      "2018-05-27 19:15:48,692 Iter 1324, Minibatch Loss= 0.0214, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:15:50,056 Iter 1325, Minibatch Loss= 0.0215, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 19:15:51,368 Iter 1326, Minibatch Loss= 0.0220, Training Accuracy= 0.9625, Minibatch error= 3.7%\n",
      "2018-05-27 19:15:52,686 Iter 1327, Minibatch Loss= 0.0088, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 19:15:53,997 Iter 1328, Minibatch Loss= 0.0123, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 19:15:55,314 Iter 1329, Minibatch Loss= 0.0231, Training Accuracy= 0.9613, Minibatch error= 3.9%\n",
      "2018-05-27 19:15:56,618 Iter 1330, Minibatch Loss= 0.0137, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 19:15:57,915 Iter 1331, Minibatch Loss= 0.0098, Training Accuracy= 0.9842, Minibatch error= 1.6%\n",
      "2018-05-27 19:15:59,248 Iter 1332, Minibatch Loss= 0.0190, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:16:00,606 Iter 1333, Minibatch Loss= 0.0220, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 19:16:01,928 Iter 1334, Minibatch Loss= 0.0064, Training Accuracy= 0.9903, Minibatch error= 1.0%\n",
      "2018-05-27 19:16:03,243 Iter 1335, Minibatch Loss= 0.0096, Training Accuracy= 0.9866, Minibatch error= 1.3%\n",
      "2018-05-27 19:16:04,601 Iter 1336, Minibatch Loss= 0.0235, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 19:16:05,922 Iter 1337, Minibatch Loss= 0.0085, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:16:07,269 Iter 1338, Minibatch Loss= 0.0189, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:16:08,600 Iter 1339, Minibatch Loss= 0.0228, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 19:16:08,601 Epoch 66, Average loss: 0.0171, learning rate: 0.0010\n",
      "2018-05-27 19:16:17,900 Iter 1340, Minibatch Loss= 0.0125, Training Accuracy= 0.9809, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:16:20,693 Iter 1340, Minibatch Loss= 0.0185, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:16:22,015 Iter 1341, Minibatch Loss= 0.0202, Training Accuracy= 0.9662, Minibatch error= 3.4%\n",
      "2018-05-27 19:16:23,320 Iter 1342, Minibatch Loss= 0.0221, Training Accuracy= 0.9636, Minibatch error= 3.6%\n",
      "2018-05-27 19:16:24,621 Iter 1343, Minibatch Loss= 0.0148, Training Accuracy= 0.9755, Minibatch error= 2.4%\n",
      "2018-05-27 19:16:25,904 Iter 1344, Minibatch Loss= 0.0258, Training Accuracy= 0.9560, Minibatch error= 4.4%\n",
      "2018-05-27 19:16:27,186 Iter 1345, Minibatch Loss= 0.0016, Training Accuracy= 0.9974, Minibatch error= 0.3%\n",
      "2018-05-27 19:16:28,507 Iter 1346, Minibatch Loss= 0.0258, Training Accuracy= 0.9547, Minibatch error= 4.5%\n",
      "2018-05-27 19:16:29,874 Iter 1347, Minibatch Loss= 0.0179, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 19:16:31,202 Iter 1348, Minibatch Loss= 0.0183, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 19:16:32,549 Iter 1349, Minibatch Loss= 0.0051, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 19:16:33,857 Iter 1350, Minibatch Loss= 0.0082, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:16:35,229 Iter 1351, Minibatch Loss= 0.0221, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 19:16:36,557 Iter 1352, Minibatch Loss= 0.0206, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:16:37,874 Iter 1353, Minibatch Loss= 0.0053, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 19:16:39,176 Iter 1354, Minibatch Loss= 0.0049, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 19:16:40,521 Iter 1355, Minibatch Loss= 0.0222, Training Accuracy= 0.9723, Minibatch error= 2.8%\n",
      "2018-05-27 19:16:41,850 Iter 1356, Minibatch Loss= 0.0065, Training Accuracy= 0.9906, Minibatch error= 0.9%\n",
      "2018-05-27 19:16:43,141 Iter 1357, Minibatch Loss= 0.0110, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 19:16:44,507 Iter 1358, Minibatch Loss= 0.0170, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 19:16:45,820 Iter 1359, Minibatch Loss= 0.0195, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:16:45,821 Epoch 67, Average loss: 0.0155, learning rate: 0.0010\n",
      "2018-05-27 19:16:55,059 Iter 1360, Minibatch Loss= 0.0127, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 19:16:57,825 Iter 1360, Minibatch Loss= 0.0164, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 19:16:59,157 Iter 1361, Minibatch Loss= 0.0213, Training Accuracy= 0.9655, Minibatch error= 3.5%\n",
      "2018-05-27 19:17:00,451 Iter 1362, Minibatch Loss= 0.0060, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:17:01,765 Iter 1363, Minibatch Loss= 0.0224, Training Accuracy= 0.9625, Minibatch error= 3.8%\n",
      "2018-05-27 19:17:03,059 Iter 1364, Minibatch Loss= 0.0131, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:17:04,377 Iter 1365, Minibatch Loss= 0.0045, Training Accuracy= 0.9922, Minibatch error= 0.8%\n",
      "2018-05-27 19:17:05,698 Iter 1366, Minibatch Loss= 0.0036, Training Accuracy= 0.9947, Minibatch error= 0.5%\n",
      "2018-05-27 19:17:07,001 Iter 1367, Minibatch Loss= 0.0088, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 19:17:08,296 Iter 1368, Minibatch Loss= 0.0196, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:17:09,617 Iter 1369, Minibatch Loss= 0.0101, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 19:17:10,947 Iter 1370, Minibatch Loss= 0.0174, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:17:12,274 Iter 1371, Minibatch Loss= 0.0162, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 19:17:13,593 Iter 1372, Minibatch Loss= 0.0023, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 19:17:14,913 Iter 1373, Minibatch Loss= 0.0168, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 19:17:16,198 Iter 1374, Minibatch Loss= 0.0179, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:17:17,502 Iter 1375, Minibatch Loss= 0.0218, Training Accuracy= 0.9610, Minibatch error= 3.9%\n",
      "2018-05-27 19:17:18,781 Iter 1376, Minibatch Loss= 0.0181, Training Accuracy= 0.9668, Minibatch error= 3.3%\n",
      "2018-05-27 19:17:20,140 Iter 1377, Minibatch Loss= 0.0177, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 19:17:21,458 Iter 1378, Minibatch Loss= 0.0199, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 19:17:22,815 Iter 1379, Minibatch Loss= 0.0163, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 19:17:22,816 Epoch 68, Average loss: 0.0147, learning rate: 0.0010\n",
      "2018-05-27 19:17:32,005 Iter 1380, Minibatch Loss= 0.0125, Training Accuracy= 0.9796, Minibatch error= 2.0%\n",
      "2018-05-27 19:17:34,795 Iter 1380, Minibatch Loss= 0.0180, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 19:17:36,097 Iter 1381, Minibatch Loss= 0.0196, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:17:37,385 Iter 1382, Minibatch Loss= 0.0093, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 19:17:38,710 Iter 1383, Minibatch Loss= 0.0152, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 19:17:40,065 Iter 1384, Minibatch Loss= 0.0077, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 19:17:41,379 Iter 1385, Minibatch Loss= 0.0050, Training Accuracy= 0.9918, Minibatch error= 0.8%\n",
      "2018-05-27 19:17:42,717 Iter 1386, Minibatch Loss= 0.0027, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 19:17:44,078 Iter 1387, Minibatch Loss= 0.0131, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:17:45,405 Iter 1388, Minibatch Loss= 0.0183, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:17:46,721 Iter 1389, Minibatch Loss= 0.0106, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:17:48,042 Iter 1390, Minibatch Loss= 0.0148, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 19:17:49,374 Iter 1391, Minibatch Loss= 0.0182, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:17:50,710 Iter 1392, Minibatch Loss= 0.0195, Training Accuracy= 0.9664, Minibatch error= 3.4%\n",
      "2018-05-27 19:17:52,057 Iter 1393, Minibatch Loss= 0.0036, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 19:17:53,393 Iter 1394, Minibatch Loss= 0.0039, Training Accuracy= 0.9942, Minibatch error= 0.6%\n",
      "2018-05-27 19:17:54,748 Iter 1395, Minibatch Loss= 0.0134, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 19:17:56,091 Iter 1396, Minibatch Loss= 0.0117, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 19:17:57,476 Iter 1397, Minibatch Loss= 0.0141, Training Accuracy= 0.9751, Minibatch error= 2.5%\n",
      "2018-05-27 19:17:58,801 Iter 1398, Minibatch Loss= 0.0069, Training Accuracy= 0.9877, Minibatch error= 1.2%\n",
      "2018-05-27 19:18:00,238 Iter 1399, Minibatch Loss= 0.0073, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:18:00,239 Epoch 69, Average loss: 0.0114, learning rate: 0.0010\n",
      "2018-05-27 19:18:09,491 Iter 1400, Minibatch Loss= 0.0118, Training Accuracy= 0.9820, Minibatch error= 1.8%\n",
      "2018-05-27 19:18:12,429 Iter 1400, Minibatch Loss= 0.0120, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:18:13,779 Iter 1401, Minibatch Loss= 0.0153, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:18:15,083 Iter 1402, Minibatch Loss= 0.0153, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 19:18:16,376 Iter 1403, Minibatch Loss= 0.0194, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 19:18:17,716 Iter 1404, Minibatch Loss= 0.0203, Training Accuracy= 0.9609, Minibatch error= 3.9%\n",
      "2018-05-27 19:18:19,059 Iter 1405, Minibatch Loss= 0.0207, Training Accuracy= 0.9644, Minibatch error= 3.6%\n",
      "2018-05-27 19:18:20,363 Iter 1406, Minibatch Loss= 0.0159, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:18:21,684 Iter 1407, Minibatch Loss= 0.0143, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:18:22,981 Iter 1408, Minibatch Loss= 0.0039, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:18:24,284 Iter 1409, Minibatch Loss= 0.0158, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 19:18:25,605 Iter 1410, Minibatch Loss= 0.0159, Training Accuracy= 0.9725, Minibatch error= 2.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:18:26,929 Iter 1411, Minibatch Loss= 0.0225, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:18:28,234 Iter 1412, Minibatch Loss= 0.0012, Training Accuracy= 0.9984, Minibatch error= 0.2%\n",
      "2018-05-27 19:18:29,576 Iter 1413, Minibatch Loss= 0.0199, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 19:18:30,868 Iter 1414, Minibatch Loss= 0.0069, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 19:18:32,180 Iter 1415, Minibatch Loss= 0.0068, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 19:18:33,515 Iter 1416, Minibatch Loss= 0.0087, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 19:18:34,831 Iter 1417, Minibatch Loss= 0.0176, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 19:18:36,160 Iter 1418, Minibatch Loss= 0.0179, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:18:37,456 Iter 1419, Minibatch Loss= 0.0093, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:18:37,457 Epoch 70, Average loss: 0.0141, learning rate: 0.0010\n",
      "2018-05-27 19:18:46,677 Iter 1420, Minibatch Loss= 0.0136, Training Accuracy= 0.9754, Minibatch error= 2.5%\n",
      "2018-05-27 19:18:49,543 Iter 1420, Minibatch Loss= 0.0066, Training Accuracy= 0.9845, Minibatch error= 1.5%\n",
      "2018-05-27 19:18:50,871 Iter 1421, Minibatch Loss= 0.0155, Training Accuracy= 0.9594, Minibatch error= 4.1%\n",
      "2018-05-27 19:18:52,258 Iter 1422, Minibatch Loss= 0.0240, Training Accuracy= 0.9481, Minibatch error= 5.2%\n",
      "2018-05-27 19:18:53,605 Iter 1423, Minibatch Loss= 0.0152, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:18:54,961 Iter 1424, Minibatch Loss= 0.0137, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:18:56,309 Iter 1425, Minibatch Loss= 0.0112, Training Accuracy= 0.9815, Minibatch error= 1.8%\n",
      "2018-05-27 19:18:57,660 Iter 1426, Minibatch Loss= 0.0263, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 19:18:59,054 Iter 1427, Minibatch Loss= 0.0046, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 19:19:00,399 Iter 1428, Minibatch Loss= 0.0089, Training Accuracy= 0.9869, Minibatch error= 1.3%\n",
      "2018-05-27 19:19:01,725 Iter 1429, Minibatch Loss= 0.0210, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:19:03,069 Iter 1430, Minibatch Loss= 0.0179, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:19:04,401 Iter 1431, Minibatch Loss= 0.0058, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 19:19:05,759 Iter 1432, Minibatch Loss= 0.0165, Training Accuracy= 0.9675, Minibatch error= 3.3%\n",
      "2018-05-27 19:19:07,103 Iter 1433, Minibatch Loss= 0.0237, Training Accuracy= 0.9452, Minibatch error= 5.5%\n",
      "2018-05-27 19:19:08,468 Iter 1434, Minibatch Loss= 0.0221, Training Accuracy= 0.9581, Minibatch error= 4.2%\n",
      "2018-05-27 19:19:09,821 Iter 1435, Minibatch Loss= 0.0169, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 19:19:11,205 Iter 1436, Minibatch Loss= 0.0171, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 19:19:12,576 Iter 1437, Minibatch Loss= 0.0025, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 19:19:13,915 Iter 1438, Minibatch Loss= 0.0137, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 19:19:15,271 Iter 1439, Minibatch Loss= 0.0124, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 19:19:15,272 Epoch 71, Average loss: 0.0150, learning rate: 0.0010\n",
      "2018-05-27 19:19:24,567 Iter 1440, Minibatch Loss= 0.0125, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 19:19:27,409 Iter 1440, Minibatch Loss= 0.0171, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:19:28,763 Iter 1441, Minibatch Loss= 0.0237, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 19:19:30,164 Iter 1442, Minibatch Loss= 0.0196, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:19:31,516 Iter 1443, Minibatch Loss= 0.0171, Training Accuracy= 0.9710, Minibatch error= 2.9%\n",
      "2018-05-27 19:19:32,858 Iter 1444, Minibatch Loss= 0.0170, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 19:19:34,171 Iter 1445, Minibatch Loss= 0.0111, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:19:35,507 Iter 1446, Minibatch Loss= 0.0248, Training Accuracy= 0.9434, Minibatch error= 5.7%\n",
      "2018-05-27 19:19:36,855 Iter 1447, Minibatch Loss= 0.0020, Training Accuracy= 0.9950, Minibatch error= 0.5%\n",
      "2018-05-27 19:19:38,182 Iter 1448, Minibatch Loss= 0.0247, Training Accuracy= 0.9506, Minibatch error= 4.9%\n",
      "2018-05-27 19:19:39,542 Iter 1449, Minibatch Loss= 0.0204, Training Accuracy= 0.9635, Minibatch error= 3.7%\n",
      "2018-05-27 19:19:40,878 Iter 1450, Minibatch Loss= 0.0167, Training Accuracy= 0.9727, Minibatch error= 2.7%\n",
      "2018-05-27 19:19:42,173 Iter 1451, Minibatch Loss= 0.0135, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 19:19:43,534 Iter 1452, Minibatch Loss= 0.0095, Training Accuracy= 0.9855, Minibatch error= 1.5%\n",
      "2018-05-27 19:19:44,871 Iter 1453, Minibatch Loss= 0.0146, Training Accuracy= 0.9785, Minibatch error= 2.2%\n",
      "2018-05-27 19:19:46,233 Iter 1454, Minibatch Loss= 0.0097, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 19:19:47,568 Iter 1455, Minibatch Loss= 0.0215, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:19:48,957 Iter 1456, Minibatch Loss= 0.0154, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 19:19:50,296 Iter 1457, Minibatch Loss= 0.0195, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 19:19:51,644 Iter 1458, Minibatch Loss= 0.0197, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 19:19:52,986 Iter 1459, Minibatch Loss= 0.0049, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:19:52,987 Epoch 72, Average loss: 0.0166, learning rate: 0.0010\n",
      "2018-05-27 19:20:02,287 Iter 1460, Minibatch Loss= 0.0120, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:20:05,157 Iter 1460, Minibatch Loss= 0.0145, Training Accuracy= 0.9765, Minibatch error= 2.3%\n",
      "2018-05-27 19:20:06,503 Iter 1461, Minibatch Loss= 0.0219, Training Accuracy= 0.9617, Minibatch error= 3.8%\n",
      "2018-05-27 19:20:07,848 Iter 1462, Minibatch Loss= 0.0180, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:20:09,196 Iter 1463, Minibatch Loss= 0.0057, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 19:20:10,548 Iter 1464, Minibatch Loss= 0.0219, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 19:20:11,888 Iter 1465, Minibatch Loss= 0.0112, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 19:20:13,216 Iter 1466, Minibatch Loss= 0.0015, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 19:20:14,533 Iter 1467, Minibatch Loss= 0.0209, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 19:20:15,901 Iter 1468, Minibatch Loss= 0.0185, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 19:20:17,247 Iter 1469, Minibatch Loss= 0.0208, Training Accuracy= 0.9651, Minibatch error= 3.5%\n",
      "2018-05-27 19:20:18,602 Iter 1470, Minibatch Loss= 0.0086, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 19:20:19,925 Iter 1471, Minibatch Loss= 0.0033, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 19:20:21,289 Iter 1472, Minibatch Loss= 0.0198, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:20:22,638 Iter 1473, Minibatch Loss= 0.0093, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:20:24,012 Iter 1474, Minibatch Loss= 0.0029, Training Accuracy= 0.9956, Minibatch error= 0.4%\n",
      "2018-05-27 19:20:25,346 Iter 1475, Minibatch Loss= 0.0068, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 19:20:26,669 Iter 1476, Minibatch Loss= 0.0055, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 19:20:28,002 Iter 1477, Minibatch Loss= 0.0132, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:20:29,360 Iter 1478, Minibatch Loss= 0.0080, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 19:20:30,733 Iter 1479, Minibatch Loss= 0.0166, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 19:20:30,735 Epoch 73, Average loss: 0.0122, learning rate: 0.0010\n",
      "2018-05-27 19:20:39,945 Iter 1480, Minibatch Loss= 0.0114, Training Accuracy= 0.9815, Minibatch error= 1.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:20:42,803 Iter 1480, Minibatch Loss= 0.0042, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 19:20:44,154 Iter 1481, Minibatch Loss= 0.0041, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 19:20:45,531 Iter 1482, Minibatch Loss= 0.0028, Training Accuracy= 0.9955, Minibatch error= 0.4%\n",
      "2018-05-27 19:20:46,897 Iter 1483, Minibatch Loss= 0.0217, Training Accuracy= 0.9624, Minibatch error= 3.8%\n",
      "2018-05-27 19:20:48,269 Iter 1484, Minibatch Loss= 0.0195, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 19:20:49,604 Iter 1485, Minibatch Loss= 0.0077, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 19:20:50,954 Iter 1486, Minibatch Loss= 0.0171, Training Accuracy= 0.9676, Minibatch error= 3.2%\n",
      "2018-05-27 19:20:52,323 Iter 1487, Minibatch Loss= 0.0165, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:20:53,706 Iter 1488, Minibatch Loss= 0.0133, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 19:20:55,052 Iter 1489, Minibatch Loss= 0.0190, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 19:20:56,406 Iter 1490, Minibatch Loss= 0.0072, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 19:20:57,773 Iter 1491, Minibatch Loss= 0.0147, Training Accuracy= 0.9768, Minibatch error= 2.3%\n",
      "2018-05-27 19:20:59,138 Iter 1492, Minibatch Loss= 0.0094, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:21:00,508 Iter 1493, Minibatch Loss= 0.0169, Training Accuracy= 0.9725, Minibatch error= 2.7%\n",
      "2018-05-27 19:21:01,846 Iter 1494, Minibatch Loss= 0.0199, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:21:03,177 Iter 1495, Minibatch Loss= 0.0215, Training Accuracy= 0.9639, Minibatch error= 3.6%\n",
      "2018-05-27 19:21:04,522 Iter 1496, Minibatch Loss= 0.0177, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:21:05,873 Iter 1497, Minibatch Loss= 0.0150, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 19:21:07,246 Iter 1498, Minibatch Loss= 0.0207, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 19:21:08,578 Iter 1499, Minibatch Loss= 0.0105, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:21:08,579 Epoch 74, Average loss: 0.0137, learning rate: 0.0010\n",
      "2018-05-27 19:21:17,820 Iter 1500, Minibatch Loss= 0.0122, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 19:21:20,612 Iter 1500, Minibatch Loss= 0.0100, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:21:21,951 Iter 1501, Minibatch Loss= 0.0186, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 19:21:23,285 Iter 1502, Minibatch Loss= 0.0026, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 19:21:24,624 Iter 1503, Minibatch Loss= 0.0131, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 19:21:25,989 Iter 1504, Minibatch Loss= 0.0184, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:21:27,313 Iter 1505, Minibatch Loss= 0.0175, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:21:28,670 Iter 1506, Minibatch Loss= 0.0050, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 19:21:30,050 Iter 1507, Minibatch Loss= 0.0172, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:21:31,380 Iter 1508, Minibatch Loss= 0.0207, Training Accuracy= 0.9630, Minibatch error= 3.7%\n",
      "2018-05-27 19:21:32,747 Iter 1509, Minibatch Loss= 0.0110, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 19:21:34,073 Iter 1510, Minibatch Loss= 0.0113, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:21:35,460 Iter 1511, Minibatch Loss= 0.0028, Training Accuracy= 0.9954, Minibatch error= 0.5%\n",
      "2018-05-27 19:21:36,808 Iter 1512, Minibatch Loss= 0.0169, Training Accuracy= 0.9695, Minibatch error= 3.0%\n",
      "2018-05-27 19:21:38,151 Iter 1513, Minibatch Loss= 0.0148, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:21:39,506 Iter 1514, Minibatch Loss= 0.0082, Training Accuracy= 0.9898, Minibatch error= 1.0%\n",
      "2018-05-27 19:21:40,847 Iter 1515, Minibatch Loss= 0.0186, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 19:21:42,179 Iter 1516, Minibatch Loss= 0.0194, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 19:21:43,537 Iter 1517, Minibatch Loss= 0.0077, Training Accuracy= 0.9879, Minibatch error= 1.2%\n",
      "2018-05-27 19:21:44,885 Iter 1518, Minibatch Loss= 0.0090, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 19:21:46,245 Iter 1519, Minibatch Loss= 0.0172, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 19:21:46,246 Epoch 75, Average loss: 0.0129, learning rate: 0.0010\n",
      "2018-05-27 19:21:55,597 Iter 1520, Minibatch Loss= 0.0116, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 19:21:58,590 Iter 1520, Minibatch Loss= 0.0151, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 19:21:59,981 Iter 1521, Minibatch Loss= 0.0039, Training Accuracy= 0.9939, Minibatch error= 0.6%\n",
      "2018-05-27 19:22:01,338 Iter 1522, Minibatch Loss= 0.0187, Training Accuracy= 0.9660, Minibatch error= 3.4%\n",
      "2018-05-27 19:22:02,723 Iter 1523, Minibatch Loss= 0.0193, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 19:22:04,072 Iter 1524, Minibatch Loss= 0.0179, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:05,474 Iter 1525, Minibatch Loss= 0.0166, Training Accuracy= 0.9685, Minibatch error= 3.1%\n",
      "2018-05-27 19:22:06,814 Iter 1526, Minibatch Loss= 0.0126, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:22:08,185 Iter 1527, Minibatch Loss= 0.0162, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:22:09,562 Iter 1528, Minibatch Loss= 0.0166, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:10,928 Iter 1529, Minibatch Loss= 0.0182, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:12,300 Iter 1530, Minibatch Loss= 0.0182, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 19:22:13,648 Iter 1531, Minibatch Loss= 0.0090, Training Accuracy= 0.9861, Minibatch error= 1.4%\n",
      "2018-05-27 19:22:14,978 Iter 1532, Minibatch Loss= 0.0166, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:16,288 Iter 1533, Minibatch Loss= 0.0121, Training Accuracy= 0.9809, Minibatch error= 1.9%\n",
      "2018-05-27 19:22:17,682 Iter 1534, Minibatch Loss= 0.0194, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 19:22:19,047 Iter 1535, Minibatch Loss= 0.0174, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:22:20,391 Iter 1536, Minibatch Loss= 0.0123, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 19:22:21,743 Iter 1537, Minibatch Loss= 0.0176, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:22:23,150 Iter 1538, Minibatch Loss= 0.0183, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:22:24,463 Iter 1539, Minibatch Loss= 0.0082, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:22:24,464 Epoch 76, Average loss: 0.0148, learning rate: 0.0010\n",
      "2018-05-27 19:22:33,482 Iter 1540, Minibatch Loss= 0.0116, Training Accuracy= 0.9817, Minibatch error= 1.8%\n",
      "2018-05-27 19:22:36,500 Iter 1540, Minibatch Loss= 0.0181, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:37,936 Iter 1541, Minibatch Loss= 0.0090, Training Accuracy= 0.9862, Minibatch error= 1.4%\n",
      "2018-05-27 19:22:39,378 Iter 1542, Minibatch Loss= 0.0183, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:22:40,774 Iter 1543, Minibatch Loss= 0.0146, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 19:22:42,150 Iter 1544, Minibatch Loss= 0.0175, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:22:43,531 Iter 1545, Minibatch Loss= 0.0166, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:22:44,908 Iter 1546, Minibatch Loss= 0.0071, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 19:22:46,286 Iter 1547, Minibatch Loss= 0.0198, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:22:47,684 Iter 1548, Minibatch Loss= 0.0135, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 19:22:49,097 Iter 1549, Minibatch Loss= 0.0149, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 19:22:50,476 Iter 1550, Minibatch Loss= 0.0156, Training Accuracy= 0.9722, Minibatch error= 2.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:22:51,803 Iter 1551, Minibatch Loss= 0.0178, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 19:22:53,185 Iter 1552, Minibatch Loss= 0.0067, Training Accuracy= 0.9885, Minibatch error= 1.2%\n",
      "2018-05-27 19:22:54,532 Iter 1553, Minibatch Loss= 0.0068, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 19:22:55,882 Iter 1554, Minibatch Loss= 0.0039, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:22:57,284 Iter 1555, Minibatch Loss= 0.0151, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 19:22:58,664 Iter 1556, Minibatch Loss= 0.0213, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 19:23:00,061 Iter 1557, Minibatch Loss= 0.0174, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:23:01,418 Iter 1558, Minibatch Loss= 0.0139, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 19:23:02,773 Iter 1559, Minibatch Loss= 0.0195, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 19:23:02,774 Epoch 77, Average loss: 0.0142, learning rate: 0.0010\n",
      "2018-05-27 19:23:12,066 Iter 1560, Minibatch Loss= 0.0122, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 19:23:15,074 Iter 1560, Minibatch Loss= 0.0174, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:23:16,427 Iter 1561, Minibatch Loss= 0.0136, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 19:23:17,815 Iter 1562, Minibatch Loss= 0.0194, Training Accuracy= 0.9677, Minibatch error= 3.2%\n",
      "2018-05-27 19:23:19,213 Iter 1563, Minibatch Loss= 0.0196, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:23:20,584 Iter 1564, Minibatch Loss= 0.0104, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 19:23:21,955 Iter 1565, Minibatch Loss= 0.0041, Training Accuracy= 0.9938, Minibatch error= 0.6%\n",
      "2018-05-27 19:23:23,331 Iter 1566, Minibatch Loss= 0.0090, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 19:23:24,723 Iter 1567, Minibatch Loss= 0.0039, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:23:26,106 Iter 1568, Minibatch Loss= 0.0166, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:23:27,484 Iter 1569, Minibatch Loss= 0.0073, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 19:23:28,856 Iter 1570, Minibatch Loss= 0.0073, Training Accuracy= 0.9871, Minibatch error= 1.3%\n",
      "2018-05-27 19:23:30,253 Iter 1571, Minibatch Loss= 0.0049, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:23:31,611 Iter 1572, Minibatch Loss= 0.0058, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:23:33,020 Iter 1573, Minibatch Loss= 0.0036, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 19:23:34,392 Iter 1574, Minibatch Loss= 0.0047, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 19:23:35,798 Iter 1575, Minibatch Loss= 0.0090, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 19:23:37,130 Iter 1576, Minibatch Loss= 0.0086, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:23:38,518 Iter 1577, Minibatch Loss= 0.0104, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 19:23:39,886 Iter 1578, Minibatch Loss= 0.0100, Training Accuracy= 0.9835, Minibatch error= 1.7%\n",
      "2018-05-27 19:23:41,257 Iter 1579, Minibatch Loss= 0.0158, Training Accuracy= 0.9781, Minibatch error= 2.2%\n",
      "2018-05-27 19:23:41,258 Epoch 78, Average loss: 0.0102, learning rate: 0.0010\n",
      "2018-05-27 19:23:50,561 Iter 1580, Minibatch Loss= 0.0110, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 19:23:53,644 Iter 1580, Minibatch Loss= 0.0163, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 19:23:55,019 Iter 1581, Minibatch Loss= 0.0209, Training Accuracy= 0.9626, Minibatch error= 3.7%\n",
      "2018-05-27 19:23:56,427 Iter 1582, Minibatch Loss= 0.0167, Training Accuracy= 0.9699, Minibatch error= 3.0%\n",
      "2018-05-27 19:23:57,837 Iter 1583, Minibatch Loss= 0.0019, Training Accuracy= 0.9972, Minibatch error= 0.3%\n",
      "2018-05-27 19:23:59,244 Iter 1584, Minibatch Loss= 0.0044, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:24:00,662 Iter 1585, Minibatch Loss= 0.0180, Training Accuracy= 0.9685, Minibatch error= 3.1%\n",
      "2018-05-27 19:24:02,061 Iter 1586, Minibatch Loss= 0.0025, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 19:24:03,456 Iter 1587, Minibatch Loss= 0.0041, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 19:24:04,872 Iter 1588, Minibatch Loss= 0.0196, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:24:06,268 Iter 1589, Minibatch Loss= 0.0142, Training Accuracy= 0.9777, Minibatch error= 2.2%\n",
      "2018-05-27 19:24:07,671 Iter 1590, Minibatch Loss= 0.0023, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 19:24:09,068 Iter 1591, Minibatch Loss= 0.0149, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 19:24:10,485 Iter 1592, Minibatch Loss= 0.0097, Training Accuracy= 0.9849, Minibatch error= 1.5%\n",
      "2018-05-27 19:24:11,910 Iter 1593, Minibatch Loss= 0.0108, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 19:24:13,350 Iter 1594, Minibatch Loss= 0.0119, Training Accuracy= 0.9815, Minibatch error= 1.9%\n",
      "2018-05-27 19:24:14,754 Iter 1595, Minibatch Loss= 0.0117, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 19:24:16,145 Iter 1596, Minibatch Loss= 0.0129, Training Accuracy= 0.9795, Minibatch error= 2.0%\n",
      "2018-05-27 19:24:17,550 Iter 1597, Minibatch Loss= 0.0163, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 19:24:18,951 Iter 1598, Minibatch Loss= 0.0163, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:24:20,340 Iter 1599, Minibatch Loss= 0.0116, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 19:24:20,341 Epoch 79, Average loss: 0.0114, learning rate: 0.0010\n",
      "2018-05-27 19:24:29,664 Iter 1600, Minibatch Loss= 0.0114, Training Accuracy= 0.9799, Minibatch error= 2.0%\n",
      "2018-05-27 19:24:32,970 Iter 1600, Minibatch Loss= 0.0130, Training Accuracy= 0.9772, Minibatch error= 2.3%\n",
      "2018-05-27 19:24:34,373 Iter 1601, Minibatch Loss= 0.0189, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 19:24:35,802 Iter 1602, Minibatch Loss= 0.0121, Training Accuracy= 0.9780, Minibatch error= 2.2%\n",
      "2018-05-27 19:24:37,208 Iter 1603, Minibatch Loss= 0.0013, Training Accuracy= 0.9980, Minibatch error= 0.2%\n",
      "2018-05-27 19:24:38,635 Iter 1604, Minibatch Loss= 0.0157, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 19:24:40,038 Iter 1605, Minibatch Loss= 0.0082, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:24:41,442 Iter 1606, Minibatch Loss= 0.0043, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 19:24:42,863 Iter 1607, Minibatch Loss= 0.0110, Training Accuracy= 0.9798, Minibatch error= 2.0%\n",
      "2018-05-27 19:24:44,263 Iter 1608, Minibatch Loss= 0.0173, Training Accuracy= 0.9738, Minibatch error= 2.6%\n",
      "2018-05-27 19:24:45,649 Iter 1609, Minibatch Loss= 0.0056, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:24:47,067 Iter 1610, Minibatch Loss= 0.0083, Training Accuracy= 0.9856, Minibatch error= 1.4%\n",
      "2018-05-27 19:24:48,509 Iter 1611, Minibatch Loss= 0.0202, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 19:24:49,934 Iter 1612, Minibatch Loss= 0.0158, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:24:51,351 Iter 1613, Minibatch Loss= 0.0205, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 19:24:52,755 Iter 1614, Minibatch Loss= 0.0059, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:24:54,146 Iter 1615, Minibatch Loss= 0.0165, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:24:55,541 Iter 1616, Minibatch Loss= 0.0121, Training Accuracy= 0.9829, Minibatch error= 1.7%\n",
      "2018-05-27 19:24:56,940 Iter 1617, Minibatch Loss= 0.0134, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 19:24:58,360 Iter 1618, Minibatch Loss= 0.0057, Training Accuracy= 0.9905, Minibatch error= 1.0%\n",
      "2018-05-27 19:24:59,765 Iter 1619, Minibatch Loss= 0.0165, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:24:59,766 Epoch 80, Average loss: 0.0119, learning rate: 0.0010\n",
      "2018-05-27 19:25:09,190 Iter 1620, Minibatch Loss= 0.0121, Training Accuracy= 0.9825, Minibatch error= 1.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:25:12,293 Iter 1620, Minibatch Loss= 0.0165, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:25:13,684 Iter 1621, Minibatch Loss= 0.0086, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 19:25:15,131 Iter 1622, Minibatch Loss= 0.0204, Training Accuracy= 0.9679, Minibatch error= 3.2%\n",
      "2018-05-27 19:25:16,532 Iter 1623, Minibatch Loss= 0.0087, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:25:17,958 Iter 1624, Minibatch Loss= 0.0013, Training Accuracy= 0.9978, Minibatch error= 0.2%\n",
      "2018-05-27 19:25:19,409 Iter 1625, Minibatch Loss= 0.0133, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:25:20,806 Iter 1626, Minibatch Loss= 0.0153, Training Accuracy= 0.9767, Minibatch error= 2.3%\n",
      "2018-05-27 19:25:22,214 Iter 1627, Minibatch Loss= 0.0030, Training Accuracy= 0.9951, Minibatch error= 0.5%\n",
      "2018-05-27 19:25:23,617 Iter 1628, Minibatch Loss= 0.0091, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:25:25,034 Iter 1629, Minibatch Loss= 0.0155, Training Accuracy= 0.9705, Minibatch error= 3.0%\n",
      "2018-05-27 19:25:26,426 Iter 1630, Minibatch Loss= 0.0035, Training Accuracy= 0.9946, Minibatch error= 0.5%\n",
      "2018-05-27 19:25:27,849 Iter 1631, Minibatch Loss= 0.0182, Training Accuracy= 0.9669, Minibatch error= 3.3%\n",
      "2018-05-27 19:25:29,244 Iter 1632, Minibatch Loss= 0.0191, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 19:25:30,681 Iter 1633, Minibatch Loss= 0.0100, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:25:32,066 Iter 1634, Minibatch Loss= 0.0029, Training Accuracy= 0.9952, Minibatch error= 0.5%\n",
      "2018-05-27 19:25:33,457 Iter 1635, Minibatch Loss= 0.0107, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 19:25:34,856 Iter 1636, Minibatch Loss= 0.0165, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 19:25:36,231 Iter 1637, Minibatch Loss= 0.0038, Training Accuracy= 0.9933, Minibatch error= 0.7%\n",
      "2018-05-27 19:25:37,630 Iter 1638, Minibatch Loss= 0.0093, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 19:25:39,027 Iter 1639, Minibatch Loss= 0.0210, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 19:25:39,028 Epoch 81, Average loss: 0.0111, learning rate: 0.0010\n",
      "2018-05-27 19:25:48,347 Iter 1640, Minibatch Loss= 0.0110, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:25:51,487 Iter 1640, Minibatch Loss= 0.0113, Training Accuracy= 0.9808, Minibatch error= 1.9%\n",
      "2018-05-27 19:25:52,891 Iter 1641, Minibatch Loss= 0.0102, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:25:54,303 Iter 1642, Minibatch Loss= 0.0044, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 19:25:55,719 Iter 1643, Minibatch Loss= 0.0013, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 19:25:57,105 Iter 1644, Minibatch Loss= 0.0179, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 19:25:58,487 Iter 1645, Minibatch Loss= 0.0042, Training Accuracy= 0.9935, Minibatch error= 0.6%\n",
      "2018-05-27 19:25:59,895 Iter 1646, Minibatch Loss= 0.0152, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 19:26:01,290 Iter 1647, Minibatch Loss= 0.0057, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:26:02,694 Iter 1648, Minibatch Loss= 0.0131, Training Accuracy= 0.9792, Minibatch error= 2.1%\n",
      "2018-05-27 19:26:04,106 Iter 1649, Minibatch Loss= 0.0080, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:26:05,515 Iter 1650, Minibatch Loss= 0.0166, Training Accuracy= 0.9712, Minibatch error= 2.9%\n",
      "2018-05-27 19:26:06,945 Iter 1651, Minibatch Loss= 0.0097, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:26:08,343 Iter 1652, Minibatch Loss= 0.0183, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 19:26:09,762 Iter 1653, Minibatch Loss= 0.0163, Training Accuracy= 0.9682, Minibatch error= 3.2%\n",
      "2018-05-27 19:26:11,158 Iter 1654, Minibatch Loss= 0.0029, Training Accuracy= 0.9955, Minibatch error= 0.5%\n",
      "2018-05-27 19:26:12,566 Iter 1655, Minibatch Loss= 0.0120, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:26:13,984 Iter 1656, Minibatch Loss= 0.0151, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 19:26:15,443 Iter 1657, Minibatch Loss= 0.0164, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 19:26:16,824 Iter 1658, Minibatch Loss= 0.0127, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 19:26:18,207 Iter 1659, Minibatch Loss= 0.0082, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:26:18,208 Epoch 82, Average loss: 0.0105, learning rate: 0.0010\n",
      "2018-05-27 19:26:27,450 Iter 1660, Minibatch Loss= 0.0112, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 19:26:30,539 Iter 1660, Minibatch Loss= 0.0193, Training Accuracy= 0.9665, Minibatch error= 3.3%\n",
      "2018-05-27 19:26:31,954 Iter 1661, Minibatch Loss= 0.0069, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 19:26:33,324 Iter 1662, Minibatch Loss= 0.0121, Training Accuracy= 0.9804, Minibatch error= 2.0%\n",
      "2018-05-27 19:26:34,712 Iter 1663, Minibatch Loss= 0.0099, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:26:36,099 Iter 1664, Minibatch Loss= 0.0162, Training Accuracy= 0.9773, Minibatch error= 2.3%\n",
      "2018-05-27 19:26:37,512 Iter 1665, Minibatch Loss= 0.0103, Training Accuracy= 0.9823, Minibatch error= 1.8%\n",
      "2018-05-27 19:26:38,935 Iter 1666, Minibatch Loss= 0.0151, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:26:40,376 Iter 1667, Minibatch Loss= 0.0076, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:26:41,820 Iter 1668, Minibatch Loss= 0.0088, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:26:43,223 Iter 1669, Minibatch Loss= 0.0132, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 19:26:44,649 Iter 1670, Minibatch Loss= 0.0070, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:26:46,052 Iter 1671, Minibatch Loss= 0.0067, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:26:47,453 Iter 1672, Minibatch Loss= 0.0078, Training Accuracy= 0.9864, Minibatch error= 1.4%\n",
      "2018-05-27 19:26:48,865 Iter 1673, Minibatch Loss= 0.0066, Training Accuracy= 0.9888, Minibatch error= 1.1%\n",
      "2018-05-27 19:26:50,291 Iter 1674, Minibatch Loss= 0.0149, Training Accuracy= 0.9770, Minibatch error= 2.3%\n",
      "2018-05-27 19:26:51,725 Iter 1675, Minibatch Loss= 0.0087, Training Accuracy= 0.9852, Minibatch error= 1.5%\n",
      "2018-05-27 19:26:53,130 Iter 1676, Minibatch Loss= 0.0096, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 19:26:54,537 Iter 1677, Minibatch Loss= 0.0035, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 19:26:55,951 Iter 1678, Minibatch Loss= 0.0068, Training Accuracy= 0.9878, Minibatch error= 1.2%\n",
      "2018-05-27 19:26:57,394 Iter 1679, Minibatch Loss= 0.0036, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 19:26:57,395 Epoch 83, Average loss: 0.0092, learning rate: 0.0010\n",
      "2018-05-27 19:27:06,677 Iter 1680, Minibatch Loss= 0.0115, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 19:27:09,781 Iter 1680, Minibatch Loss= 0.0190, Training Accuracy= 0.9672, Minibatch error= 3.3%\n",
      "2018-05-27 19:27:11,168 Iter 1681, Minibatch Loss= 0.0096, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 19:27:12,596 Iter 1682, Minibatch Loss= 0.0126, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:27:13,994 Iter 1683, Minibatch Loss= 0.0162, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:27:15,397 Iter 1684, Minibatch Loss= 0.0144, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 19:27:16,797 Iter 1685, Minibatch Loss= 0.0174, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 19:27:18,208 Iter 1686, Minibatch Loss= 0.0163, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:27:19,652 Iter 1687, Minibatch Loss= 0.0176, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 19:27:21,051 Iter 1688, Minibatch Loss= 0.0121, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 19:27:22,485 Iter 1689, Minibatch Loss= 0.0047, Training Accuracy= 0.9926, Minibatch error= 0.7%\n",
      "2018-05-27 19:27:23,885 Iter 1690, Minibatch Loss= 0.0187, Training Accuracy= 0.9733, Minibatch error= 2.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:27:25,326 Iter 1691, Minibatch Loss= 0.0154, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 19:27:26,738 Iter 1692, Minibatch Loss= 0.0120, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 19:27:28,144 Iter 1693, Minibatch Loss= 0.0042, Training Accuracy= 0.9932, Minibatch error= 0.7%\n",
      "2018-05-27 19:27:29,570 Iter 1694, Minibatch Loss= 0.0100, Training Accuracy= 0.9828, Minibatch error= 1.7%\n",
      "2018-05-27 19:27:30,995 Iter 1695, Minibatch Loss= 0.0205, Training Accuracy= 0.9620, Minibatch error= 3.8%\n",
      "2018-05-27 19:27:32,394 Iter 1696, Minibatch Loss= 0.0202, Training Accuracy= 0.9653, Minibatch error= 3.5%\n",
      "2018-05-27 19:27:33,803 Iter 1697, Minibatch Loss= 0.0173, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:27:35,261 Iter 1698, Minibatch Loss= 0.0135, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:27:36,680 Iter 1699, Minibatch Loss= 0.0122, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:27:36,681 Epoch 84, Average loss: 0.0140, learning rate: 0.0010\n",
      "2018-05-27 19:27:46,163 Iter 1700, Minibatch Loss= 0.0111, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 19:27:49,376 Iter 1700, Minibatch Loss= 0.0078, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:27:50,832 Iter 1701, Minibatch Loss= 0.0112, Training Accuracy= 0.9819, Minibatch error= 1.8%\n",
      "2018-05-27 19:27:52,272 Iter 1702, Minibatch Loss= 0.0182, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 19:27:53,702 Iter 1703, Minibatch Loss= 0.0091, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:27:55,118 Iter 1704, Minibatch Loss= 0.0169, Training Accuracy= 0.9701, Minibatch error= 3.0%\n",
      "2018-05-27 19:27:56,541 Iter 1705, Minibatch Loss= 0.0158, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:27:58,004 Iter 1706, Minibatch Loss= 0.0098, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:27:59,453 Iter 1707, Minibatch Loss= 0.0257, Training Accuracy= 0.9540, Minibatch error= 4.6%\n",
      "2018-05-27 19:28:00,874 Iter 1708, Minibatch Loss= 0.0164, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:28:02,279 Iter 1709, Minibatch Loss= 0.0045, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:28:03,694 Iter 1710, Minibatch Loss= 0.0078, Training Accuracy= 0.9876, Minibatch error= 1.2%\n",
      "2018-05-27 19:28:05,123 Iter 1711, Minibatch Loss= 0.0187, Training Accuracy= 0.9681, Minibatch error= 3.2%\n",
      "2018-05-27 19:28:06,543 Iter 1712, Minibatch Loss= 0.0169, Training Accuracy= 0.9745, Minibatch error= 2.6%\n",
      "2018-05-27 19:28:07,962 Iter 1713, Minibatch Loss= 0.0023, Training Accuracy= 0.9962, Minibatch error= 0.4%\n",
      "2018-05-27 19:28:09,381 Iter 1714, Minibatch Loss= 0.0205, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:28:10,844 Iter 1715, Minibatch Loss= 0.0044, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:28:12,306 Iter 1716, Minibatch Loss= 0.0094, Training Accuracy= 0.9848, Minibatch error= 1.5%\n",
      "2018-05-27 19:28:13,735 Iter 1717, Minibatch Loss= 0.0160, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:28:15,141 Iter 1718, Minibatch Loss= 0.0180, Training Accuracy= 0.9680, Minibatch error= 3.2%\n",
      "2018-05-27 19:28:16,562 Iter 1719, Minibatch Loss= 0.0170, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:28:16,563 Epoch 85, Average loss: 0.0132, learning rate: 0.0010\n",
      "2018-05-27 19:28:25,974 Iter 1720, Minibatch Loss= 0.0117, Training Accuracy= 0.9803, Minibatch error= 2.0%\n",
      "2018-05-27 19:28:29,190 Iter 1720, Minibatch Loss= 0.0132, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 19:28:30,694 Iter 1721, Minibatch Loss= 0.0165, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:28:32,174 Iter 1722, Minibatch Loss= 0.0055, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:28:33,662 Iter 1723, Minibatch Loss= 0.0152, Training Accuracy= 0.9729, Minibatch error= 2.7%\n",
      "2018-05-27 19:28:35,115 Iter 1724, Minibatch Loss= 0.0065, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:28:36,612 Iter 1725, Minibatch Loss= 0.0049, Training Accuracy= 0.9913, Minibatch error= 0.9%\n",
      "2018-05-27 19:28:38,090 Iter 1726, Minibatch Loss= 0.0041, Training Accuracy= 0.9934, Minibatch error= 0.7%\n",
      "2018-05-27 19:28:39,586 Iter 1727, Minibatch Loss= 0.0122, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 19:28:41,067 Iter 1728, Minibatch Loss= 0.0124, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 19:28:42,562 Iter 1729, Minibatch Loss= 0.0161, Training Accuracy= 0.9748, Minibatch error= 2.5%\n",
      "2018-05-27 19:28:44,048 Iter 1730, Minibatch Loss= 0.0128, Training Accuracy= 0.9757, Minibatch error= 2.4%\n",
      "2018-05-27 19:28:45,540 Iter 1731, Minibatch Loss= 0.0023, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 19:28:47,041 Iter 1732, Minibatch Loss= 0.0194, Training Accuracy= 0.9614, Minibatch error= 3.9%\n",
      "2018-05-27 19:28:48,549 Iter 1733, Minibatch Loss= 0.0167, Training Accuracy= 0.9702, Minibatch error= 3.0%\n",
      "2018-05-27 19:28:50,048 Iter 1734, Minibatch Loss= 0.0165, Training Accuracy= 0.9708, Minibatch error= 2.9%\n",
      "2018-05-27 19:28:51,584 Iter 1735, Minibatch Loss= 0.0098, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 19:28:53,059 Iter 1736, Minibatch Loss= 0.0022, Training Accuracy= 0.9968, Minibatch error= 0.3%\n",
      "2018-05-27 19:28:54,597 Iter 1737, Minibatch Loss= 0.0174, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:28:56,096 Iter 1738, Minibatch Loss= 0.0065, Training Accuracy= 0.9890, Minibatch error= 1.1%\n",
      "2018-05-27 19:28:57,604 Iter 1739, Minibatch Loss= 0.0175, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 19:28:57,605 Epoch 86, Average loss: 0.0116, learning rate: 0.0010\n",
      "2018-05-27 19:29:07,006 Iter 1740, Minibatch Loss= 0.0114, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 19:29:10,496 Iter 1740, Minibatch Loss= 0.0177, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:29:12,030 Iter 1741, Minibatch Loss= 0.0159, Training Accuracy= 0.9703, Minibatch error= 3.0%\n",
      "2018-05-27 19:29:13,532 Iter 1742, Minibatch Loss= 0.0197, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 19:29:15,067 Iter 1743, Minibatch Loss= 0.0170, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:29:16,596 Iter 1744, Minibatch Loss= 0.0107, Training Accuracy= 0.9836, Minibatch error= 1.6%\n",
      "2018-05-27 19:29:18,108 Iter 1745, Minibatch Loss= 0.0169, Training Accuracy= 0.9697, Minibatch error= 3.0%\n",
      "2018-05-27 19:29:19,614 Iter 1746, Minibatch Loss= 0.0077, Training Accuracy= 0.9865, Minibatch error= 1.3%\n",
      "2018-05-27 19:29:21,150 Iter 1747, Minibatch Loss= 0.0162, Training Accuracy= 0.9726, Minibatch error= 2.7%\n",
      "2018-05-27 19:29:22,657 Iter 1748, Minibatch Loss= 0.0101, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 19:29:24,172 Iter 1749, Minibatch Loss= 0.0199, Training Accuracy= 0.9687, Minibatch error= 3.1%\n",
      "2018-05-27 19:29:25,664 Iter 1750, Minibatch Loss= 0.0026, Training Accuracy= 0.9960, Minibatch error= 0.4%\n",
      "2018-05-27 19:29:27,172 Iter 1751, Minibatch Loss= 0.0193, Training Accuracy= 0.9671, Minibatch error= 3.3%\n",
      "2018-05-27 19:29:28,668 Iter 1752, Minibatch Loss= 0.0034, Training Accuracy= 0.9944, Minibatch error= 0.6%\n",
      "2018-05-27 19:29:30,177 Iter 1753, Minibatch Loss= 0.0176, Training Accuracy= 0.9700, Minibatch error= 3.0%\n",
      "2018-05-27 19:29:31,691 Iter 1754, Minibatch Loss= 0.0204, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:29:33,196 Iter 1755, Minibatch Loss= 0.0111, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 19:29:34,716 Iter 1756, Minibatch Loss= 0.0195, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:29:36,226 Iter 1757, Minibatch Loss= 0.0124, Training Accuracy= 0.9782, Minibatch error= 2.2%\n",
      "2018-05-27 19:29:37,750 Iter 1758, Minibatch Loss= 0.0162, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:29:39,279 Iter 1759, Minibatch Loss= 0.0163, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:29:39,280 Epoch 87, Average loss: 0.0140, learning rate: 0.0010\n",
      "2018-05-27 19:29:48,783 Iter 1760, Minibatch Loss= 0.0112, Training Accuracy= 0.9809, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:29:52,044 Iter 1760, Minibatch Loss= 0.0212, Training Accuracy= 0.9629, Minibatch error= 3.7%\n",
      "2018-05-27 19:29:53,490 Iter 1761, Minibatch Loss= 0.0024, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 19:29:54,944 Iter 1762, Minibatch Loss= 0.0041, Training Accuracy= 0.9930, Minibatch error= 0.7%\n",
      "2018-05-27 19:29:56,401 Iter 1763, Minibatch Loss= 0.0086, Training Accuracy= 0.9847, Minibatch error= 1.5%\n",
      "2018-05-27 19:29:57,885 Iter 1764, Minibatch Loss= 0.0138, Training Accuracy= 0.9758, Minibatch error= 2.4%\n",
      "2018-05-27 19:29:59,319 Iter 1765, Minibatch Loss= 0.0191, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:30:00,789 Iter 1766, Minibatch Loss= 0.0151, Training Accuracy= 0.9736, Minibatch error= 2.6%\n",
      "2018-05-27 19:30:02,255 Iter 1767, Minibatch Loss= 0.0171, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:30:03,679 Iter 1768, Minibatch Loss= 0.0152, Training Accuracy= 0.9715, Minibatch error= 2.9%\n",
      "2018-05-27 19:30:05,162 Iter 1769, Minibatch Loss= 0.0200, Training Accuracy= 0.9642, Minibatch error= 3.6%\n",
      "2018-05-27 19:30:06,631 Iter 1770, Minibatch Loss= 0.0197, Training Accuracy= 0.9647, Minibatch error= 3.5%\n",
      "2018-05-27 19:30:08,159 Iter 1771, Minibatch Loss= 0.0151, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 19:30:09,651 Iter 1772, Minibatch Loss= 0.0136, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 19:30:11,164 Iter 1773, Minibatch Loss= 0.0085, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:30:12,700 Iter 1774, Minibatch Loss= 0.0217, Training Accuracy= 0.9684, Minibatch error= 3.2%\n",
      "2018-05-27 19:30:14,206 Iter 1775, Minibatch Loss= 0.0136, Training Accuracy= 0.9801, Minibatch error= 2.0%\n",
      "2018-05-27 19:30:15,722 Iter 1776, Minibatch Loss= 0.0020, Training Accuracy= 0.9969, Minibatch error= 0.3%\n",
      "2018-05-27 19:30:17,227 Iter 1777, Minibatch Loss= 0.0157, Training Accuracy= 0.9764, Minibatch error= 2.4%\n",
      "2018-05-27 19:30:18,714 Iter 1778, Minibatch Loss= 0.0242, Training Accuracy= 0.9589, Minibatch error= 4.1%\n",
      "2018-05-27 19:30:20,207 Iter 1779, Minibatch Loss= 0.0151, Training Accuracy= 0.9734, Minibatch error= 2.7%\n",
      "2018-05-27 19:30:20,208 Epoch 88, Average loss: 0.0141, learning rate: 0.0010\n",
      "2018-05-27 19:30:29,456 Iter 1780, Minibatch Loss= 0.0112, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:30:32,924 Iter 1780, Minibatch Loss= 0.0138, Training Accuracy= 0.9790, Minibatch error= 2.1%\n",
      "2018-05-27 19:30:34,426 Iter 1781, Minibatch Loss= 0.0156, Training Accuracy= 0.9715, Minibatch error= 2.9%\n",
      "2018-05-27 19:30:35,989 Iter 1782, Minibatch Loss= 0.0221, Training Accuracy= 0.9650, Minibatch error= 3.5%\n",
      "2018-05-27 19:30:37,523 Iter 1783, Minibatch Loss= 0.0163, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 19:30:39,006 Iter 1784, Minibatch Loss= 0.0157, Training Accuracy= 0.9718, Minibatch error= 2.8%\n",
      "2018-05-27 19:30:40,532 Iter 1785, Minibatch Loss= 0.0040, Training Accuracy= 0.9937, Minibatch error= 0.6%\n",
      "2018-05-27 19:30:42,039 Iter 1786, Minibatch Loss= 0.0148, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 19:30:43,556 Iter 1787, Minibatch Loss= 0.0096, Training Accuracy= 0.9838, Minibatch error= 1.6%\n",
      "2018-05-27 19:30:45,013 Iter 1788, Minibatch Loss= 0.0071, Training Accuracy= 0.9886, Minibatch error= 1.1%\n",
      "2018-05-27 19:30:46,480 Iter 1789, Minibatch Loss= 0.0192, Training Accuracy= 0.9646, Minibatch error= 3.5%\n",
      "2018-05-27 19:30:47,974 Iter 1790, Minibatch Loss= 0.0143, Training Accuracy= 0.9762, Minibatch error= 2.4%\n",
      "2018-05-27 19:30:49,475 Iter 1791, Minibatch Loss= 0.0137, Training Accuracy= 0.9788, Minibatch error= 2.1%\n",
      "2018-05-27 19:30:50,970 Iter 1792, Minibatch Loss= 0.0110, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:30:52,458 Iter 1793, Minibatch Loss= 0.0052, Training Accuracy= 0.9907, Minibatch error= 0.9%\n",
      "2018-05-27 19:30:53,967 Iter 1794, Minibatch Loss= 0.0023, Training Accuracy= 0.9966, Minibatch error= 0.3%\n",
      "2018-05-27 19:30:55,456 Iter 1795, Minibatch Loss= 0.0196, Training Accuracy= 0.9645, Minibatch error= 3.5%\n",
      "2018-05-27 19:30:56,986 Iter 1796, Minibatch Loss= 0.0161, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 19:30:58,517 Iter 1797, Minibatch Loss= 0.0187, Training Accuracy= 0.9674, Minibatch error= 3.3%\n",
      "2018-05-27 19:31:00,010 Iter 1798, Minibatch Loss= 0.0102, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 19:31:01,497 Iter 1799, Minibatch Loss= 0.0028, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 19:31:01,498 Epoch 89, Average loss: 0.0124, learning rate: 0.0010\n",
      "2018-05-27 19:31:10,940 Iter 1800, Minibatch Loss= 0.0111, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:31:14,551 Iter 1800, Minibatch Loss= 0.0007, Training Accuracy= 0.9993, Minibatch error= 0.1%\n",
      "2018-05-27 19:31:16,022 Iter 1801, Minibatch Loss= 0.0367, Training Accuracy= 0.9408, Minibatch error= 5.9%\n",
      "2018-05-27 19:31:17,531 Iter 1802, Minibatch Loss= 0.0054, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:31:19,068 Iter 1803, Minibatch Loss= 0.0169, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 19:31:20,620 Iter 1804, Minibatch Loss= 0.0145, Training Accuracy= 0.9739, Minibatch error= 2.6%\n",
      "2018-05-27 19:31:22,054 Iter 1805, Minibatch Loss= 0.0155, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:31:23,552 Iter 1806, Minibatch Loss= 0.0162, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:31:25,063 Iter 1807, Minibatch Loss= 0.0164, Training Accuracy= 0.9698, Minibatch error= 3.0%\n",
      "2018-05-27 19:31:26,554 Iter 1808, Minibatch Loss= 0.0187, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:31:28,024 Iter 1809, Minibatch Loss= 0.0158, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:31:29,479 Iter 1810, Minibatch Loss= 0.0117, Training Accuracy= 0.9812, Minibatch error= 1.9%\n",
      "2018-05-27 19:31:30,950 Iter 1811, Minibatch Loss= 0.0081, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 19:31:32,411 Iter 1812, Minibatch Loss= 0.0185, Training Accuracy= 0.9685, Minibatch error= 3.2%\n",
      "2018-05-27 19:31:33,858 Iter 1813, Minibatch Loss= 0.0163, Training Accuracy= 0.9743, Minibatch error= 2.6%\n",
      "2018-05-27 19:31:35,319 Iter 1814, Minibatch Loss= 0.0009, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 19:31:36,759 Iter 1815, Minibatch Loss= 0.0081, Training Accuracy= 0.9875, Minibatch error= 1.3%\n",
      "2018-05-27 19:31:38,226 Iter 1816, Minibatch Loss= 0.0164, Training Accuracy= 0.9728, Minibatch error= 2.7%\n",
      "2018-05-27 19:31:39,671 Iter 1817, Minibatch Loss= 0.0154, Training Accuracy= 0.9721, Minibatch error= 2.8%\n",
      "2018-05-27 19:31:41,119 Iter 1818, Minibatch Loss= 0.0035, Training Accuracy= 0.9941, Minibatch error= 0.6%\n",
      "2018-05-27 19:31:42,578 Iter 1819, Minibatch Loss= 0.0132, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 19:31:42,579 Epoch 90, Average loss: 0.0134, learning rate: 0.0010\n",
      "2018-05-27 19:31:51,784 Iter 1820, Minibatch Loss= 0.0182, Training Accuracy= 0.9543, Minibatch error= 4.6%\n",
      "2018-05-27 19:31:55,252 Iter 1820, Minibatch Loss= 0.0233, Training Accuracy= 0.9422, Minibatch error= 5.8%\n",
      "2018-05-27 19:31:56,703 Iter 1821, Minibatch Loss= 0.0175, Training Accuracy= 0.9640, Minibatch error= 3.6%\n",
      "2018-05-27 19:31:58,166 Iter 1822, Minibatch Loss= 0.0226, Training Accuracy= 0.9600, Minibatch error= 4.0%\n",
      "2018-05-27 19:31:59,618 Iter 1823, Minibatch Loss= 0.0114, Training Accuracy= 0.9837, Minibatch error= 1.6%\n",
      "2018-05-27 19:32:01,084 Iter 1824, Minibatch Loss= 0.0099, Training Accuracy= 0.9826, Minibatch error= 1.7%\n",
      "2018-05-27 19:32:02,542 Iter 1825, Minibatch Loss= 0.0029, Training Accuracy= 0.9958, Minibatch error= 0.4%\n",
      "2018-05-27 19:32:04,017 Iter 1826, Minibatch Loss= 0.0054, Training Accuracy= 0.9917, Minibatch error= 0.8%\n",
      "2018-05-27 19:32:05,463 Iter 1827, Minibatch Loss= 0.0213, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:32:06,904 Iter 1828, Minibatch Loss= 0.0027, Training Accuracy= 0.9963, Minibatch error= 0.4%\n",
      "2018-05-27 19:32:08,410 Iter 1829, Minibatch Loss= 0.0235, Training Accuracy= 0.9720, Minibatch error= 2.8%\n",
      "2018-05-27 19:32:09,905 Iter 1830, Minibatch Loss= 0.0008, Training Accuracy= 0.9991, Minibatch error= 0.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:32:11,392 Iter 1831, Minibatch Loss= 0.0130, Training Accuracy= 0.9793, Minibatch error= 2.1%\n",
      "2018-05-27 19:32:12,890 Iter 1832, Minibatch Loss= 0.0170, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:32:14,378 Iter 1833, Minibatch Loss= 0.0056, Training Accuracy= 0.9915, Minibatch error= 0.9%\n",
      "2018-05-27 19:32:15,857 Iter 1834, Minibatch Loss= 0.0112, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 19:32:17,346 Iter 1835, Minibatch Loss= 0.0213, Training Accuracy= 0.9658, Minibatch error= 3.4%\n",
      "2018-05-27 19:32:18,827 Iter 1836, Minibatch Loss= 0.0060, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:32:20,312 Iter 1837, Minibatch Loss= 0.0199, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 19:32:21,779 Iter 1838, Minibatch Loss= 0.0079, Training Accuracy= 0.9881, Minibatch error= 1.2%\n",
      "2018-05-27 19:32:23,237 Iter 1839, Minibatch Loss= 0.0137, Training Accuracy= 0.9797, Minibatch error= 2.0%\n",
      "2018-05-27 19:32:23,238 Epoch 91, Average loss: 0.0126, learning rate: 0.0010\n",
      "2018-05-27 19:32:32,608 Iter 1840, Minibatch Loss= 0.0122, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 19:32:36,019 Iter 1840, Minibatch Loss= 0.0111, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 19:32:37,549 Iter 1841, Minibatch Loss= 0.0045, Training Accuracy= 0.9927, Minibatch error= 0.7%\n",
      "2018-05-27 19:32:39,045 Iter 1842, Minibatch Loss= 0.0139, Training Accuracy= 0.9776, Minibatch error= 2.2%\n",
      "2018-05-27 19:32:40,516 Iter 1843, Minibatch Loss= 0.0013, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 19:32:42,015 Iter 1844, Minibatch Loss= 0.0052, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 19:32:43,449 Iter 1845, Minibatch Loss= 0.0082, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 19:32:44,894 Iter 1846, Minibatch Loss= 0.0201, Training Accuracy= 0.9752, Minibatch error= 2.5%\n",
      "2018-05-27 19:32:46,346 Iter 1847, Minibatch Loss= 0.0147, Training Accuracy= 0.9742, Minibatch error= 2.6%\n",
      "2018-05-27 19:32:47,804 Iter 1848, Minibatch Loss= 0.0044, Training Accuracy= 0.9931, Minibatch error= 0.7%\n",
      "2018-05-27 19:32:49,279 Iter 1849, Minibatch Loss= 0.0170, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 19:32:50,740 Iter 1850, Minibatch Loss= 0.0190, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 19:32:52,193 Iter 1851, Minibatch Loss= 0.0076, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 19:32:53,661 Iter 1852, Minibatch Loss= 0.0110, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 19:32:55,138 Iter 1853, Minibatch Loss= 0.0096, Training Accuracy= 0.9831, Minibatch error= 1.7%\n",
      "2018-05-27 19:32:56,618 Iter 1854, Minibatch Loss= 0.0199, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 19:32:58,100 Iter 1855, Minibatch Loss= 0.0197, Training Accuracy= 0.9657, Minibatch error= 3.4%\n",
      "2018-05-27 19:32:59,586 Iter 1856, Minibatch Loss= 0.0027, Training Accuracy= 0.9957, Minibatch error= 0.4%\n",
      "2018-05-27 19:33:01,045 Iter 1857, Minibatch Loss= 0.0060, Training Accuracy= 0.9909, Minibatch error= 0.9%\n",
      "2018-05-27 19:33:02,547 Iter 1858, Minibatch Loss= 0.0202, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:33:04,021 Iter 1859, Minibatch Loss= 0.0186, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 19:33:04,022 Epoch 92, Average loss: 0.0118, learning rate: 0.0010\n",
      "2018-05-27 19:33:13,298 Iter 1860, Minibatch Loss= 0.0112, Training Accuracy= 0.9824, Minibatch error= 1.8%\n",
      "2018-05-27 19:33:16,769 Iter 1860, Minibatch Loss= 0.0165, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 19:33:18,264 Iter 1861, Minibatch Loss= 0.0149, Training Accuracy= 0.9737, Minibatch error= 2.6%\n",
      "2018-05-27 19:33:19,748 Iter 1862, Minibatch Loss= 0.0190, Training Accuracy= 0.9735, Minibatch error= 2.6%\n",
      "2018-05-27 19:33:21,260 Iter 1863, Minibatch Loss= 0.0090, Training Accuracy= 0.9840, Minibatch error= 1.6%\n",
      "2018-05-27 19:33:22,835 Iter 1864, Minibatch Loss= 0.0159, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:33:24,352 Iter 1865, Minibatch Loss= 0.0187, Training Accuracy= 0.9652, Minibatch error= 3.5%\n",
      "2018-05-27 19:33:25,867 Iter 1866, Minibatch Loss= 0.0071, Training Accuracy= 0.9874, Minibatch error= 1.3%\n",
      "2018-05-27 19:33:27,328 Iter 1867, Minibatch Loss= 0.0166, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 19:33:28,893 Iter 1868, Minibatch Loss= 0.0176, Training Accuracy= 0.9675, Minibatch error= 3.2%\n",
      "2018-05-27 19:33:30,357 Iter 1869, Minibatch Loss= 0.0126, Training Accuracy= 0.9805, Minibatch error= 2.0%\n",
      "2018-05-27 19:33:31,904 Iter 1870, Minibatch Loss= 0.0167, Training Accuracy= 0.9693, Minibatch error= 3.1%\n",
      "2018-05-27 19:33:33,365 Iter 1871, Minibatch Loss= 0.0120, Training Accuracy= 0.9807, Minibatch error= 1.9%\n",
      "2018-05-27 19:33:34,867 Iter 1872, Minibatch Loss= 0.0157, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:33:36,383 Iter 1873, Minibatch Loss= 0.0078, Training Accuracy= 0.9859, Minibatch error= 1.4%\n",
      "2018-05-27 19:33:37,858 Iter 1874, Minibatch Loss= 0.0064, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 19:33:39,363 Iter 1875, Minibatch Loss= 0.0077, Training Accuracy= 0.9882, Minibatch error= 1.2%\n",
      "2018-05-27 19:33:40,837 Iter 1876, Minibatch Loss= 0.0159, Training Accuracy= 0.9713, Minibatch error= 2.9%\n",
      "2018-05-27 19:33:42,324 Iter 1877, Minibatch Loss= 0.0085, Training Accuracy= 0.9883, Minibatch error= 1.2%\n",
      "2018-05-27 19:33:43,822 Iter 1878, Minibatch Loss= 0.0023, Training Accuracy= 0.9964, Minibatch error= 0.4%\n",
      "2018-05-27 19:33:45,321 Iter 1879, Minibatch Loss= 0.0012, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 19:33:45,323 Epoch 93, Average loss: 0.0118, learning rate: 0.0010\n",
      "2018-05-27 19:33:54,684 Iter 1880, Minibatch Loss= 0.0108, Training Accuracy= 0.9818, Minibatch error= 1.8%\n",
      "2018-05-27 19:33:58,180 Iter 1880, Minibatch Loss= 0.0026, Training Accuracy= 0.9959, Minibatch error= 0.4%\n",
      "2018-05-27 19:33:59,654 Iter 1881, Minibatch Loss= 0.0045, Training Accuracy= 0.9936, Minibatch error= 0.6%\n",
      "2018-05-27 19:34:01,182 Iter 1882, Minibatch Loss= 0.0156, Training Accuracy= 0.9747, Minibatch error= 2.5%\n",
      "2018-05-27 19:34:02,683 Iter 1883, Minibatch Loss= 0.0168, Training Accuracy= 0.9732, Minibatch error= 2.7%\n",
      "2018-05-27 19:34:04,176 Iter 1884, Minibatch Loss= 0.0054, Training Accuracy= 0.9908, Minibatch error= 0.9%\n",
      "2018-05-27 19:34:05,642 Iter 1885, Minibatch Loss= 0.0074, Training Accuracy= 0.9884, Minibatch error= 1.2%\n",
      "2018-05-27 19:34:07,122 Iter 1886, Minibatch Loss= 0.0030, Training Accuracy= 0.9949, Minibatch error= 0.5%\n",
      "2018-05-27 19:34:08,637 Iter 1887, Minibatch Loss= 0.0066, Training Accuracy= 0.9893, Minibatch error= 1.1%\n",
      "2018-05-27 19:34:10,148 Iter 1888, Minibatch Loss= 0.0093, Training Accuracy= 0.9844, Minibatch error= 1.6%\n",
      "2018-05-27 19:34:11,625 Iter 1889, Minibatch Loss= 0.0033, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 19:34:13,087 Iter 1890, Minibatch Loss= 0.0162, Training Accuracy= 0.9704, Minibatch error= 3.0%\n",
      "2018-05-27 19:34:14,542 Iter 1891, Minibatch Loss= 0.0041, Training Accuracy= 0.9935, Minibatch error= 0.7%\n",
      "2018-05-27 19:34:16,036 Iter 1892, Minibatch Loss= 0.0083, Training Accuracy= 0.9860, Minibatch error= 1.4%\n",
      "2018-05-27 19:34:17,566 Iter 1893, Minibatch Loss= 0.0140, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 19:34:19,098 Iter 1894, Minibatch Loss= 0.0065, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 19:34:20,591 Iter 1895, Minibatch Loss= 0.0074, Training Accuracy= 0.9880, Minibatch error= 1.2%\n",
      "2018-05-27 19:34:22,053 Iter 1896, Minibatch Loss= 0.0044, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:34:23,551 Iter 1897, Minibatch Loss= 0.0173, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:34:25,040 Iter 1898, Minibatch Loss= 0.0151, Training Accuracy= 0.9733, Minibatch error= 2.7%\n",
      "2018-05-27 19:34:26,527 Iter 1899, Minibatch Loss= 0.0157, Training Accuracy= 0.9714, Minibatch error= 2.9%\n",
      "2018-05-27 19:34:26,528 Epoch 94, Average loss: 0.0090, learning rate: 0.0010\n",
      "2018-05-27 19:34:35,727 Iter 1900, Minibatch Loss= 0.0109, Training Accuracy= 0.9814, Minibatch error= 1.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:34:39,220 Iter 1900, Minibatch Loss= 0.0212, Training Accuracy= 0.9631, Minibatch error= 3.7%\n",
      "2018-05-27 19:34:40,675 Iter 1901, Minibatch Loss= 0.0130, Training Accuracy= 0.9784, Minibatch error= 2.2%\n",
      "2018-05-27 19:34:42,120 Iter 1902, Minibatch Loss= 0.0120, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 19:34:43,553 Iter 1903, Minibatch Loss= 0.0051, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 19:34:45,009 Iter 1904, Minibatch Loss= 0.0189, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:34:46,517 Iter 1905, Minibatch Loss= 0.0189, Training Accuracy= 0.9663, Minibatch error= 3.4%\n",
      "2018-05-27 19:34:47,975 Iter 1906, Minibatch Loss= 0.0162, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:34:49,482 Iter 1907, Minibatch Loss= 0.0034, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 19:34:50,973 Iter 1908, Minibatch Loss= 0.0161, Training Accuracy= 0.9690, Minibatch error= 3.1%\n",
      "2018-05-27 19:34:52,476 Iter 1909, Minibatch Loss= 0.0158, Training Accuracy= 0.9695, Minibatch error= 3.1%\n",
      "2018-05-27 19:34:53,967 Iter 1910, Minibatch Loss= 0.0048, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:34:55,481 Iter 1911, Minibatch Loss= 0.0097, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 19:34:56,957 Iter 1912, Minibatch Loss= 0.0084, Training Accuracy= 0.9867, Minibatch error= 1.3%\n",
      "2018-05-27 19:34:58,438 Iter 1913, Minibatch Loss= 0.0050, Training Accuracy= 0.9914, Minibatch error= 0.9%\n",
      "2018-05-27 19:34:59,924 Iter 1914, Minibatch Loss= 0.0060, Training Accuracy= 0.9900, Minibatch error= 1.0%\n",
      "2018-05-27 19:35:01,385 Iter 1915, Minibatch Loss= 0.0156, Training Accuracy= 0.9791, Minibatch error= 2.1%\n",
      "2018-05-27 19:35:02,865 Iter 1916, Minibatch Loss= 0.0158, Training Accuracy= 0.9744, Minibatch error= 2.6%\n",
      "2018-05-27 19:35:04,349 Iter 1917, Minibatch Loss= 0.0094, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 19:35:05,818 Iter 1918, Minibatch Loss= 0.0033, Training Accuracy= 0.9948, Minibatch error= 0.5%\n",
      "2018-05-27 19:35:07,298 Iter 1919, Minibatch Loss= 0.0120, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 19:35:07,299 Epoch 95, Average loss: 0.0111, learning rate: 0.0010\n",
      "2018-05-27 19:35:16,664 Iter 1920, Minibatch Loss= 0.0105, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 19:35:20,174 Iter 1920, Minibatch Loss= 0.0058, Training Accuracy= 0.9901, Minibatch error= 1.0%\n",
      "2018-05-27 19:35:21,692 Iter 1921, Minibatch Loss= 0.0150, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 19:35:23,192 Iter 1922, Minibatch Loss= 0.0158, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:35:24,698 Iter 1923, Minibatch Loss= 0.0164, Training Accuracy= 0.9691, Minibatch error= 3.1%\n",
      "2018-05-27 19:35:26,225 Iter 1924, Minibatch Loss= 0.0044, Training Accuracy= 0.9929, Minibatch error= 0.7%\n",
      "2018-05-27 19:35:27,726 Iter 1925, Minibatch Loss= 0.0141, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 19:35:29,231 Iter 1926, Minibatch Loss= 0.0204, Training Accuracy= 0.9632, Minibatch error= 3.7%\n",
      "2018-05-27 19:35:30,739 Iter 1927, Minibatch Loss= 0.0154, Training Accuracy= 0.9715, Minibatch error= 2.9%\n",
      "2018-05-27 19:35:32,257 Iter 1928, Minibatch Loss= 0.0175, Training Accuracy= 0.9688, Minibatch error= 3.1%\n",
      "2018-05-27 19:35:33,724 Iter 1929, Minibatch Loss= 0.0123, Training Accuracy= 0.9810, Minibatch error= 1.9%\n",
      "2018-05-27 19:35:35,213 Iter 1930, Minibatch Loss= 0.0179, Training Accuracy= 0.9711, Minibatch error= 2.9%\n",
      "2018-05-27 19:35:36,722 Iter 1931, Minibatch Loss= 0.0167, Training Accuracy= 0.9707, Minibatch error= 2.9%\n",
      "2018-05-27 19:35:38,203 Iter 1932, Minibatch Loss= 0.0152, Training Accuracy= 0.9783, Minibatch error= 2.2%\n",
      "2018-05-27 19:35:39,705 Iter 1933, Minibatch Loss= 0.0039, Training Accuracy= 0.9940, Minibatch error= 0.6%\n",
      "2018-05-27 19:35:41,168 Iter 1934, Minibatch Loss= 0.0111, Training Accuracy= 0.9814, Minibatch error= 1.9%\n",
      "2018-05-27 19:35:42,699 Iter 1935, Minibatch Loss= 0.0065, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 19:35:44,215 Iter 1936, Minibatch Loss= 0.0049, Training Accuracy= 0.9916, Minibatch error= 0.8%\n",
      "2018-05-27 19:35:45,728 Iter 1937, Minibatch Loss= 0.0065, Training Accuracy= 0.9887, Minibatch error= 1.1%\n",
      "2018-05-27 19:35:47,281 Iter 1938, Minibatch Loss= 0.0047, Training Accuracy= 0.9920, Minibatch error= 0.8%\n",
      "2018-05-27 19:35:48,810 Iter 1939, Minibatch Loss= 0.0085, Training Accuracy= 0.9851, Minibatch error= 1.5%\n",
      "2018-05-27 19:35:48,811 Epoch 96, Average loss: 0.0113, learning rate: 0.0010\n",
      "2018-05-27 19:35:58,252 Iter 1940, Minibatch Loss= 0.0106, Training Accuracy= 0.9822, Minibatch error= 1.8%\n",
      "2018-05-27 19:36:01,765 Iter 1940, Minibatch Loss= 0.0098, Training Accuracy= 0.9834, Minibatch error= 1.7%\n",
      "2018-05-27 19:36:03,271 Iter 1941, Minibatch Loss= 0.0049, Training Accuracy= 0.9919, Minibatch error= 0.8%\n",
      "2018-05-27 19:36:04,792 Iter 1942, Minibatch Loss= 0.0176, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:36:06,266 Iter 1943, Minibatch Loss= 0.0175, Training Accuracy= 0.9724, Minibatch error= 2.8%\n",
      "2018-05-27 19:36:07,798 Iter 1944, Minibatch Loss= 0.0080, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:36:09,316 Iter 1945, Minibatch Loss= 0.0160, Training Accuracy= 0.9722, Minibatch error= 2.8%\n",
      "2018-05-27 19:36:10,827 Iter 1946, Minibatch Loss= 0.0049, Training Accuracy= 0.9912, Minibatch error= 0.9%\n",
      "2018-05-27 19:36:12,362 Iter 1947, Minibatch Loss= 0.0038, Training Accuracy= 0.9928, Minibatch error= 0.7%\n",
      "2018-05-27 19:36:13,875 Iter 1948, Minibatch Loss= 0.0076, Training Accuracy= 0.9868, Minibatch error= 1.3%\n",
      "2018-05-27 19:36:15,362 Iter 1949, Minibatch Loss= 0.0124, Training Accuracy= 0.9778, Minibatch error= 2.2%\n",
      "2018-05-27 19:36:16,869 Iter 1950, Minibatch Loss= 0.0167, Training Accuracy= 0.9683, Minibatch error= 3.2%\n",
      "2018-05-27 19:36:18,352 Iter 1951, Minibatch Loss= 0.0141, Training Accuracy= 0.9741, Minibatch error= 2.6%\n",
      "2018-05-27 19:36:19,863 Iter 1952, Minibatch Loss= 0.0007, Training Accuracy= 0.9991, Minibatch error= 0.1%\n",
      "2018-05-27 19:36:21,342 Iter 1953, Minibatch Loss= 0.0175, Training Accuracy= 0.9670, Minibatch error= 3.3%\n",
      "2018-05-27 19:36:22,833 Iter 1954, Minibatch Loss= 0.0165, Training Accuracy= 0.9706, Minibatch error= 2.9%\n",
      "2018-05-27 19:36:24,307 Iter 1955, Minibatch Loss= 0.0085, Training Accuracy= 0.9853, Minibatch error= 1.5%\n",
      "2018-05-27 19:36:25,786 Iter 1956, Minibatch Loss= 0.0024, Training Accuracy= 0.9965, Minibatch error= 0.3%\n",
      "2018-05-27 19:36:27,297 Iter 1957, Minibatch Loss= 0.0091, Training Accuracy= 0.9846, Minibatch error= 1.5%\n",
      "2018-05-27 19:36:28,794 Iter 1958, Minibatch Loss= 0.0162, Training Accuracy= 0.9719, Minibatch error= 2.8%\n",
      "2018-05-27 19:36:30,282 Iter 1959, Minibatch Loss= 0.0144, Training Accuracy= 0.9750, Minibatch error= 2.5%\n",
      "2018-05-27 19:36:30,283 Epoch 97, Average loss: 0.0105, learning rate: 0.0010\n",
      "2018-05-27 19:36:39,676 Iter 1960, Minibatch Loss= 0.0110, Training Accuracy= 0.9816, Minibatch error= 1.8%\n",
      "2018-05-27 19:36:43,208 Iter 1960, Minibatch Loss= 0.0149, Training Accuracy= 0.9725, Minibatch error= 2.8%\n",
      "2018-05-27 19:36:44,707 Iter 1961, Minibatch Loss= 0.0103, Training Accuracy= 0.9827, Minibatch error= 1.7%\n",
      "2018-05-27 19:36:46,201 Iter 1962, Minibatch Loss= 0.0185, Training Accuracy= 0.9673, Minibatch error= 3.3%\n",
      "2018-05-27 19:36:47,668 Iter 1963, Minibatch Loss= 0.0217, Training Accuracy= 0.9608, Minibatch error= 3.9%\n",
      "2018-05-27 19:36:49,160 Iter 1964, Minibatch Loss= 0.0186, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:36:50,662 Iter 1965, Minibatch Loss= 0.0134, Training Accuracy= 0.9765, Minibatch error= 2.4%\n",
      "2018-05-27 19:36:52,167 Iter 1966, Minibatch Loss= 0.0099, Training Accuracy= 0.9850, Minibatch error= 1.5%\n",
      "2018-05-27 19:36:53,663 Iter 1967, Minibatch Loss= 0.0155, Training Accuracy= 0.9715, Minibatch error= 2.8%\n",
      "2018-05-27 19:36:55,146 Iter 1968, Minibatch Loss= 0.0138, Training Accuracy= 0.9779, Minibatch error= 2.2%\n",
      "2018-05-27 19:36:56,643 Iter 1969, Minibatch Loss= 0.0123, Training Accuracy= 0.9786, Minibatch error= 2.1%\n",
      "2018-05-27 19:36:58,166 Iter 1970, Minibatch Loss= 0.0146, Training Accuracy= 0.9764, Minibatch error= 2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 19:36:59,653 Iter 1971, Minibatch Loss= 0.0138, Training Accuracy= 0.9761, Minibatch error= 2.4%\n",
      "2018-05-27 19:37:01,134 Iter 1972, Minibatch Loss= 0.0098, Training Accuracy= 0.9835, Minibatch error= 1.6%\n",
      "2018-05-27 19:37:02,624 Iter 1973, Minibatch Loss= 0.0138, Training Accuracy= 0.9746, Minibatch error= 2.5%\n",
      "2018-05-27 19:37:04,109 Iter 1974, Minibatch Loss= 0.0168, Training Accuracy= 0.9692, Minibatch error= 3.1%\n",
      "2018-05-27 19:37:05,608 Iter 1975, Minibatch Loss= 0.0110, Training Accuracy= 0.9811, Minibatch error= 1.9%\n",
      "2018-05-27 19:37:07,110 Iter 1976, Minibatch Loss= 0.0050, Training Accuracy= 0.9923, Minibatch error= 0.8%\n",
      "2018-05-27 19:37:08,602 Iter 1977, Minibatch Loss= 0.0139, Training Accuracy= 0.9740, Minibatch error= 2.6%\n",
      "2018-05-27 19:37:10,117 Iter 1978, Minibatch Loss= 0.0073, Training Accuracy= 0.9863, Minibatch error= 1.4%\n",
      "2018-05-27 19:37:11,611 Iter 1979, Minibatch Loss= 0.0165, Training Accuracy= 0.9774, Minibatch error= 2.3%\n",
      "2018-05-27 19:37:11,612 Epoch 98, Average loss: 0.0128, learning rate: 0.0010\n",
      "2018-05-27 19:37:21,097 Iter 1980, Minibatch Loss= 0.0103, Training Accuracy= 0.9821, Minibatch error= 1.8%\n",
      "2018-05-27 19:37:24,622 Iter 1980, Minibatch Loss= 0.0081, Training Accuracy= 0.9857, Minibatch error= 1.4%\n",
      "2018-05-27 19:37:26,127 Iter 1981, Minibatch Loss= 0.0092, Training Accuracy= 0.9839, Minibatch error= 1.6%\n",
      "2018-05-27 19:37:27,661 Iter 1982, Minibatch Loss= 0.0141, Training Accuracy= 0.9753, Minibatch error= 2.5%\n",
      "2018-05-27 19:37:29,175 Iter 1983, Minibatch Loss= 0.0160, Training Accuracy= 0.9716, Minibatch error= 2.8%\n",
      "2018-05-27 19:37:30,669 Iter 1984, Minibatch Loss= 0.0081, Training Accuracy= 0.9872, Minibatch error= 1.3%\n",
      "2018-05-27 19:37:32,181 Iter 1985, Minibatch Loss= 0.0136, Training Accuracy= 0.9759, Minibatch error= 2.4%\n",
      "2018-05-27 19:37:33,681 Iter 1986, Minibatch Loss= 0.0044, Training Accuracy= 0.9924, Minibatch error= 0.8%\n",
      "2018-05-27 19:37:35,192 Iter 1987, Minibatch Loss= 0.0149, Training Accuracy= 0.9745, Minibatch error= 2.5%\n",
      "2018-05-27 19:37:36,682 Iter 1988, Minibatch Loss= 0.0009, Training Accuracy= 0.9989, Minibatch error= 0.1%\n",
      "2018-05-27 19:37:38,188 Iter 1989, Minibatch Loss= 0.0175, Training Accuracy= 0.9694, Minibatch error= 3.1%\n",
      "2018-05-27 19:37:39,685 Iter 1990, Minibatch Loss= 0.0066, Training Accuracy= 0.9891, Minibatch error= 1.1%\n",
      "2018-05-27 19:37:41,168 Iter 1991, Minibatch Loss= 0.0211, Training Accuracy= 0.9645, Minibatch error= 3.6%\n",
      "2018-05-27 19:37:42,684 Iter 1992, Minibatch Loss= 0.0084, Training Accuracy= 0.9855, Minibatch error= 1.4%\n",
      "2018-05-27 19:37:44,179 Iter 1993, Minibatch Loss= 0.0166, Training Accuracy= 0.9730, Minibatch error= 2.7%\n",
      "2018-05-27 19:37:45,712 Iter 1994, Minibatch Loss= 0.0183, Training Accuracy= 0.9689, Minibatch error= 3.1%\n",
      "2018-05-27 19:37:47,228 Iter 1995, Minibatch Loss= 0.0145, Training Accuracy= 0.9794, Minibatch error= 2.1%\n",
      "2018-05-27 19:37:48,730 Iter 1996, Minibatch Loss= 0.0178, Training Accuracy= 0.9696, Minibatch error= 3.0%\n",
      "2018-05-27 19:37:50,284 Iter 1997, Minibatch Loss= 0.0154, Training Accuracy= 0.9717, Minibatch error= 2.8%\n",
      "2018-05-27 19:37:51,801 Iter 1998, Minibatch Loss= 0.0013, Training Accuracy= 0.9982, Minibatch error= 0.2%\n",
      "2018-05-27 19:37:53,284 Iter 1999, Minibatch Loss= 0.0082, Training Accuracy= 0.9858, Minibatch error= 1.4%\n",
      "2018-05-27 19:37:53,285 Epoch 99, Average loss: 0.0114, learning rate: 0.0010\n",
      "2018-05-27 19:38:02,736 Iter 2000, Minibatch Loss= 0.0118, Training Accuracy= 0.9800, Minibatch error= 2.0%\n",
      "2018-05-27 19:38:04,733 Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from tf_unet import unet2, util, image_util2\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_iters = [20]\n",
    "num_epochs = [100]\n",
    "feat_roots = [32]\n",
    "num_layers = 3\n",
    "class_weight_tumor = [1]\n",
    "reg_lambda = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "batch_size = [1]\n",
    "optimizer = \"adam\"\n",
    "\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "#preparing data loading\n",
    "data_provider = image_util2.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/img/reduce_data/*\", data_suffix = \"flair_.jpg\", mask_suffix = \"seg_.jpg\")\n",
    "#data_provider = image_util.ImageDataProvider(\"/home/javi_fdez_093/deep-learn/flairdata/preprocess_Data/*\", data_suffix = \"flair_.jpg\", mask_suffix = \"seg_.jpg\")\n",
    "for reg_lambda_i in reg_lambda:\n",
    "    for feat_roots_i in feat_roots:\n",
    "        for batch_size_i in batch_size:\n",
    "            for train_iters_i in train_iters: \n",
    "                for num_epochs_i in num_epochs:\n",
    "                    output_path = \"./outputs/appro100/output{}-#epoch{}-#iter{}-lambda{}\".format(datetime.now(),num_epochs_i, train_iters_i, reg_lambda_i)\n",
    "                    #output_path = \"./outputs/output-fr{}-bs{}-lambda{}-clswei{}\".format(feat_roots_i,batch_size_i,reg_lambda_i, class_weight_tumor)\n",
    "                    print('Approach-- #epoch{}-#iter{}-lambda{}'.format(num_epochs_i, train_iters_i, reg_lambda_i))\n",
    "                    net = unet2.Unet(layers=num_layers, features_root=feat_roots_i, channels=1, n_class=n_classes,  \n",
    "                                    cost = \"cross_entropy\", cost_kwargs=dict(regularizer=reg_lambda_i, \n",
    "                                                                             class_weights=[0.2,0.8]))\n",
    "                    trainer = unet2.Trainer(net, optimizer=optimizer, batch_size = batch_size_i)\n",
    "                    trainer.train(data_provider, output_path, training_iters=train_iters_i, epochs=num_epochs_i, restore=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
